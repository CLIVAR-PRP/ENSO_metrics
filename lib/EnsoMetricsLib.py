# -*- coding:UTF-8 -*-
from copy import deepcopy
from inspect import stack as INSPECTstack
from numpy import sign as NUMPYsign
from numpy import sqrt as NUMPYsqrt
from numpy import square as NUMPYsquare

# ENSO_metrics package functions:
from .EnsoCollectionsLib import ReferenceRegions
from . import EnsoErrorsWarnings
from .EnsoPlotLib import metric_variable_names
from .EnsoToolsLib import add_up_errors, percentage_val_eastward, statistical_dispersion
from .EnsoUvcdatToolsLib import ArrayListAx, ArrayToList, AverageAxis, AverageHorizontal, AverageMeridional, \
    AverageTemporal, AverageZonal, BasinMask, CheckTime, Composite, ComputeInterannualAnomalies, ComputePDF, \
    Concatenate, Correlation, DetectEvents, DurationAllEvent, DurationEvent, Event_selection, fill_dict_axis, \
    FindXYMinMaxInTs, get_year_by_year, LinearRegressionAndNonlinearity, LinearRegressionTsAgainstMap, \
    LinearRegressionTsAgainstTs, MinMax, MyEmpty, PreProcessTS, Read_data_mask_area, Read_data_mask_area_multifile, \
    Regrid, remove_global_mean, RmsAxis, RmsHorizontal, RmsMeridional, RmsZonal, SaveNetcdf, SeasonalMean, \
    SkewnessTemporal, SlabOcean, Smoothing, Std, StdMonthly, TimeBounds, TsToMap, TwoVarRegrid
from .KeyArgLib import default_arg_values


# ---------------------------------------------------------------------------------------------------------------------#
#
# Library to compute ENSO metrics
# These functions have file names and variable names as inputs and metric as output
#
def BiasLhfMapRmse(lhffilemod, lhfnamemod, lhfareafilemod, lhfareanamemod, lhflandmaskfilemod, lhflandmasknamemod,
                   lhffileobs, lhfnameobs, lhfareafileobs, lhfareanameobs, lhflandmaskfileobs, lhflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLhfMapRmse() function computes the LHF (latent heat flux) spatial root mean square error (RMSE) in a 'box'
    (usually the tropical Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param lhffilemod: string
        path_to/filename of the file (NetCDF) of the modeled LHF
    :param lhfnamemod: string
        name of LHF variable (hfls, lhf, lhtfl, solatent) in 'lhffilemod'
    :param lhfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LHF
    :param lhfareanamemod: string
        name of areacell variable (areacella, areacello) in 'lhfareafilemod'
    :param lhflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LHF
    :param lhflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfilemod'
    :param lhffileobs: string
        path_to/filename of the file (NetCDF) of the observed LHF
    :param lhfnameobs: string
        name of LHF variable (hfls, lhf, lhtfl, solatent) in 'lhffileobs'
    :param lhfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LHF
    :param lhfareanameobs: string
        name of areacell variable (areacella, areacello) in 'lhfareafileobs'
    :param lhflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LHF
    :param lhflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for LHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLhfMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean LHF RMSE"
    units = "W/m2"
    method = "Spatial root mean square error of " + box + " latent heat flux (LHF)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLhfMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        lhf_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            lhffilemod, lhfnamemod, "heat flux", metric, box, file_area=lhfareafilemod, name_area=lhfareanamemod,
            file_mask=lhflandmaskfilemod, name_mask=lhflandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        lhf_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            lhffileobs, lhfnameobs, "heat flux", metric, box, file_area=lhfareafileobs, name_area=lhfareanameobs,
            file_mask=lhflandmaskfileobs, name_mask=lhflandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(lhf_mod.shape[0] / 12.))
        nbr_year_obs = int(round(lhf_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(lhf_mod)
        actualtimebounds_obs = TimeBounds(lhf_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 LHF in 'box' are normalized / detrended / smoothed (running average) if applicable
        lhf_mod, method, keyerror_mod = PreProcessTS(
            lhf_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        lhf_obs, _, keyerror_obs = PreProcessTS(
            lhf_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regridding
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            lhf_mod, lhf_obs, method = TwoVarRegrid(lhf_mod, lhf_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsHorizontal(lhf_mod, lhf_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(lhf_mod, lhf_obs, axis="xy", centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(lhf_mod, weights=None, axis="xy", centered=1, biased=1))
        sm_std_obs = float(Std(lhf_obs, weights=None, axis="xy", centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "spatialSTD": sm_std_mod, "description": "mean LHF map"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "spatialSTD": sm_std_obs, "description": "mean LHF map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                     "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error, "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=lhf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=lhf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLhfLatRmse(lhffilemod, lhfnamemod, lhfareafilemod, lhfareanamemod, lhflandmaskfilemod, lhflandmasknamemod,
                   lhffileobs, lhfnameobs, lhfareafileobs, lhfareanameobs, lhflandmaskfileobs, lhflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLhfLatRmse() function computes the LHF (latent heat flux) meridional (latitude) root mean square error
    (RMSE) in a 'box' (usually 'nino3_LatExt')

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param lhffilemod: string
        path_to/filename of the file (NetCDF) of the modeled LHF
    :param lhfnamemod: string
        name of LHF variable (hfls, lhf, lhtfl, solatent) in 'lhffilemod'
    :param lhfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LHF
    :param lhfareanamemod: string
        name of areacell variable (areacella, areacello) in 'lhfareafilemod'
    :param lhflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LHF
    :param lhflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfilemod'
    :param lhffileobs: string
        path_to/filename of the file (NetCDF) of the observed LHF
    :param lhfnameobs: string
        name of LHF variable (hfls, lhf, lhtfl, solatent) in 'lhffileobs'
    :param lhfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LHF
    :param lhfareanameobs: string
        name of areacell variable (areacella, areacello) in 'lhfareafileobs'
    :param lhflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LHF
    :param lhflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for LHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLhfLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean LHF meridional RMSE"
    units = "W/m2"
    method = "Meridional root mean square error of " + box + " latent heat flux (LHF)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLhfLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        lhf_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            lhffilemod, lhfnamemod, "heat flux", metric, box, file_area=lhfareafilemod, name_area=lhfareanamemod,
            file_mask=lhflandmaskfilemod, name_mask=lhflandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        lhf_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            lhffileobs, lhfnameobs, "heat flux", metric, box, file_area=lhfareafileobs, name_area=lhfareanameobs,
            file_mask=lhflandmaskfileobs, name_mask=lhflandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(lhf_mod.shape[0] / 12.))
        nbr_year_obs = int(round(lhf_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(lhf_mod)
        actualtimebounds_obs = TimeBounds(lhf_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 LHF averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        lhf_mod, method, keyerror_mod = PreProcessTS(
            lhf_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        lhf_obs, _, keyerror_obs = PreProcessTS(
            lhf_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Zonal average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            lhf_mod, lhf_obs, method = TwoVarRegrid(lhf_mod, lhf_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Zonal average
        lhf_mod, keyerror_mod = AverageZonal(lhf_mod)
        lhf_obs, keyerror_obs = AverageZonal(lhf_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(lhf_mod, lhf_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(lhf_mod, lhf_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(lhf_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(lhf_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                lhffilemod, lhfnamemod, "heat flux", metric, "equatorial_pacific_LatExt2", file_area=lhfareafilemod,
                name_area=lhfareanamemod, file_mask=lhflandmaskfilemod, name_mask=lhflandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                lhffileobs, lhfnameobs, "heat flux", metric, "equatorial_pacific_LatExt2", file_area=lhfareafileobs,
                name_area=lhfareanameobs, file_mask=lhflandmaskfileobs, name_mask=lhflandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess LHF (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean LHF map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean LHF across latitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean LHF across latitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=lhf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=lhf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLhfLonRmse(lhffilemod, lhfnamemod, lhfareafilemod, lhfareanamemod, lhflandmaskfilemod, lhflandmasknamemod,
                   lhffileobs, lhfnameobs, lhfareafileobs, lhfareanameobs, lhflandmaskfileobs, lhflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLhfLonRmse() function computes the LHF (latent heat flux) zonal (longitude) root mean square error (RMSE) in
    a 'box' (usually the Equatorial Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param lhffilemod: string
        path_to/filename of the file (NetCDF) of the modeled LHF
    :param lhfnamemod: string
        name of LHF variable (hfls, lhf, lhtfl, solatent) in 'lhffilemod'
    :param lhfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LHF
    :param lhfareanamemod: string
        name of areacell variable (areacella, areacello) in 'lhfareafilemod'
    :param lhflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LHF
    :param lhflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfilemod'
    :param lhffileobs: string
        path_to/filename of the file (NetCDF) of the observed LHF
    :param lhfnameobs: string
        name of LHF variable (hfls, lhf, lhtfl, solatent) in 'lhffileobs'
    :param lhfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LHF
    :param lhfareanameobs: string
        name of areacell variable (areacella, areacello) in 'lhfareafileobs'
    :param lhflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LHF
    :param lhflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for LHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLhfLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean LHF zonal RMSE"
    units = "W/m2"
    method = "Zonal root mean square error of " + box + " latent heat flux (LHF)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLhfLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        lhf_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            lhffilemod, lhfnamemod, "heat flux", metric, box, file_area=lhfareafilemod, name_area=lhfareanamemod,
            file_mask=lhflandmaskfilemod, name_mask=lhflandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        lhf_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            lhffileobs, lhfnameobs, "heat flux", metric, box, file_area=lhfareafileobs, name_area=lhfareanameobs,
            file_mask=lhflandmaskfileobs, name_mask=lhflandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(lhf_mod.shape[0] / 12.))
        nbr_year_obs = int(round(lhf_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(lhf_mod)
        actualtimebounds_obs = TimeBounds(lhf_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 LHF averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        lhf_mod, method, keyerror_mod = PreProcessTS(
            lhf_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        lhf_obs, _, keyerror_obs = PreProcessTS(
            lhf_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Meridional average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            lhf_mod, lhf_obs, method = TwoVarRegrid(lhf_mod, lhf_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Meridional average
        lhf_mod, keyerror_mod = AverageMeridional(lhf_mod)
        lhf_obs, keyerror_obs = AverageMeridional(lhf_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(lhf_mod, lhf_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(lhf_mod, lhf_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(lhf_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(lhf_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                lhffilemod, lhfnamemod, "heat flux", metric, "equatorial_pacific_LatExt2", file_area=lhfareafilemod,
                name_area=lhfareanamemod, file_mask=lhflandmaskfilemod, name_mask=lhflandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                lhffileobs, lhfnameobs, "heat flux", metric, "equatorial_pacific_LatExt2", file_area=lhfareafileobs,
                name_area=lhfareanameobs, file_mask=lhflandmaskfileobs, name_mask=lhflandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess LHF (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean LHF map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean LHF across longitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean LHF across longitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=lhf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=lhf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLwrMapRmse(lwrfilemod, lwrnamemod, lwrareafilemod, lwrareanamemod, lwrlandmaskfilemod, lwrlandmasknamemod,
                   lwrfileobs, lwrnameobs, lwrareafileobs, lwrareanameobs, lwrlandmaskfileobs, lwrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLwrMapRmse() function computes the LWR (net surface longwave radiation) spatial root mean square error
    (RMSE) in a 'box' (usually the tropical Pacific)

    The net surface longwave radiation is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into lwrfile and lwrname of this
    function a list() of the two needed files and variable names (CMIP: rlds-rlus)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param lwrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled LWR
    :param lwrnamemod: string
        name of LWR variable (lwr, rls, dlwrf - ulwrf, rlds - rlus) (may be a list of variables) in 'lwrfilemod'
    :param lwrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LWR
    :param lwrareanamemod: string
        name of areacell variable (areacella, areacello) in 'lwrareafilemod'
    :param lwrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LWR
    :param lwrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfilemod'
    :param lwrfileobs: string
        path_to/filename of the file (NetCDF) of the observed LWR
    :param lwrnameobs: string
        name of LWR variable (lwr, rls, dlwrf - ulwrf, rlds - rlus) (may be a list of variables) in 'lwrfileobs'
    :param lwrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LWR
    :param lwrareanameobs: string
        name of areacell variable (areacella, areacello) in 'lwrareafileobs'
    :param lwrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LWR
    :param lwrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for LWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLwrMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean LWR RMSE"
    units = "W/m2"
    method = "Spatial root mean square error of " + box + " net surface longwave radiation (LWR)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLwrMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        lwr_mod, areacell_mod, keyerror_mod = Read_data_mask_area_multifile(
            lwrfilemod, lwrnamemod, "heat flux", "lwr", metric, box, file_area=lwrareafilemod, name_area=lwrareanamemod,
            file_mask=lwrlandmaskfilemod, name_mask=lwrlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
        lwr_obs, areacell_obs, keyerror_obs = Read_data_mask_area_multifile(
            lwrfileobs, lwrnameobs, "heat flux", "lwr", metric, box, file_area=lwrareafileobs, name_area=lwrareanameobs,
            file_mask=lwrlandmaskfileobs, name_mask=lwrlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(lwr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(lwr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(lwr_mod)
        actualtimebounds_obs = TimeBounds(lwr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 LWR in 'box' are normalized / detrended / smoothed (running average) if applicable
        lwr_mod, method, keyerror_mod = PreProcessTS(
            lwr_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        lwr_obs, _, keyerror_obs = PreProcessTS(
            lwr_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regridding
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            lwr_mod, lwr_obs, method = TwoVarRegrid(lwr_mod, lwr_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsHorizontal(lwr_mod, lwr_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(lwr_mod, lwr_obs, axis="xy", centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(lwr_mod, weights=None, axis="xy", centered=1, biased=1))
        sm_std_obs = float(Std(lwr_obs, weights=None, axis="xy", centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "spatialSTD": sm_std_mod, "description": "mean LWR map"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "spatialSTD": sm_std_obs, "description": "mean LWR map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                     "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error, "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=lwr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=lwr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLwrLatRmse(lwrfilemod, lwrnamemod, lwrareafilemod, lwrareanamemod, lwrlandmaskfilemod, lwrlandmasknamemod,
                   lwrfileobs, lwrnameobs, lwrareafileobs, lwrareanameobs, lwrlandmaskfileobs, lwrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLwrLatRmse() function computes the LWR (net surface longwave radiation) meridional (latitude) root mean
    square error (RMSE) in a 'box' (usually 'nino3_LatExt')

    The net surface longwave radiation is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into lwrfile and lwrname of this
    function a list() of the two needed files and variable names (CMIP: rlds-rlus)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param lwrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled LWR
    :param lwrnamemod: string
        name of LWR variable (lwr, rls, dlwrf - ulwrf, rlds - rlus) (may be a list of variables) in 'lwrfilemod'
    :param lwrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LWR
    :param lwrareanamemod: string
        name of areacell variable (areacella, areacello) in 'lwrareafilemod'
    :param lwrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LWR
    :param lwrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfilemod'
    :param lwrfileobs: string
        path_to/filename of the file (NetCDF) of the observed LWR
    :param lwrnameobs: string
        name of LWR variable (lwr, rls, dlwrf - ulwrf, rlds - rlus) (may be a list of variables) in 'lwrfileobs'
    :param lwrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LWR
    :param lwrareanameobs: string
        name of areacell variable (areacella, areacello) in 'lwrareafileobs'
    :param lwrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LWR
    :param lwrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for LWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLwrLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean LWR meridional RMSE"
    units = "W/m2"
    method = "Meridional root mean square error of " + box + " net surface longwave radiation (LWR)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLwrLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        lwr_mod, areacell_mod, keyerror_mod = Read_data_mask_area_multifile(
            lwrfilemod, lwrnamemod, "heat flux", "lwr", metric, box, file_area=lwrareafilemod, name_area=lwrareanamemod,
            file_mask=lwrlandmaskfilemod, name_mask=lwrlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
        lwr_obs, areacell_obs, keyerror_obs = Read_data_mask_area_multifile(
            lwrfileobs, lwrnameobs, "heat flux", "lwr", metric, box, file_area=lwrareafileobs, name_area=lwrareanameobs,
            file_mask=lwrlandmaskfileobs, name_mask=lwrlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(lwr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(lwr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(lwr_mod)
        actualtimebounds_obs = TimeBounds(lwr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 LWR averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        lwr_mod, method, keyerror_mod = PreProcessTS(
            lwr_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        lwr_obs, _, keyerror_obs = PreProcessTS(
            lwr_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Zonal average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            lwr_mod, lwr_obs, method = TwoVarRegrid(lwr_mod, lwr_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Zonal average
        lwr_mod, keyerror_mod = AverageZonal(lwr_mod)
        lwr_obs, keyerror_obs = AverageZonal(lwr_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(lwr_mod, lwr_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(lwr_mod, lwr_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(lwr_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(lwr_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                lwrfilemod, lwrnamemod, "heat flux", "lwr", metric, "equatorial_pacific_LatExt2",
                file_area=lwrareafilemod, name_area=lwrareanamemod, file_mask=lwrlandmaskfilemod,
                name_mask=lwrlandmasknamemod, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                lwrfileobs, lwrnameobs, "heat flux", "lwr", metric, "equatorial_pacific_LatExt2",
                file_area=lwrareafileobs, name_area=lwrareanameobs, file_mask=lwrlandmaskfileobs,
                name_mask=lwrlandmasknameobs, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess LWR (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean LWR map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean LWR across latitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean LWR across latitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=lwr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=lwr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLwrLonRmse(lwrfilemod, lwrnamemod, lwrareafilemod, lwrareanamemod, lwrlandmaskfilemod, lwrlandmasknamemod,
                   lwrfileobs, lwrnameobs, lwrareafileobs, lwrareanameobs, lwrlandmaskfileobs, lwrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLwrLonRmse() function computes the LWR (net surface longwave radiation) zonal (longitude) root mean square
    error (RMSE) in a 'box' (usually the Equatorial Pacific)

    The net surface longwave radiation is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into lwrfile and lwrname of this
    function a list() of the two needed files and variable names (CMIP: rlds-rlus)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param lwrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled LWR
    :param lwrnamemod: string
        name of LWR variable (lwr, rls, dlwrf - ulwrf, rlds - rlus) (may be a list of variables) in 'lwrfilemod'
    :param lwrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LWR
    :param lwrareanamemod: string
        name of areacell variable (areacella, areacello) in 'lwrareafilemod'
    :param lwrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LWR
    :param lwrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfilemod'
    :param lwrfileobs: string
        path_to/filename of the file (NetCDF) of the observed LWR
    :param lwrnameobs: string
        name of LWR variable (lwr, rls, dlwrf - ulwrf, rlds - rlus) (may be a list of variables) in 'lwrfileobs'
    :param lwrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LWR
    :param lwrareanameobs: string
        name of areacell variable (areacella, areacello) in 'lwrareafileobs'
    :param lwrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LWR
    :param lwrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for LWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLwrLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean LWR zonal RMSE"
    units = "W/m2"
    method = "Zonal root mean square error of " + box + " net surface longwave radiation (LWR)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLwrLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        lwr_mod, areacell_mod, keyerror_mod = Read_data_mask_area_multifile(
            lwrfilemod, lwrnamemod, "heat flux", "lwr", metric, box, file_area=lwrareafilemod, name_area=lwrareanamemod,
            file_mask=lwrlandmaskfilemod, name_mask=lwrlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
        lwr_obs, areacell_obs, keyerror_obs = Read_data_mask_area_multifile(
            lwrfileobs, lwrnameobs, "heat flux", "lwr", metric, box, file_area=lwrareafileobs, name_area=lwrareanameobs,
            file_mask=lwrlandmaskfileobs, name_mask=lwrlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(lwr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(lwr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(lwr_mod)
        actualtimebounds_obs = TimeBounds(lwr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 LWR averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        lwr_mod, method, keyerror_mod = PreProcessTS(
            lwr_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        lwr_obs, _, keyerror_obs = PreProcessTS(
            lwr_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Meridional average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            lwr_mod, lwr_obs, method = TwoVarRegrid(lwr_mod, lwr_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Meridional average
        lwr_mod, keyerror_mod = AverageMeridional(lwr_mod)
        lwr_obs, keyerror_obs = AverageMeridional(lwr_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(lwr_mod, lwr_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(lwr_mod, lwr_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(lwr_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(lwr_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                lwrfilemod, lwrnamemod, "heat flux", "lwr", metric, "equatorial_pacific_LatExt2",
                file_area=lwrareafilemod, name_area=lwrareanamemod, file_mask=lwrlandmaskfilemod,
                name_mask=lwrlandmasknamemod, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                lwrfileobs, lwrnameobs, "heat flux", "lwr", metric, "equatorial_pacific_LatExt2",
                file_area=lwrareafileobs, name_area=lwrareanameobs, file_mask=lwrlandmaskfileobs,
                name_mask=lwrlandmasknameobs, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess LWR (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean LWR map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean LWR across longitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean LWR across longitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=lwr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=lwr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasPrMapRmse(prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                  prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, box,
                  centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                  metname="", **kwargs):
    """
    The BiasPrMapRmse() function computes the PR (precipitation) spatial root mean square error (RMSE) in a 'box'
    (usually the tropical Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr, prec, precip, rain) in 'prfilemod'
    :param prareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for PR
    :param prareanamemod: string
        name of areacell variable (areacella, areacello) in 'prareafilemod'
    :param prlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for PR
    :param prlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfilemod'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, prec, precip, rain) in 'prfileobs'
    :param prareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for PR
    :param prareanameobs: string
        name of areacell variable (areacella, areacello) in 'prareafileobs'
    :param prlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for PR
    :param prlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for PR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'GPCPv2.3',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasPrMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean PR RMSE"
    units = "mm/day"
    method = "Spatial root mean square error of " + box + " precipitation (PR)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasPrMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        pr_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            prfilemod, prnamemod, "precipitation", metric, box, file_area=prareafilemod, name_area=prareanamemod,
            file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        pr_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            prfileobs, prnameobs, "precipitation", metric, box, file_area=prareafileobs, name_area=prareanameobs,
            file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(pr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(pr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(pr_mod)
        actualtimebounds_obs = TimeBounds(pr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 PR in 'box' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, method, keyerror_mod = PreProcessTS(
            pr_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        pr_obs, _, keyerror_obs = PreProcessTS(pr_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regridding
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            pr_mod, pr_obs, method = TwoVarRegrid(pr_mod, pr_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsHorizontal(pr_mod, pr_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(pr_mod, pr_obs, axis="xy", centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(pr_mod, weights=None, axis="xy", centered=1, biased=1))
        sm_std_obs = float(Std(pr_obs, weights=None, axis="xy", centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "spatialSTD": sm_std_mod, "description": "mean PR map"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "spatialSTD": sm_std_obs, "description": "mean PR map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                     "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error, "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=pr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=pr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasPrLatRmse(prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                  prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, box,
                  centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                  metname="", **kwargs):
    """
    The BiasPrLatRmse() function computes the PR (precipitation) meridional (latitude) root mean square error (RMSE) in
    a 'box' (usually 'nino3_LatExt')

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr, prec, precip, rain) in 'prfilemod'
    :param prareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for PR
    :param prareanamemod: string
        name of areacell variable (areacella, areacello) in 'prareafilemod'
    :param prlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for PR
    :param prlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfilemod'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, prec, precip, rain) in 'prfileobs'
    :param prareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for PR
    :param prareanameobs: string
        name of areacell variable (areacella, areacello) in 'prareafileobs'
    :param prlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for PR
    :param prlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for PR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'GPCPv2.3',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasPrLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean PR meridional RMSE"
    units = "mm/day"
    method = "Meridional root mean square error of " + box + " precipitation (PR)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasPrLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        pr_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            prfilemod, prnamemod, "precipitation", metric, box, file_area=prareafilemod, name_area=prareanamemod,
            file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        pr_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            prfileobs, prnameobs, "precipitation", metric, box, file_area=prareafileobs, name_area=prareanameobs,
            file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(pr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(pr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(pr_mod)
        actualtimebounds_obs = TimeBounds(pr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 PR averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, method, keyerror_mod = PreProcessTS(
            pr_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        pr_obs, _, keyerror_obs = PreProcessTS(pr_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Zonal average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            pr_mod, pr_obs, method = TwoVarRegrid(pr_mod, pr_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Zonal average
        pr_mod, keyerror_mod = AverageZonal(pr_mod)
        pr_obs, keyerror_obs = AverageZonal(pr_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(pr_mod, pr_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(pr_mod, pr_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(pr_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(pr_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                prfilemod, prnamemod, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafilemod,
                name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                prfileobs, prnameobs, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafileobs,
                name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess PR (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean PR map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean PR across latitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean PR across latitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=pr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=pr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasPrLonRmse(prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                  prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, box,
                  centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                  metname="", **kwargs):
    """
    The BiasPrLonRmse() function computes the PR (precipitation) zonal (longitude) root mean square error (RMSE) in a
    'box' (usually the Equatorial Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr, prec, precip, rain) in 'prfilemod'
    :param prareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for PR
    :param prareanamemod: string
        name of areacell variable (areacella, areacello) in 'prareafilemod'
    :param prlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for PR
    :param prlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfilemod'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, prec, precip, rain) in 'prfileobs'
    :param prareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for PR
    :param prareanameobs: string
        name of areacell variable (areacella, areacello) in 'prareafileobs'
    :param prlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for PR
    :param prlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for PR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'GPCPv2.3',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasPrLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean PR zonal RMSE"
    units = "mm/day"
    method = "Zonal root mean square error of " + box + " precipitation (PR)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasPrLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        pr_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            prfilemod, prnamemod, "precipitation", metric, box, file_area=prareafilemod, name_area=prareanamemod,
            file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        pr_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            prfileobs, prnameobs, "precipitation", metric, box, file_area=prareafileobs, name_area=prareanameobs,
            file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(pr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(pr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(pr_mod)
        actualtimebounds_obs = TimeBounds(pr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 PR averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, method, keyerror_mod = PreProcessTS(
            pr_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        pr_obs, _, keyerror_obs = PreProcessTS(pr_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Meridional average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            pr_mod, pr_obs, method = TwoVarRegrid(pr_mod, pr_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Meridional average
        pr_mod, keyerror_mod = AverageMeridional(pr_mod)
        pr_obs, keyerror_obs = AverageMeridional(pr_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(pr_mod, pr_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(pr_mod, pr_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(pr_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(pr_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                prfilemod, prnamemod, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafilemod,
                name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                prfileobs, prnameobs, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafileobs,
                name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess PR (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean PR map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean PR across longitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean PR across longitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=pr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=pr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasShfMapRmse(shffilemod, shfnamemod, shfareafilemod, shfareanamemod, shflandmaskfilemod, shflandmasknamemod,
                   shffileobs, shfnameobs, shfareafileobs, shfareanameobs, shflandmaskfileobs, shflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasShfMapRmse() function computes the SHF (sensible heat flux) spatial root mean square error (RMSE) in a 'box'
    (usually the tropical Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param shffilemod: string
        path_to/filename of the file (NetCDF) of the modeled SHF
    :param shfnamemod: string
        name of SHF variable (hfss, shf, shtfl, sosensib) in 'shffilemod'
    :param shfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SHF
    :param shfareanamemod: string
        name of areacell variable (areacella, areacello) in 'shfareafilemod'
    :param shflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SHF
    :param shflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfilemod'
    :param shffileobs: string
        path_to/filename of the file (NetCDF) of the observed SHF
    :param shfnameobs: string
        name of SHF variable (hfss, shf, shtfl, sosensib) in 'shffileobs'
    :param shfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SHF
    :param shfareanameobs: string
        name of areacell variable (areacella, areacello) in 'shfareafileobs'
    :param shflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SHF
    :param shflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for SHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasShfMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean SHF RMSE"
    units = "W/m2"
    method = "Spatial root mean square error of " + box + " sensible heat flux (SHF)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasShfMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        shf_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            shffilemod, shfnamemod, "heat flux", metric, box, file_area=shfareafilemod, name_area=shfareanamemod,
            file_mask=shflandmaskfilemod, name_mask=shflandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        shf_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            shffileobs, shfnameobs, "heat flux", metric, box, file_area=shfareafileobs, name_area=shfareanameobs,
            file_mask=shflandmaskfileobs, name_mask=shflandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(shf_mod.shape[0] / 12.))
        nbr_year_obs = int(round(shf_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(shf_mod)
        actualtimebounds_obs = TimeBounds(shf_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SHF in 'box' are normalized / detrended / smoothed (running average) if applicable
        shf_mod, method, keyerror_mod = PreProcessTS(
            shf_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        shf_obs, _, keyerror_obs = PreProcessTS(
            shf_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regridding
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            shf_mod, shf_obs, method = TwoVarRegrid(shf_mod, shf_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsHorizontal(shf_mod, shf_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(shf_mod, shf_obs, axis="xy", centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(shf_mod, weights=None, axis="xy", centered=1, biased=1))
        sm_std_obs = float(Std(shf_obs, weights=None, axis="xy", centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "spatialSTD": sm_std_mod, "description": "mean SHF map"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "spatialSTD": sm_std_obs, "description": "mean SHF map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                     "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error, "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=shf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=shf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasShfLatRmse(shffilemod, shfnamemod, shfareafilemod, shfareanamemod, shflandmaskfilemod, shflandmasknamemod,
                   shffileobs, shfnameobs, shfareafileobs, shfareanameobs, shflandmaskfileobs, shflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasShfLatRmse() function computes the SHF (sensible heat flux) meridional (latitude) root mean square error
    (RMSE) in a 'box' (usually 'nino3_LatExt')

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param shffilemod: string
        path_to/filename of the file (NetCDF) of the modeled SHF
    :param shfnamemod: string
        name of SHF variable (hfss, shf, shtfl, sosensib) in 'shffilemod'
    :param shfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SHF
    :param shfareanamemod: string
        name of areacell variable (areacella, areacello) in 'shfareafilemod'
    :param shflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SHF
    :param shflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfilemod'
    :param shffileobs: string
        path_to/filename of the file (NetCDF) of the observed SHF
    :param shfnameobs: string
        name of SHF variable (hfss, shf, shtfl, sosensib) in 'shffileobs'
    :param shfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SHF
    :param shfareanameobs: string
        name of areacell variable (areacella, areacello) in 'shfareafileobs'
    :param shflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SHF
    :param shflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for SHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasShfLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean SHF meridional RMSE"
    units = "W/m2"
    method = "Meridional root mean square error of " + box + " sensible heat flux (SHF)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasShfLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        shf_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            shffilemod, shfnamemod, "heat flux", metric, box, file_area=shfareafilemod, name_area=shfareanamemod,
            file_mask=shflandmaskfilemod, name_mask=shflandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        shf_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            shffileobs, shfnameobs, "heat flux", metric, box, file_area=shfareafileobs, name_area=shfareanameobs,
            file_mask=shflandmaskfileobs, name_mask=shflandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(shf_mod.shape[0] / 12.))
        nbr_year_obs = int(round(shf_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(shf_mod)
        actualtimebounds_obs = TimeBounds(shf_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SHF averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        shf_mod, method, keyerror_mod = PreProcessTS(
            shf_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        shf_obs, _, keyerror_obs = PreProcessTS(
            shf_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Zonal average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            shf_mod, shf_obs, method = TwoVarRegrid(shf_mod, shf_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Zonal average
        shf_mod, keyerror_mod = AverageZonal(shf_mod)
        shf_obs, keyerror_obs = AverageZonal(shf_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(shf_mod, shf_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(shf_mod, shf_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(shf_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(shf_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                shffilemod, shfnamemod, "heat flux", metric, "equatorial_pacific_LatExt2", file_area=shfareafilemod,
                name_area=shfareanamemod, file_mask=shflandmaskfilemod, name_mask=shflandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                shffileobs, shfnameobs, "heat flux", metric, "equatorial_pacific_LatExt2", file_area=shfareafileobs,
                name_area=shfareanameobs, file_mask=shflandmaskfileobs, name_mask=shflandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess SHF (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean SHF map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean SHF across latitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean SHF across latitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=shf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=shf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasShfLonRmse(shffilemod, shfnamemod, shfareafilemod, shfareanamemod, shflandmaskfilemod, shflandmasknamemod,
                   shffileobs, shfnameobs, shfareafileobs, shfareanameobs, shflandmaskfileobs, shflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasShfLonRmse() function computes the SHF (sensible heat flux) zonal (longitude) root mean square error (RMSE)
    in a 'box' (usually the Equatorial Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param shffilemod: string
        path_to/filename of the file (NetCDF) of the modeled SHF
    :param shfnamemod: string
        name of SHF variable (hfss, shf, shtfl, sosensib) in 'shffilemod'
    :param shfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SHF
    :param shfareanamemod: string
        name of areacell variable (areacella, areacello) in 'shfareafilemod'
    :param shflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SHF
    :param shflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfilemod'
    :param shffileobs: string
        path_to/filename of the file (NetCDF) of the observed SHF
    :param shfnameobs: string
        name of SHF variable (hfss, shf, shtfl, sosensib) in 'shffileobs'
    :param shfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SHF
    :param shfareanameobs: string
        name of areacell variable (areacella, areacello) in 'shfareafileobs'
    :param shflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SHF
    :param shflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasShfLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean SHF zonal RMSE"
    units = "W/m2"
    method = "Zonal root mean square error of " + box + " sensible heat flux (SHF)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasShfLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        shf_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            shffilemod, shfnamemod, "heat flux", metric, box, file_area=shfareafilemod, name_area=shfareanamemod,
            file_mask=shflandmaskfilemod, name_mask=shflandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        shf_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            shffileobs, shfnameobs, "heat flux", metric, box, file_area=shfareafileobs, name_area=shfareanameobs,
            file_mask=shflandmaskfileobs, name_mask=shflandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(shf_mod.shape[0] / 12.))
        nbr_year_obs = int(round(shf_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(shf_mod)
        actualtimebounds_obs = TimeBounds(shf_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SHF averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        shf_mod, method, keyerror_mod = PreProcessTS(
            shf_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        shf_obs, _, keyerror_obs = PreProcessTS(
            shf_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Meridional average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            shf_mod, shf_obs, method = TwoVarRegrid(shf_mod, shf_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Meridional average
        shf_mod, keyerror_mod = AverageMeridional(shf_mod)
        shf_obs, keyerror_obs = AverageMeridional(shf_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(shf_mod, shf_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(shf_mod, shf_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(shf_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(shf_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                shffilemod, shfnamemod, "heat flux", metric, "equatorial_pacific_LatExt2", file_area=shfareafilemod,
                name_area=shfareanamemod, file_mask=shflandmaskfilemod, name_mask=shflandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                shffileobs, shfnameobs, "heat flux", metric, "equatorial_pacific_LatExt2", file_area=shfareafileobs,
                name_area=shfareanameobs, file_mask=shflandmaskfileobs, name_mask=shflandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess SHF (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean SHF map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean SHF across longitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean SHF across longitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=shf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=shf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSshMapRmse(sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                   sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSshMapRmse() function computes the dynamic SSH (sea surface height) spatial root mean square error (RMSE) in
    a 'box' (usually the tropical Pacific)
    The mean spatio-temporal value in the region is subtracted because the geoid is not defined the same way in models
    and observations. The additional mass linked to the melting of the polar caps and the thermal expansion of the ocean
    is not usually taken into account in climate models.
    This metric cannot measure the mean bias (SSH too high or too low) but can illustrate is gradients are reproduced.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May  3 2021

    Inputs:
    ------
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SSH
    :param sshareanamemod: string
        name of areacell variable (areacella, areacello) in 'sshareafilemod'
    :param sshlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SSH
    :param sshlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfilemod'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SSH
    :param sshareanameobs: string
        name of areacell variable (areacella, areacello) in 'sshareafileobs'
    :param sshlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SSH
    :param sshlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for SSH
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'AVISO',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSshMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean dynamic SSH RMSE"
    units = "cm"
    method = "Spatial root mean square error of " + box + " dynamic sea surface height (SSH)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSshMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        ssh_mod, ssh_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, box, file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        glo_mod, glo_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, "global2", file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_obs, ssh_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, box, file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        glo_obs, glo_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, "global2", file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(ssh_mod.shape[0] / 12.))
        nbr_year_obs = int(round(ssh_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(ssh_mod)
        actualtimebounds_obs = TimeBounds(ssh_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        glo_mod, _, keyerror_mod = PreProcessTS(
            glo_mod, "", areacell=glo_mod_areacell, average="horizontal", region="global2", **kwargs)
        glo_obs, _, keyerror_obs = PreProcessTS(
            glo_obs, "", areacell=glo_obs_areacell, average="horizontal", region="global2", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_mod])
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        ssh_mod, method, keyerror_mod = remove_global_mean(ssh_mod, glo_mod, "SSH", method)
        ssh_obs, _, keyerror_obs = remove_global_mean(ssh_obs, glo_obs, "", "")
        keyerror = add_up_errors([keyerror_mod, keyerror_mod])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean", 15, **dict_debug)
        # 2.3 SSH in 'box' are normalized / detrended / smoothed (running average) if applicable
        ssh_mod, method, keyerror_mod = PreProcessTS(
            ssh_mod, method, areacell=ssh_mod_areacell, average="time", region=box, **kwargs)
        ssh_obs, _, keyerror_obs = PreProcessTS(
            ssh_obs, "", areacell=ssh_obs_areacell, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del glo_mod_areacell, glo_obs_areacell, my_det, ssh_mod_areacell, ssh_mod_areacell, ssh_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Compute spatial mean value and remove it
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            ssh_mod, ssh_obs, method = TwoVarRegrid(ssh_mod, ssh_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Spatial mean value
        mean_mod, keyerror_mod = AverageHorizontal(ssh_mod, region=box)
        mean_obs, keyerror_obs = AverageHorizontal(ssh_obs, region=box)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 3.3 Remove spatial mean
        ssh_mod = ssh_mod - mean_mod
        ssh_obs = ssh_obs - mean_obs

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsHorizontal(ssh_mod, ssh_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(ssh_mod, ssh_obs, axis="xy", centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(ssh_mod, weights=None, axis="xy", centered=1, biased=1))
        sm_std_obs = float(Std(ssh_obs, weights=None, axis="xy", centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "spatialSTD": sm_std_mod, "description": "mean dynamic SSH map, mean spatial value removed"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "spatialSTD": sm_std_obs, "description": "mean dynamic SSH map, mean spatial value removed"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                     "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error, "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=ssh_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=ssh_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSshLatRmse(sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                   sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSshLatRmse() function computes the dynamic SSH (sea surface height) meridional (latitude) root mean square
    error (RMSE) in a 'box' (usually 'nino3_LatExt')
    The mean spatio-temporal value in the region is subtracted because the geoid is not defined the same way in models
    and observations. The additional mass linked to the melting of the polar caps and the thermal expansion of the ocean
    is not usually taken into account in climate models.
    This metric cannot measure the mean bias (SSH too high or too low) but can illustrate is gradients are reproduced.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May  3 2021

    Inputs:
    ------
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SSH
    :param sshareanamemod: string
        name of areacell variable (areacella, areacello) in 'sshareafilemod'
    :param sshlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SSH
    :param sshlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfilemod'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SSH
    :param sshareanameobs: string
        name of areacell variable (areacella, areacello) in 'sshareafileobs'
    :param sshlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SSH
    :param sshlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for SSH
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'AVISO',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSshLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean dynamic SSH meridional RMSE"
    units = "cm"
    method = "Meridional root mean square error of " + box + " dynamic sea surface height (SSH)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSshLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        ssh_mod, ssh_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, box, file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        glo_mod, glo_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, "global2", file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_obs, ssh_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, box, file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        glo_obs, glo_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, "global2", file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(ssh_mod.shape[0] / 12.))
        nbr_year_obs = int(round(ssh_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(ssh_mod)
        actualtimebounds_obs = TimeBounds(ssh_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        glo_mod, _, keyerror_mod = PreProcessTS(
            glo_mod, "", areacell=glo_mod_areacell, average="horizontal", region="global2", **kwargs)
        glo_obs, _, keyerror_obs = PreProcessTS(
            glo_obs, "", areacell=glo_obs_areacell, average="horizontal", region="global2", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_mod])
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        ssh_mod, method, keyerror_mod = remove_global_mean(ssh_mod, glo_mod, "SSH", method)
        ssh_obs, _, keyerror_obs = remove_global_mean(ssh_obs, glo_obs, "", "")
        keyerror = add_up_errors([keyerror_mod, keyerror_mod])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean", 15, **dict_debug)
        # 2.3 SSH in 'box' are normalized / detrended / smoothed (running average) if applicable
        ssh_mod, method, keyerror_mod = PreProcessTS(
            ssh_mod, method, areacell=ssh_mod_areacell, average="time", region=box, **kwargs)
        ssh_obs, _, keyerror_obs = PreProcessTS(
            ssh_obs, "", areacell=ssh_obs_areacell, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del glo_mod_areacell, glo_obs_areacell, my_det, ssh_mod_areacell, ssh_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Zonal average, spatial mean removed
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            ssh_mod, ssh_obs, method = TwoVarRegrid(ssh_mod, ssh_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Zonal average
        ssh_mod, keyerror_mod = AverageZonal(ssh_mod)
        ssh_obs, keyerror_obs = AverageZonal(ssh_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)
        # 3.3 Spatial mean value
        mean_mod, keyerror_mod = AverageAxis(ssh_mod)
        mean_obs, keyerror_obs = AverageAxis(ssh_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 3.4 Remove spatial mean
        ssh_mod = ssh_mod - float(mean_mod)
        ssh_obs = ssh_obs - float(mean_obs)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(ssh_mod, ssh_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(ssh_mod, ssh_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(ssh_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(ssh_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific_LatExt2",
                file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                name_mask=sshlandmasknamemod, maskland=False, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific_LatExt2",
                file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                name_mask=sshlandmasknameobs, maskland=False, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Remove global mean SSH to local SSH at every time step
            map_mod, _, keyerror_mod = remove_global_mean(map_mod, glo_mod, "", "")
            map_obs, _, keyerror_obs = remove_global_mean(map_obs, glo_obs, "", "")
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean: netcdf", 15, **dict_debug)
            # Preprocess SSH (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Spatial mean value
            mean_mod, keyerror_mod = AverageHorizontal(map_mod, region="equatorial_pacific_LatExt2")
            mean_obs, keyerror_obs = AverageHorizontal(map_obs, region="equatorial_pacific_LatExt2")
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Remove spatial mean
            map_mod = map_mod - float(mean_mod)
            map_obs = map_obs - float(mean_obs)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean dynamic SSH map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "arraySTD": sm_std_obs,
                     "description": "mean dynamic SSH across latitudes, mean spatial value removed"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "arraySTD": sm_std_obs,
                     "description": "mean dynamic SSH across latitudes, mean spatial value removed"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=ssh_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=ssh_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSshLonRmse(sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                   sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSshLonRmse() function computes the dynamic SSH (sea surface height) zonal (longitude) root mean square error
    (RMSE) in a 'box' (usually the Equatorial Pacific)
    The mean spatio-temporal value in the region is subtracted because the geoid is not defined the same way in models
    and observations. The additional mass linked to the melting of the polar caps and the thermal expansion of the ocean
    is not usually taken into account in climate models.
    This metric cannot measure the mean bias (SSH too high or too low) but can illustrate is gradients are reproduced.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May  3 2021

    Inputs:
    ------
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SSH
    :param sshareanamemod: string
        name of areacell variable (areacella, areacello) in 'sshareafilemod'
    :param sshlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SSH
    :param sshlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfilemod'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SSH
    :param sshareanameobs: string
        name of areacell variable (areacella, areacello) in 'sshareafileobs'
    :param sshlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SSH
    :param sshlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SSH
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'AVISO',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSshLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean dynamic SSH zonal RMSE"
    units = "cm"
    method = "Zonal root mean square error of " + box + " dynamic sea surface height (SSH)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSshLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        ssh_mod, ssh_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, box, file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        glo_mod, glo_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, "global2", file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_obs, ssh_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, box, file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        glo_obs, glo_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, "global2", file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(ssh_mod.shape[0] / 12.))
        nbr_year_obs = int(round(ssh_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(ssh_mod)
        actualtimebounds_obs = TimeBounds(ssh_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        glo_mod, _, keyerror_mod = PreProcessTS(
            glo_mod, "", areacell=glo_mod_areacell, average="horizontal", region="global2", **kwargs)
        glo_obs, _, keyerror_obs = PreProcessTS(
            glo_obs, "", areacell=glo_obs_areacell, average="horizontal", region="global2", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_mod])
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        ssh_mod, method, keyerror_mod = remove_global_mean(ssh_mod, glo_mod, "SSH", method)
        ssh_obs, _, keyerror_obs = remove_global_mean(ssh_obs, glo_obs, "", "")
        keyerror = add_up_errors([keyerror_mod, keyerror_mod])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean", 15, **dict_debug)
        # 2.3 SSH in 'box' are normalized / detrended / smoothed (running average) if applicable
        ssh_mod, method, keyerror_mod = PreProcessTS(
            ssh_mod, method, areacell=ssh_mod_areacell, average="time", region=box, **kwargs)
        ssh_obs, _, keyerror_obs = PreProcessTS(
            ssh_obs, "", areacell=ssh_obs_areacell, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del glo_mod_areacell, glo_obs_areacell, my_det, ssh_mod_areacell, ssh_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Meridional average, spatial mean removed
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            ssh_mod, ssh_obs, method = TwoVarRegrid(ssh_mod, ssh_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Meridional average
        ssh_mod, keyerror_mod = AverageMeridional(ssh_mod)
        ssh_obs, keyerror_obs = AverageMeridional(ssh_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
        # 3.3 Spatial mean value
        mean_mod, keyerror_mod = AverageAxis(ssh_mod)
        mean_obs, keyerror_obs = AverageAxis(ssh_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 3.4 Remove spatial mean
        ssh_mod = ssh_mod - mean_mod
        ssh_obs = ssh_obs - mean_obs

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(ssh_mod, ssh_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(ssh_mod, ssh_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(ssh_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(ssh_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific_LatExt2",
                file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                name_mask=sshlandmasknamemod, maskland=False, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific_LatExt2",
                file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                name_mask=sshlandmasknameobs, maskland=False, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Remove global mean SSH to local SSH at every time step
            map_mod, _, keyerror_mod = remove_global_mean(map_mod, glo_mod, "", "")
            map_obs, _, keyerror_obs = remove_global_mean(map_obs, glo_obs, "", "")
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean: netcdf", 15, **dict_debug)
            # Preprocess SSH (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Spatial mean value
            mean_mod, keyerror_mod = AverageHorizontal(map_mod, region="equatorial_pacific_LatExt2")
            mean_obs, keyerror_obs = AverageHorizontal(map_obs, region="equatorial_pacific_LatExt2")
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Remove spatial mean
            map_mod = map_mod - mean_mod
            map_obs = map_obs - mean_obs
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean dynamic SSH map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "arraySTD": sm_std_obs,
                     "description": "mean dynamic SSH across longitudes, mean spatial value removed"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "arraySTD": sm_std_obs,
                     "description": "mean dynamic SSH across longitudes, mean spatial value removed"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=ssh_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=ssh_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSstMapRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSstMapRmse() function computes the SST (sea surface temperature) spatial root mean square error (RMSE) in a
    'box' (usually the tropical Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSstMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean SST RMSE"
    units = "C"
    method = "Spatial root mean square error of " + box + " sea surface temperature (SST)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSstMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
            file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
            file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST in 'box' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, method, keyerror_mod = PreProcessTS(
            sst_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regridding
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            sst_mod, sst_obs, method = TwoVarRegrid(sst_mod, sst_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsHorizontal(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(sst_mod, sst_obs, axis="xy", centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(sst_mod, weights=None, axis="xy", centered=1, biased=1))
        sm_std_obs = float(Std(sst_obs, weights=None, axis="xy", centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "spatialSTD": sm_std_mod, "description": "mean SST map"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "spatialSTD": sm_std_obs, "description": "mean SST map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                     "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error, "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sst_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=sst_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSstLatRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSstLatRmse() function computes the SST (sea surface temperature) meridional (latitude) root mean square
    error (RMSE) in a 'box' (usually 'nino3_LatExt')

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSstLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean SST meridional RMSE"
    units = "C"
    method = "Meridional root mean square error of " + box + " sea surface temperature (SST)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSstLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
            file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
            file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, method, keyerror_mod = PreProcessTS(
            sst_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Zonal average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            sst_mod, sst_obs, method = TwoVarRegrid(sst_mod, sst_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Zonal average
        sst_mod, keyerror_mod = AverageZonal(sst_mod)
        sst_obs, keyerror_obs = AverageZonal(sst_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(sst_mod, sst_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(sst_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(sst_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafilemod,
                name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafileobs,
                name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess SST (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean SST map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean SST across latitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean SST across latitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sst_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=sst_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSstLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSstLonRmse() function computes the SST (sea surface temperature) zonal (longitude) root mean square error
    (RMSE) in a 'box' (usually the Equatorial Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSstLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean SST zonal RMSE"
    units = "C"
    method = "Zonal root mean square error of " + box + " sea surface temperature (SST)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSstLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
            file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
            file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, method, keyerror_mod = PreProcessTS(
            sst_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Meridional average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            sst_mod, sst_obs, method = TwoVarRegrid(sst_mod, sst_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Meridional average
        sst_mod, keyerror_mod = AverageMeridional(sst_mod)
        sst_obs, keyerror_obs = AverageMeridional(sst_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(sst_mod, sst_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(sst_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(sst_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafilemod,
                name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafileobs,
                name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess SST (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean SST map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean SST across longitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean SST across longitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sst_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=sst_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSwrMapRmse(swrfilemod, swrnamemod, swrareafilemod, swrareanamemod, swrlandmaskfilemod, swrlandmasknamemod,
                   swrfileobs, swrnameobs, swrareafileobs, swrareanameobs, swrlandmaskfileobs, swrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSwrMapRmse() function computes the SWR (net surface shortwave radiation) spatial root mean square error
    (RMSE) in a 'box' (usually the tropical Pacific)

    The net surface shortwave radiation is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into swrfile and swrname of this
    function a list() of the two needed files and variable names (CMIP: rsds-rsus)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param swrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SWR
    :param swrnamemod: string
        name of SWR variable (rss, swr, dswrf - uswrf, rsds - rsus) (may be a list of variables) in 'swrfilemod'
    :param swrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SWR
    :param swrareanamemod: string
        name of areacell variable (areacella, areacello) in 'swrareafilemod'
    :param swrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SWR
    :param swrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfilemod'
    :param swrfileobs: string
        path_to/filename of the file (NetCDF) of the observed SWR
    :param swrnameobs: string
        name of SWR variable (rss, swr, dswrf - uswrf, rsds - rsus) (may be a list of variables) in 'swrfileobs'
    :param swrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SWR
    :param swrareanameobs: string
        name of areacell variable (areacella, areacello) in 'swrareafileobs'
    :param swrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SWR
    :param swrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for SWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSwrMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean SWR RMSE"
    units = "W/m2"
    method = "Spatial root mean square error of " + box + " net surface shortwave radiation (SWR)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSwrMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        swr_mod, areacell_mod, keyerror_mod = Read_data_mask_area_multifile(
            swrfilemod, swrnamemod, "heat flux", "swr", metric, box, file_area=swrareafilemod, name_area=swrareanamemod,
            file_mask=swrlandmaskfilemod, name_mask=swrlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
        swr_obs, areacell_obs, keyerror_obs = Read_data_mask_area_multifile(
            swrfileobs, swrnameobs, "heat flux", "swr", metric, box, file_area=swrareafileobs, name_area=swrareanameobs,
            file_mask=swrlandmaskfileobs, name_mask=swrlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(swr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(swr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(swr_mod)
        actualtimebounds_obs = TimeBounds(swr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SWR in 'box' are normalized / detrended / smoothed (running average) if applicable
        swr_mod, method, keyerror_mod = PreProcessTS(
            swr_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        swr_obs, _, keyerror_obs = PreProcessTS(
            swr_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regridding
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            swr_mod, swr_obs, method = TwoVarRegrid(swr_mod, swr_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsHorizontal(swr_mod, swr_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(swr_mod, swr_obs, axis="xy", centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(swr_mod, weights=None, axis="xy", centered=1, biased=1))
        sm_std_obs = float(Std(swr_obs, weights=None, axis="xy", centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "spatialSTD": sm_std_mod, "description": "mean SWR map"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "spatialSTD": sm_std_obs, "description": "mean SWR map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                     "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error, "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=swr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=swr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSwrLatRmse(swrfilemod, swrnamemod, swrareafilemod, swrareanamemod, swrlandmaskfilemod, swrlandmasknamemod,
                   swrfileobs, swrnameobs, swrareafileobs, swrareanameobs, swrlandmaskfileobs, swrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSwrLatRmse() function computes the SWR (net surface shortwave radiation) meridional (latitude) root mean
    square error (RMSE) in a 'box' (usually 'nino3_LatExt')

    The net surface shortwave radiation is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into swrfile and swrname of this
    function a list() of the two needed files and variable names (CMIP: rsds-rsus)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param swrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SWR
    :param swrnamemod: string
        name of SWR variable (rss, swr, dswrf - uswrf, rsds - rsus) (may be a list of variables) in 'swrfilemod'
    :param swrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SWR
    :param swrareanamemod: string
        name of areacell variable (areacella, areacello) in 'swrareafilemod'
    :param swrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SWR
    :param swrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfilemod'
    :param swrfileobs: string
        path_to/filename of the file (NetCDF) of the observed SWR
    :param swrnameobs: string
        name of SWR variable (rss, swr, dswrf - uswrf, rsds - rsus) (may be a list of variables) in 'swrfileobs'
    :param swrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SWR
    :param swrareanameobs: string
        name of areacell variable (areacella, areacello) in 'swrareafileobs'
    :param swrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SWR
    :param swrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for SWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSwrLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean SWR meridional RMSE"
    units = "W/m2"
    method = "Meridional root mean square error of " + box + " net surface shortwave radiation (SWR)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSwrLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        swr_mod, areacell_mod, keyerror_mod = Read_data_mask_area_multifile(
            swrfilemod, swrnamemod, "heat flux", "swr", metric, box, file_area=swrareafilemod, name_area=swrareanamemod,
            file_mask=swrlandmaskfilemod, name_mask=swrlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
        swr_obs, areacell_obs, keyerror_obs = Read_data_mask_area_multifile(
            swrfileobs, swrnameobs, "heat flux", "swr", metric, box, file_area=swrareafileobs, name_area=swrareanameobs,
            file_mask=swrlandmaskfileobs, name_mask=swrlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(swr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(swr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(swr_mod)
        actualtimebounds_obs = TimeBounds(swr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SWR averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        swr_mod, method, keyerror_mod = PreProcessTS(
            swr_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        swr_obs, _, keyerror_obs = PreProcessTS(
            swr_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Zonal average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            swr_mod, swr_obs, method = TwoVarRegrid(swr_mod, swr_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Zonal average
        swr_mod, keyerror_mod = AverageZonal(swr_mod)
        swr_obs, keyerror_obs = AverageZonal(swr_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(swr_mod, swr_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(swr_mod, swr_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(swr_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(swr_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                swrfilemod, swrnamemod, "heat flux", "swr", metric, "equatorial_pacific_LatExt2",
                file_area=swrareafilemod, name_area=swrareanamemod, file_mask=swrlandmaskfilemod,
                name_mask=swrlandmasknamemod, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                swrfileobs, swrnameobs, "heat flux", "swr", metric, "equatorial_pacific_LatExt2",
                file_area=swrareafileobs, name_area=swrareanameobs, file_mask=swrlandmaskfileobs,
                name_mask=swrlandmasknameobs, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess SWR (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean SWR map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean SWR across latitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean SWR across latitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=swr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=swr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSwrLonRmse(swrfilemod, swrnamemod, swrareafilemod, swrareanamemod, swrlandmaskfilemod, swrlandmasknamemod,
                   swrfileobs, swrnameobs, swrareafileobs, swrareanameobs, swrlandmaskfileobs, swrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSwrLonRmse() function computes the SWR (net surface shortwave radiation) zonal (longitude) root mean square
    error (RMSE) in a 'box' (usually the Equatorial Pacific)

    The net surface shortwave radiation is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into swrfile and swrname of this
    function a list() of the two needed files and variable names (CMIP: rsds-rsus)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param swrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SWR
    :param swrnamemod: string
        name of SWR variable (rss, swr, dswrf - uswrf, rsds - rsus) (may be a list of variables) in 'swrfilemod'
    :param swrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SWR
    :param swrareanamemod: string
        name of areacell variable (areacella, areacello) in 'swrareafilemod'
    :param swrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SWR
    :param swrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfilemod'
    :param swrfileobs: string
        path_to/filename of the file (NetCDF) of the observed SWR
    :param swrnameobs: string
        name of SWR variable (rss, swr, dswrf - uswrf, rsds - rsus) (may be a list of variables) in 'swrfileobs'
    :param swrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SWR
    :param swrareanameobs: string
        name of areacell variable (areacella, areacello) in 'swrareafileobs'
    :param swrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SWR
    :param swrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSwrLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean SWR zonal RMSE"
    units = "W/m2"
    method = "Zonal root mean square error of " + box + " net surface shortwave radiation (SWR)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSwrLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        swr_mod, areacell_mod, keyerror_mod = Read_data_mask_area_multifile(
            swrfilemod, swrnamemod, "heat flux", "swr", metric, box, file_area=swrareafilemod, name_area=swrareanamemod,
            file_mask=swrlandmaskfilemod, name_mask=swrlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
        swr_obs, areacell_obs, keyerror_obs = Read_data_mask_area_multifile(
            swrfileobs, swrnameobs, "heat flux", "swr", metric, box, file_area=swrareafileobs, name_area=swrareanameobs,
            file_mask=swrlandmaskfileobs, name_mask=swrlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(swr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(swr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(swr_mod)
        actualtimebounds_obs = TimeBounds(swr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SWR averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        swr_mod, method, keyerror_mod = PreProcessTS(
            swr_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        swr_obs, _, keyerror_obs = PreProcessTS(
            swr_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Meridional average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            swr_mod, swr_obs, method = TwoVarRegrid(swr_mod, swr_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Meridional average
        swr_mod, keyerror_mod = AverageMeridional(swr_mod)
        swr_obs, keyerror_obs = AverageMeridional(swr_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(swr_mod, swr_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(swr_mod, swr_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(swr_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(swr_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                swrfilemod, swrnamemod, "heat flux", "swr", metric, "equatorial_pacific_LatExt2",
                file_area=swrareafilemod, name_area=swrareanamemod, file_mask=swrlandmaskfilemod,
                name_mask=swrlandmasknamemod, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                swrfileobs, swrnameobs, "heat flux", "swr", metric, "equatorial_pacific_LatExt2",
                file_area=swrareafileobs, name_area=swrareanameobs, file_mask=swrlandmaskfileobs,
                name_mask=swrlandmasknameobs, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess SWR (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean SWR map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean SWR across longitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean SWR across longitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=swr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=swr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauxMapRmse(taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                    taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs, box,
                    centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                    metname="", **kwargs):
    """
    The BiasTauxMapRmse() function computes the Taux (zonal wind stress) spatial root mean square error (RMSE) in a
    'box' (usually the tropical Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Taux
    :param taunamemod: string
        name of Taux variable (tauu, tauuo, taux) in 'taufilemod'
    :param tauareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for Taux
    :param tauareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauareafilemod'
    :param taulandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for Taux
    :param taulandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfilemod'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Taux
    :param taunameobs: string
        name of Taux variable (tauu, tauuo, taux) in 'taufileobs'
    :param tauareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for Taux
    :param tauareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauareafileobs'
    :param taulandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for Taux
    :param taulandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for Taux
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauxMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled Taux file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed Taux file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean Taux RMSE"
    units = "1e-3 N/m2"
    method = "Spatial root mean square error of " + box + " zonal wind stress (Taux)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauxMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        tau_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, box, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, box, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(tau_mod.shape[0] / 12.))
        nbr_year_obs = int(round(tau_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(tau_mod)
        actualtimebounds_obs = TimeBounds(tau_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 Taux in 'box' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod = PreProcessTS(
            tau_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        tau_obs, _, keyerror_obs = PreProcessTS(
            tau_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regridding
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tau_mod, tau_obs, method = TwoVarRegrid(tau_mod, tau_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsHorizontal(tau_mod, tau_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(tau_mod, tau_obs, axis="xy", centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tau_mod, weights=None, axis="xy", centered=1, biased=1))
        sm_std_obs = float(Std(tau_obs, weights=None, axis="xy", centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "spatialSTD": sm_std_mod, "description": "mean Taux map"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "spatialSTD": sm_std_obs, "description": "mean Taux map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                     "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error, "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tau_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tau_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauxLatRmse(taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                    taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs, box,
                    centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                    metname="", **kwargs):
    """
    The BiasTauxLatRmse() function computes the Taux (zonal wind stress) meridional (latitude) root mean square error
    (RMSE) in a 'box' (usually 'equatorial_pacific_LatExt')

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Taux
    :param taunamemod: string
        name of Taux variable (tauu, tauuo, taux) in 'taufilemod'
    :param tauareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for Taux
    :param tauareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauareafilemod'
    :param taulandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for Taux
    :param taulandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfilemod'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Taux
    :param taunameobs: string
        name of Taux variable (tauu, tauuo, taux) in 'taufileobs'
    :param tauareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for Taux
    :param tauareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauareafileobs'
    :param taulandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for Taux
    :param taulandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for Taux
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauxLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled Taux file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed Taux file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean Taux meridional RMSE"
    units = "1e-3 N/m2"
    method = "Meridional root mean square error of " + box + " zonal wind stress (Taux)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauxLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        tau_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, box, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, box, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(tau_mod.shape[0] / 12.))
        nbr_year_obs = int(round(tau_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(tau_mod)
        actualtimebounds_obs = TimeBounds(tau_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 Taux averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod = PreProcessTS(
            tau_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        tau_obs, _, keyerror_obs = PreProcessTS(
            tau_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Zonal average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tau_mod, tau_obs, method = TwoVarRegrid(tau_mod, tau_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Zonal average
        tau_mod, keyerror_mod = AverageZonal(tau_mod)
        tau_obs, keyerror_obs = AverageZonal(tau_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(tau_mod, tau_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(tau_mod, tau_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tau_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tau_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess Taux (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean Taux map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean Taux across latitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean Taux across latitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tau_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tau_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauxLonRmse(taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                    taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs, box,
                    centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                    metname="", **kwargs):
    """
    The BiasTauxLonRmse() function computes the Taux (zonal wind stress) zonal (longitude) root mean square error (RMSE)
    in a 'box' (usually the Equatorial Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Taux
    :param taunamemod: string
        name of Taux variable (tauu, tauuo, taux) in 'taufilemod'
    :param tauareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for Taux
    :param tauareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauareafilemod'
    :param taulandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for Taux
    :param taulandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfilemod'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Taux
    :param taunameobs: string
        name of Taux variable (tauu, tauuo, taux) in 'taufileobs'
    :param tauareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for Taux
    :param tauareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauareafileobs'
    :param taulandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for Taux
    :param taulandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for Taux
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauxLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled Taux file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed Taux file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean Taux zonal RMSE"
    units = "1e-3 N/m2"
    method = "Zonal root mean square error of " + box + " zonal wind stress (Taux)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauxLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        tau_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, box, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, box, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(tau_mod.shape[0] / 12.))
        nbr_year_obs = int(round(tau_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(tau_mod)
        actualtimebounds_obs = TimeBounds(tau_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 Taux averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod = PreProcessTS(
            tau_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        tau_obs, _, keyerror_obs = PreProcessTS(
            tau_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Meridional average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tau_mod, tau_obs, method = TwoVarRegrid(tau_mod, tau_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Meridional average
        tau_mod, keyerror_mod = AverageMeridional(tau_mod)
        tau_obs, keyerror_obs = AverageMeridional(tau_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(tau_mod, tau_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(tau_mod, tau_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tau_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tau_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess Taux (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean Taux map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean Taux across longitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean Taux across longitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tau_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tau_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauyMapRmse(taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                    taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs, box,
                    centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                    metname="", **kwargs):
    """
    The BiasTauyMapRmse() function computes the Tauy (meridional wind stress) spatial root mean square error (RMSE) in a
    'box' (usually the tropical Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Wed Dec  9 2020

    Inputs:
    ------
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Tauy
    :param taunamemod: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufilemod'
    :param tauareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for Tauy
    :param tauareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauareafilemod'
    :param taulandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for Tauy
    :param taulandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfilemod'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Tauy
    :param taunameobs: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufileobs'
    :param tauareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for Tauy
    :param tauareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauareafileobs'
    :param taulandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for Tauy
    :param taulandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for Tauy
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauyMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled Tauy file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed Tauy file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean Tauy RMSE"
    units = "1e-3 N/m2"
    method = "Spatial root mean square error of " + box + " meridional wind stress (Tauy)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauyMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        tau_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, box, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, box, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(tau_mod.shape[0] / 12.))
        nbr_year_obs = int(round(tau_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(tau_mod)
        actualtimebounds_obs = TimeBounds(tau_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 Tauy in 'box' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod = PreProcessTS(
            tau_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        tau_obs, _, keyerror_obs = PreProcessTS(
            tau_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regridding
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tau_mod, tau_obs, method = TwoVarRegrid(tau_mod, tau_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsHorizontal(tau_mod, tau_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(tau_mod, tau_obs, axis="xy", centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tau_mod, weights=None, axis="xy", centered=1, biased=1))
        sm_std_obs = float(Std(tau_obs, weights=None, axis="xy", centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "spatialSTD": sm_std_mod, "description": "mean Tauy map"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "spatialSTD": sm_std_obs, "description": "mean Tauy map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                     "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error, "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tau_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tau_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauyLatRmse(taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                    taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs, box,
                    centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                    metname="", **kwargs):
    """
    The BiasTauyLatRmse() function computes the Tauy (meridional wind stress) meridional (latitude) root mean square
    error (RMSE) in a 'box' (usually 'equatorial_pacific_LatExt')

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Wed Dec  9 2020

    Inputs:
    ------
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Tauy
    :param taunamemod: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufilemod'
    :param tauareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for Tauy
    :param tauareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauareafilemod'
    :param taulandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for Tauy
    :param taulandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfilemod'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Tauy
    :param taunameobs: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufileobs'
    :param tauareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for Tauy
    :param tauareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauareafileobs'
    :param taulandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for Tauy
    :param taulandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for Tauy
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauyLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled Tauy file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed Tauy file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean Tauy meridional RMSE"
    units = "1e-3 N/m2"
    method = "Meridional root mean square error of " + box + " meridional wind stress (Tauy)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauyLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        tau_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, box, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, box, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(tau_mod.shape[0] / 12.))
        nbr_year_obs = int(round(tau_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(tau_mod)
        actualtimebounds_obs = TimeBounds(tau_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 Tauy averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod = PreProcessTS(
            tau_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        tau_obs, _, keyerror_obs = PreProcessTS(
            tau_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Zonal average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tau_mod, tau_obs, method = TwoVarRegrid(tau_mod, tau_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Zonal average
        tau_mod, keyerror_mod = AverageZonal(tau_mod)
        tau_obs, keyerror_obs = AverageZonal(tau_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(tau_mod, tau_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(tau_mod, tau_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tau_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tau_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess Tauy (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean Tauy map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean Tauy across latitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean Tauy across latitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tau_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tau_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauyLonRmse(taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                    taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs, box,
                    centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                    metname="", **kwargs):
    """
    The BiasTauyLonRmse() function computes the Tauy (meridional wind stress) zonal (longitude) root mean square error
    (RMSE) in a 'box' (usually the Equatorial Pacific)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Wed Dec  9 2020

    Inputs:
    ------
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Tauy
    :param taunamemod: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufilemod'
    :param tauareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for Tauy
    :param tauareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauareafilemod'
    :param taulandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for Tauy
    :param taulandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfilemod'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Tauy
    :param taunameobs: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufileobs'
    :param tauareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for Tauy
    :param tauareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauareafileobs'
    :param taulandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for Tauy
    :param taulandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for Tauy
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauyLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled Tauy file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed Tauy file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean Tauy zonal RMSE"
    units = "1e-3 N/m2"
    method = "Zonal root mean square error of " + box + " meridional wind stress (Tauy)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauyLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        tau_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, box, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, box, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(tau_mod.shape[0] / 12.))
        nbr_year_obs = int(round(tau_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(tau_mod)
        actualtimebounds_obs = TimeBounds(tau_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 Tauy averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod = PreProcessTS(
            tau_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        tau_obs, _, keyerror_obs = PreProcessTS(
            tau_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Meridional average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tau_mod, tau_obs, method = TwoVarRegrid(tau_mod, tau_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Meridional average
        tau_mod, keyerror_mod = AverageMeridional(tau_mod)
        tau_obs, keyerror_obs = AverageMeridional(tau_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(tau_mod, tau_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(tau_mod, tau_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tau_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tau_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess Tauy (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean Tauy map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean Tauy across longitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean Tauy across longitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tau_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tau_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasThfMapRmse(thffilemod, thfnamemod, thfareafilemod, thfareanamemod, thflandmaskfilemod, thflandmasknamemod,
                   thffileobs, thfnameobs, thfareafileobs, thfareanameobs, thflandmaskfileobs, thflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasThfMapRmse() function computes the THF (net heat flux) spatial root mean square error (RMSE) in a
    'box' (usually the tropical Pacific)
    The net heat flux is the sum of four term:
         - net surface shortwave radiation,
         - net surface longwave radiation,
         - latent heat flux,
         - sensible heat flux

    The net heat flux is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into thffile and thfname of this
    function a list() of the four needed files and variable names (CMIP: rsds-rsus, rlds-rlus, hfls, hfss)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param thffilemod: string
        path_to/filename of the file (NetCDF) of the modeled THF
    :param thfnamemod: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffilemod'
    :param thfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for THF
    :param thfareanamemod: string
        name of areacell variable (areacella, areacello) in 'thfareafilemod'
    :param thflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for THF
    :param thflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfilemod'
    :param thffileobs: string
        path_to/filename of the file (NetCDF) of the observed THF
    :param thfnameobs: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffileobs'
    :param thfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for THF
    :param thfareanameobs: string
        name of areacell variable (areacella, areacello) in 'thfareafileobs'
    :param thflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for THF
    :param thflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for THF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasThfMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean THF RMSE"
    units = "W/m2"
    method = "Spatial root mean square error of " + box + " net heat flux (THF)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasThfMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        thf_mod, areacell_mod, keyerror_mod = Read_data_mask_area_multifile(
            thffilemod, thfnamemod, "heat flux", "thf", metric, box, file_area=thfareafilemod, name_area=thfareanamemod,
            file_mask=thflandmaskfilemod, name_mask=thflandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
        thf_obs, areacell_obs, keyerror_obs = Read_data_mask_area_multifile(
            thffileobs, thfnameobs, "heat flux", "thf", metric, box, file_area=thfareafileobs, name_area=thfareanameobs,
            file_mask=thflandmaskfileobs, name_mask=thflandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(thf_mod.shape[0] / 12.))
        nbr_year_obs = int(round(thf_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(thf_mod)
        actualtimebounds_obs = TimeBounds(thf_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 THF in 'box' are normalized / detrended / smoothed (running average) if applicable
        thf_mod, method, keyerror_mod = PreProcessTS(
            thf_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        thf_obs, _, keyerror_obs = PreProcessTS(
            thf_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regridding
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            thf_mod, thf_obs, method = TwoVarRegrid(thf_mod, thf_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsHorizontal(thf_mod, thf_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(thf_mod, thf_obs, axis="xy", centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(thf_mod, weights=None, axis="xy", centered=1, biased=1))
        sm_std_obs = float(Std(thf_obs, weights=None, axis="xy", centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "spatialSTD": sm_std_mod, "description": "mean THF map"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "spatialSTD": sm_std_obs, "description": "mean THF map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                     "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error, "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=thf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=thf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasThfLatRmse(thffilemod, thfnamemod, thfareafilemod, thfareanamemod, thflandmaskfilemod, thflandmasknamemod,
                   thffileobs, thfnameobs, thfareafileobs, thfareanameobs, thflandmaskfileobs, thflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasThfLatRmse() function computes the THF (net heat flux) meridional (latitude) root mean square
    error (RMSE) in a 'box' (usually 'nino3_LatExt')
    The net heat flux is the sum of four term:
         - net surface shortwave radiation,
         - net surface longwave radiation,
         - latent heat flux,
         - sensible heat flux

    The net heat flux is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into thffile and thfname of this
    function a list() of the four needed files and variable names (CMIP: rsds-rsus, rlds-rlus, hfls, hfss)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param thffilemod: string
        path_to/filename of the file (NetCDF) of the modeled THF
    :param thfnamemod: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffilemod'
    :param thfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for THF
    :param thfareanamemod: string
        name of areacell variable (areacella, areacello) in 'thfareafilemod'
    :param thflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for THF
    :param thflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfilemod'
    :param thffileobs: string
        path_to/filename of the file (NetCDF) of the observed THF
    :param thfnameobs: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffileobs'
    :param thfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for THF
    :param thfareanameobs: string
        name of areacell variable (areacella, areacello) in 'thfareafileobs'
    :param thflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for THF
    :param thflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for THF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasThfLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean THF meridional RMSE"
    units = "W/m2"
    method = "Meridional root mean square error of " + box + " net heat flux (THF)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasThfLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        thf_mod, areacell_mod, keyerror_mod = Read_data_mask_area_multifile(
            thffilemod, thfnamemod, "heat flux", "thf", metric, box, file_area=thfareafilemod, name_area=thfareanamemod,
            file_mask=thflandmaskfilemod, name_mask=thflandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
        thf_obs, areacell_obs, keyerror_obs = Read_data_mask_area_multifile(
            thffileobs, thfnameobs, "heat flux", "thf", metric, box, file_area=thfareafileobs, name_area=thfareanameobs,
            file_mask=thflandmaskfileobs, name_mask=thflandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(thf_mod.shape[0] / 12.))
        nbr_year_obs = int(round(thf_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(thf_mod)
        actualtimebounds_obs = TimeBounds(thf_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 THF averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        thf_mod, method, keyerror_mod = PreProcessTS(
            thf_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        thf_obs, _, keyerror_obs = PreProcessTS(
            thf_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Zonal average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            thf_mod, thf_obs, method = TwoVarRegrid(thf_mod, thf_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Zonal average
        thf_mod, keyerror_mod = AverageZonal(thf_mod)
        thf_obs, keyerror_obs = AverageZonal(thf_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(thf_mod, thf_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(thf_mod, thf_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(thf_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(thf_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                thffilemod, thfnamemod, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                file_area=thfareafilemod, name_area=thfareanamemod, file_mask=thflandmaskfilemod,
                name_mask=thflandmasknamemod, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                thffileobs, thfnameobs, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                file_area=thfareafileobs, name_area=thfareanameobs, file_mask=thflandmaskfileobs,
                name_mask=thflandmasknameobs, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess THF (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean THF map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean THF across latitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean THF across latitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=thf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=thf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasThfLonRmse(thffilemod, thfnamemod, thfareafilemod, thfareanamemod, thflandmaskfilemod, thflandmasknamemod,
                   thffileobs, thfnameobs, thfareafileobs, thfareanameobs, thflandmaskfileobs, thflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasThfLonRmse() function computes the THF (net heat flux) zonal (longitude) root mean square error
    (RMSE) in a 'box' (usually the Equatorial Pacific)
    The net heat flux is the sum of four term:
         - net surface shortwave radiation,
         - net surface longwave radiation,
         - latent heat flux,
         - sensible heat flux

    The net heat flux is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into thffile and thfname of this
    function a list() of the four needed files and variable names (CMIP: rsds-rsus, rlds-rlus, hfls, hfss)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param thffilemod: string
        path_to/filename of the file (NetCDF) of the modeled THF
    :param thfnamemod: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffilemod'
    :param thfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for THF
    :param thfareanamemod: string
        name of areacell variable (areacella, areacello) in 'thfareafilemod'
    :param thflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for THF
    :param thflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfilemod'
    :param thffileobs: string
        path_to/filename of the file (NetCDF) of the observed THF
    :param thfnameobs: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffileobs'
    :param thfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for THF
    :param thfareanameobs: string
        name of areacell variable (areacella, areacello) in 'thfareafileobs'
    :param thflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for THF
    :param thflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for THF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'TropFlux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasThfLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "time mean THF zonal RMSE"
    units = "W/m2"
    method = "Zonal root mean square error of " + box + " net heat flux (THF)"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasThfLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        thf_mod, areacell_mod, keyerror_mod = Read_data_mask_area_multifile(
            thffilemod, thfnamemod, "heat flux", "thf", metric, box, file_area=thfareafilemod, name_area=thfareanamemod,
            file_mask=thflandmaskfilemod, name_mask=thflandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
        thf_obs, areacell_obs, keyerror_obs = Read_data_mask_area_multifile(
            thffileobs, thfnameobs, "heat flux", "thf", metric, box, file_area=thfareafileobs, name_area=thfareanameobs,
            file_mask=thflandmaskfileobs, name_mask=thflandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(thf_mod.shape[0] / 12.))
        nbr_year_obs = int(round(thf_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(thf_mod)
        actualtimebounds_obs = TimeBounds(thf_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 THF averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        thf_mod, method, keyerror_mod = PreProcessTS(
            thf_mod, method, areacell=areacell_mod, average="time", region=box, **kwargs)
        thf_obs, _, keyerror_obs = PreProcessTS(
            thf_obs, "", areacell=areacell_obs, average="time", region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Meridional average
        # ------------------------------------------------
        # 3.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            thf_mod, thf_obs, method = TwoVarRegrid(thf_mod, thf_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.2 Meridional average
        thf_mod, keyerror_mod = AverageMeridional(thf_mod)
        thf_obs, keyerror_obs = AverageMeridional(thf_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                          "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(thf_mod, thf_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(thf_mod, thf_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(thf_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(thf_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                thffilemod, thfnamemod, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                file_area=thfareafilemod, name_area=thfareanamemod, file_mask=thflandmaskfilemod,
                name_mask=thflandmasknamemod, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, interpreter="project_interpreter_mod_var1", **kwargs)
            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                thffileobs, thfnameobs, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                file_area=thfareafileobs, name_area=thfareanameobs, file_mask=thflandmaskfileobs,
                name_mask=thflandmasknameobs, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, interpreter="project_interpreter_obs_var1", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess THF (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=mod_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=obs_areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del mod_areacell, obs_areacell
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean THF map")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": "mean THF across longitudes", "arraySTD": sm_std_obs}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": "mean THF across longitudes", "arraySTD": sm_std_obs}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=thf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=thf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoAmpl(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, dataset="",
             debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoAmpl() function computes the standard deviation of 'sstbox' SSTA (sea surface temperature anomalies)
    (usually the standard deviation of nino3.4 SSTA)

    Author:	Eric Guilyardi : Eric.Guilyardi@locean-ipsl.upmc.fr
    Co-author: Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon Jan  9 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box ('nino3.4') for SST
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoAmpl_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "ENSO SSTA amplitude"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "C"
    method = "Standard deviation of " + sstbox + " averaged sea surface temperature anomalies (SSTA)"
    ref = "Using CDAT regression calculation"
    metric = "EnsoAmpl"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error = None, None
    dive_down_diag, actualtimebounds, keyerror, nbr_year = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        del sst_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                          "shape1": "(sst) " + str(sst.shape), "time1": "(sst) " + str(TimeBounds(sst))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation (diagnostic value)
        # ------------------------------------------------
        # 3.1 Computes the standard deviation
        mv = float(Std(sst))
        # 3.2 Standard Error of the Standard Deviation (function of npts)
        mv_error = mv / NUMPYsqrt(2 * (float(len(sst)) - 1))

        # ------------------------------------------------
        # 4. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, sst_areacell1, keyerror1 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            sst_hov, sst_areacell2, keyerror2 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            sst_map, _, keyerror1 = PreProcessTS(
                sst_map, "", areacell=sst_areacell1, compute_anom=True, region="equatorial_pacific_LatExt2", **kwargs)
            sst_hov, _, keyerror2 = PreProcessTS(
                sst_hov, "", areacell=sst_areacell2, compute_anom=True, region="equatorial_pacific", **kwargs)
            del sst_areacell1, sst_areacell2
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(sst hov) " + str([ax.id for ax in sst_hov.getAxisList()]),
                              "shape1": "(sst map) " + str(sst_map.shape), "shape2": "(sst hov) " + str(sst_hov.shape),
                              "time1": "(sst map) " + str(TimeBounds(sst_map)),
                              "time2": "(sst hov) " + str(TimeBounds(sst_hov))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 10, **dict_debug)
            # Standard deviation
            sst_map = Std(sst_map)
            sst_hov = Std(sst_hov)
            if debug is True:
                dict_debug = {"axes1": "(sst map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(sst hov) " + str([ax.id for ax in sst_hov.getAxisList()]),
                              "shape1": "(sst map) " + str(sst_map.shape), "shape2": "(sst hov) " + str(sst_hov.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std: netcdf", 10, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
            sst_hov = Regrid(sst_hov, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(sst map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(sst hov) " + str([ax.id for ax in sst_hov.getAxisList()]),
                              "shape1": "(sst map) " + str(sst_map.shape), "shape2": "(sst hov) " + str(sst_hov.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 10, **dict_debug)
            # Meridional average
            sst_hov, keyerror = AverageMeridional(sst_hov)
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst hov) " + str([ax.id for ax in sst_hov.getAxisList()]),
                              "shape1": "(sst hov) " + str(sst_hov.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 10, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            lat = ReferenceRegions("equatorial_pacific")["latitude"]
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error,
                     "description": "zonal curve of standard deviation of equatorial_pacific SSTA (meridional " +
                                    "averaged [" + str(lat[0]) + " ; " + str(lat[1]) + "])"}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "map of standard deviation of tropical Pacific SSTA"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3, var1=sst_hov, var1_attributes=dict1,
                       var1_name=ovar[0] + dataset, var2=sst_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": nbr_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSstSkew(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, dataset="",
                debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstSkew() function computes the skewness of 'sstbox' SSTA (sea surface temperature anomalies) (usually the
    skewness of nino3.4 SSTA)
    The standard error on the coefficient of skewness is based on Fisher (1930; https://doi.org/10.1098/rspa.1930.0185)

    Author:	Eric Guilyardi : Eric.Guilyardi@locean-ipsl.upmc.fr
    Co-author: Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon Jan  9 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box ('nino3.4') for SST
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstSkew_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "ENSO SSTA skewness"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "C"
    method = "Skewness of " + sstbox + " averaged sea surface temperature anomalies (SSTA)"
    ref = "Using CDAT regression calculation"
    metric = "EnsoSstSkew"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error = None, None
    dive_down_diag, actualtimebounds, keyerror, nbr_year = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        del sst_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                          "shape1": "(sst) " + str(sst.shape), "time1": "(sst) " + str(TimeBounds(sst))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Skewness (diagnostic value)
        # ------------------------------------------------
        # 3.1 Computes the skewness
        mv = float(SkewnessTemporal(sst))
        # 3.2 Standard Error of the skewness (function of npts)
        npts = float(len(sst))
        mv_error = NUMPYsqrt(6 * npts * (npts - 1) / ((npts - 2) * (npts + 1) * (npts + 3)))

        # ------------------------------------------------
        # 4. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, sst_areacell1, keyerror1 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            sst_hov, sst_areacell2, keyerror2 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            sst_map, _, keyerror1 = PreProcessTS(
                sst_map, "", areacell=sst_areacell1, compute_anom=True, region="equatorial_pacific_LatExt2", **kwargs)
            sst_hov, _, keyerror2 = PreProcessTS(
                sst_hov, "", areacell=sst_areacell2, compute_anom=True, region="equatorial_pacific", **kwargs)
            del sst_areacell1, sst_areacell2
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(sst hov) " + str([ax.id for ax in sst_hov.getAxisList()]),
                              "shape1": "(sst map) " + str(sst_map.shape), "shape2": "(sst hov) " + str(sst_hov.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 10, **dict_debug)
            # Skewness
            sst_map = SkewnessTemporal(sst_map)
            sst_hov = SkewnessTemporal(sst_hov)
            if debug is True:
                dict_debug = {"axes1": "(sst map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(sst hov) " + str([ax.id for ax in sst_hov.getAxisList()]),
                              "shape1": "(sst map) " + str(sst_map.shape), "shape2": "(sst hov) " + str(sst_hov.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SkewnessTemporal: netcdf", 10, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
            sst_hov = Regrid(sst_hov, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(sst map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(sst hov) " + str([ax.id for ax in sst_hov.getAxisList()]),
                              "shape1": "(sst map) " + str(sst_map.shape), "shape2": "(sst hov) " + str(sst_hov.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 10, **dict_debug)
            # Meridional average
            sst_hov, keyerror = AverageMeridional(sst_hov)
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst hov) " + str([ax.id for ax in sst_hov.getAxisList()]),
                              "shape1": "(sst hov) " + str(sst_hov.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 10, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            lat = ReferenceRegions("equatorial_pacific")["latitude"]
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error,
                     "description": "zonal curve of skewness of equatorial_pacific SSTA (meridional " +
                                    "averaged [" + str(lat[0]) + " ; " + str(lat[1]) + "])"}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "map of skewness of tropical Pacific SSTA"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3, var1=sst_hov, var1_attributes=dict1,
                       var1_name=ovar[0] + dataset, var2=sst_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": nbr_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSeasonality(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, dataset="",
                    debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSeasonality() function computes ratio between the November-December-January (NDJ) and March-April-May (MAM)
    average standard deviation of 'sstbox' SSTA (sea surface temperature anomalies) (usually nino3.4 SSTA)

    Author:	Eric Guilyardi : Eric.Guilyardi@locean-ipsl.upmc.fr
    Co-author: Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon Jan  9 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box ('nino3.4') for SST
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSeasonality_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "ENSO SSTA seasonality"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "C/C"
    method = "Ratio between NDJ and MAM standard deviation " + sstbox + " sea surface temperature anomalies (SSTA)"
    ref = "Using CDAT std dev calculation"
    metric = "EnsoSeasonality"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error = None, None
    dive_down_diag, actualtimebounds, keyerror, nbr_year = {"value": None, "axis": None}, None, None, None
    
    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        sst_ts, method, keyerror = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", region=sstbox, **kwargs)
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_ts.getAxisList()]),
                          "shape1": "(sst) " + str(sst_ts.shape), "time1": "(sst) " + str(TimeBounds(sst_ts))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean
        # ------------------------------------------------
        # 3.1 Seasonal mean
        sst_NDJ = SeasonalMean(sst_ts, "NDJ", compute_anom=True)
        sst_MAM = SeasonalMean(sst_ts, "MAM", compute_anom=True)
        if debug is True:
            dict_debug = {"axes1": "(sst_NDJ) " + str([ax.id for ax in sst_NDJ.getAxisList()]),
                          "axes2": "(sst_NDJ) " + str([ax.id for ax in sst_MAM.getAxisList()]),
                          "shape1": "(sst_NDJ) " + str(sst_NDJ.shape), "shape2": "(sst_NDJ) " + str(sst_MAM.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Standard deviation (diagnostic value)
        # ------------------------------------------------
        # 4.1 Computes the standard deviation
        sst_NDJ_std = float(Std(sst_NDJ))
        sst_MAM_std = float(Std(sst_MAM))
        mv = sst_NDJ_std / sst_MAM_std
        # 4.2 Standard Error of the Standard Deviation (function of npts)
        NDJ_err = sst_NDJ_std / NUMPYsqrt(2 * (float(len(sst_NDJ)) - 1))
        MAM_err = sst_MAM_std / NUMPYsqrt(2 * (float(len(sst_MAM)) - 1))
        # 4.3 The error (dy) on ratio ('y = x/z'): dy = (z*dx + x*dz) / z2
        mv_error = float((sst_MAM_std * NDJ_err + sst_NDJ_std * MAM_err) / NUMPYsquare(sst_MAM_std))

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map1, sst_map_areacell1, keyerror1 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            sst_map2, sst_map_areacell2, keyerror2 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            sst1, _, keyerror1 = PreProcessTS(
                sst, "", areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
            sst2, _, keyerror2 = PreProcessTS(
                sst_map1, "", areacell=sst_map_areacell1, region="equatorial_pacific_LatExt2", **kwargs)
            sst3, _, keyerror3 = PreProcessTS(
                sst_map2, "", areacell=sst_map_areacell2, region="equatorial_pacific", **kwargs)
            sst4, _, keyerror4 = PreProcessTS(
                sst_map2, "", areacell=sst_map_areacell2, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2, keyerror3, keyerror4])
            del sst_areacell, sst_map_areacell1, sst_map_areacell2
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {
                    "axes1": "(sst1) " + str([ax.id for ax in sst1.getAxisList()]),
                    "axes2": "(sst2) " + str([ax.id for ax in sst2.getAxisList()]),
                    "axes3": "(sst3) " + str([ax.id for ax in sst3.getAxisList()]),
                    "axes4": "(sst4) " + str([ax.id for ax in sst4.getAxisList()]),
                    "shape1": "(sst1) " + str(sst1.shape), "shape2": "(sst2) " + str(sst2.shape),
                    "shape3": "(sst3) " + str(sst3.shape), "shape4": "(sst4) " + str(sst4.shape),
                    "time1": "(sst1) " + str(TimeBounds(sst1)), "time2": "(sst2) " + str(TimeBounds(sst2)),
                    "time3": "(sst3) " + str(TimeBounds(sst3)), "time4": "(sst4) " + str(TimeBounds(sst4))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 10, **dict_debug)
            # Seasonal mean
            sst2_NDJ = SeasonalMean(sst2, "NDJ", compute_anom=True)
            sst2_MAM = SeasonalMean(sst2, "MAM", compute_anom=True)
            sst4_NDJ = SeasonalMean(sst4, "NDJ", compute_anom=True)
            sst4_MAM = SeasonalMean(sst4, "MAM", compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst2_NDJ) " + str([ax.id for ax in sst2_NDJ.getAxisList()]),
                    "axes2": "(sst2_MAM) " + str([ax.id for ax in sst2_MAM.getAxisList()]),
                    "axes3": "(sst4_NDJ) " + str([ax.id for ax in sst4_NDJ.getAxisList()]),
                    "axes4": "(sst4_MAM) " + str([ax.id for ax in sst4_MAM.getAxisList()]),
                    "shape1": "(sst2_NDJ) " + str(sst2_NDJ.shape), "shape2": "(sst2_MAM) " + str(sst2_MAM.shape),
                    "shape3": "(sst4_NDJ) " + str(sst4_NDJ.shape), "shape4": "(sst4_MAM) " + str(sst4_MAM.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean: netcdf", 15, **dict_debug)
            # Standard deviation
            sst1_std = StdMonthly(sst1)
            sst2_NDJ = Std(sst2_NDJ)
            sst2_MAM = Std(sst2_MAM)
            sst3_hov = StdMonthly(sst3)
            sst4_NDJ = Std(sst4_NDJ)
            sst4_MAM = Std(sst4_MAM)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst1_std) " + str([ax.id for ax in sst1_std.getAxisList()]),
                    "axes2": "(sst2_NDJ) " + str([ax.id for ax in sst2_NDJ.getAxisList()]),
                    "axes3": "(sst2_MAM) " + str([ax.id for ax in sst2_MAM.getAxisList()]),
                    "axes4": "(sst3_hov) " + str([ax.id for ax in sst3_hov.getAxisList()]),
                    "axes5": "(sst4_NDJ) " + str([ax.id for ax in sst4_NDJ.getAxisList()]),
                    "axes6": "(sst4_MAM) " + str([ax.id for ax in sst4_MAM.getAxisList()]),
                    "shape1": "(sst1_std) " + str(sst1_std.shape), "shape2": "(sst2_NDJ) " + str(sst2_NDJ.shape),
                    "shape3": "(sst2_MAM) " + str(sst2_MAM.shape), "shape4": "(sst3_hov) " + str(sst3_hov.shape),
                    "shape5": "(sst4_NDJ) " + str(sst4_NDJ.shape), "shape6": "(sst4_MAM) " + str(sst4_MAM.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst2_NDJ = Regrid(sst2_NDJ, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
            sst2_MAM = Regrid(sst2_MAM, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
            sst3_hov = Regrid(sst3_hov, None, region="equatorial_pacific", **kwargs["regridding"])
            sst4_NDJ = Regrid(sst4_NDJ, None, region="equatorial_pacific", **kwargs["regridding"])
            sst4_MAM = Regrid(sst4_MAM, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {
                    "axes1": "(sst2_NDJ) " + str([ax.id for ax in sst2_NDJ.getAxisList()]),
                    "axes2": "(sst2_MAM) " + str([ax.id for ax in sst2_MAM.getAxisList()]),
                    "axes3": "(sst3_hov) " + str([ax.id for ax in sst3_hov.getAxisList()]),
                    "axes4": "(sst4_NDJ) " + str([ax.id for ax in sst4_NDJ.getAxisList()]),
                    "axes5": "(sst4_MAM) " + str([ax.id for ax in sst4_MAM.getAxisList()]),
                    "shape1": "(sst2_NDJ) " + str(sst2_NDJ.shape), "shape2": "(sst2_MAM) " + str(sst2_MAM.shape),
                    "shape3": "(sst3_hov) " + str(sst3_hov.shape), "shape4": "(sst4_NDJ) " + str(sst4_NDJ.shape),
                    "shape5": "(sst4_MAM) " + str(sst4_MAM.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            sst3_hov, keyerror1 = AverageMeridional(sst3_hov)
            sst4_NDJ, keyerror2 = AverageMeridional(sst4_NDJ)
            sst4_MAM, keyerror3 = AverageMeridional(sst4_MAM)
            keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst3_hov) " + str([ax.id for ax in sst3_hov.getAxisList()]),
                              "axes2": "(sst4_NDJ) " + str([ax.id for ax in sst4_NDJ.getAxisList()]),
                              "axes3": "(sst4_MAM) " + str([ax.id for ax in sst4_MAM.getAxisList()]),
                              "shape1": "(sst3_hov) " + str(sst3_hov.shape),
                              "shape2": "(sst4_NDJ) " + str(sst4_NDJ.shape),
                              "shape3": "(sst4_MAM) " + str(sst4_MAM.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Save netCDF
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "C"
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": my_units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "monthly standard deviation of " + sstbox + " SSTA",
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error}
            dict2 = {"units": my_units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of monthly standard deviation of " +
                                    "equatorial_pacific SSTA"}
            dict3 = {"units": my_units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "zonal curve of standard deviation of equatorial_pacific SSTA (during NDJ)"}
            dict4 = {"units": my_units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "zonal curve of standard deviation of equatorial_pacific sstA (during MAM)"}
            dict5 = {"units": my_units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "map of standard deviation of equatorial_pacific SSTA (during NDJ)"}
            dict6 = {"units": my_units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "map of standard deviation of equatorial_pacific SSTA (during MAM)"}
            dict7 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict7,
                       var1=sst1_std, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                       var2=sst3_hov, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                       var3=sst4_NDJ, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=sst4_MAM, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=sst2_NDJ, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=sst2_MAM, var6_attributes=dict6, var6_name=ovar[5] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears": nbr_year, "time_frequency": kwargs["frequency"], "time_period": actualtimebounds,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSstDiversity(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box,
                     event_definition, dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstDiversity() function computes a zonal composite of El Nino and La Nina events during the peak of the
    event.
        1.) detect events
            1.1) SSTA (sea surface temperature anomalies) averaged in 'region_ev' are normalized / detrended / smoothed
                 (running average) if applicable
            1.2) SSTA > 'threshold' (SSTA < -'threshold') during 'season' are considered as El Nino (La Nina) events
        2.) diversity of the zonal location of the maximum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the maximum SSTA for each selected event
            2.3) compute the spread of the distribution

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Fri Nov 23 2018

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstDiversity_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, ref, keyerror,
        dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        enso_method = " ("
        if isinstance(smooth_ev, dict) is True:
            enso_method += "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            if isinstance(length_ev, int) is True:
                enso_method += "; "
        if isinstance(length_ev, int) is True:
            enso_method += "threshold met during at least " + str(length_ev) + " consecutive months"
        enso_method += ")"
    else:
        enso_method = ""
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(box)["latitude"]
    name = "ENSO SSTA diversity (interquartile range)"
    units = "long"
    method = "Nino (Nina) events = " + region_ev + " sea surface temperature anomalies > " + my_thresh + " (< -" + \
             my_thresh + ") during " + season_ev + enso_method + ", zonal SSTA (meridional averaged [" + str(lat[0]) + \
             " ; " + str(lat[1]) + "]), zonal location of the maximum (minimum) SSTA detected for each event, the " + \
             "diversity is the interquartile range (IQR = Q3 - Q1)"
    ref = "Using CDAT regridding"
    metric = "EnsoSstDiversity"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv1, mv1_err, mv2, mv2_err, dive_down_diag = None, None, None, None, {"value": None, "axis": None}
    mv3, mv3_err, mv4, mv4_err, mv5, mv5_err, mv6, mv6_err = None, None, None, None, None, None, None, None
    ln_years, en_years, actualtimebounds, keyerror, nbr_year = list(), list(), None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, region_ev, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        sst_map, sst_map_areacell, keyerror2 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, box, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        enso_sst, _, keyerror1 = PreProcessTS(
            sst, "", areacell=sst_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_map, method, keyerror2 = PreProcessTS(sst_map, method, areacell=sst_map_areacell, region=box, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del sst_areacell, sst_map_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(enso_sst) " + str([ax.id for ax in enso_sst.getAxisList()]),
                          "axes2": "(sst_map) " + str([ax.id for ax in sst_map.getAxisList()]),
                          "shape1": "(enso_sst) " + str(enso_sst.shape), "shape2": "(sst map) " + str(sst_map.shape),
                          "time1": "(enso_sst) " + str(TimeBounds(enso_sst)),
                          "time2": "(sst_map) " + str(TimeBounds(sst_map))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
        # 2.2 SSTA > 'threshold' (SSTA < -'threshold') during 'season' are considered as El Nino (La Nina) events
        # Lists event years
        ln_years = DetectEvents(enso_sst, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                compute_season=True, duration=length_ev)
        en_years = DetectEvents(enso_sst, season_ev, thresh_ev, normalization=normalize, nino=True,
                                compute_season=True, duration=length_ev)
        if debug is True:
            dict_debug = {"nina1": "nbr(" + str(len(ln_years)) + "): " + str(ln_years),
                          "nino1": "nbr(" + str(len(en_years)) + "): " + str(en_years)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean, meridional average, position of max(|SSTA|)
        # ------------------------------------------------
        # 3.1 Seasonal mean and seasonal anomalies
        sst_map = SeasonalMean(sst_map, season_ev, compute_anom=True)
        if debug is True:
            dict_debug = {"axes1": "(sst_map) " + str([ax.id for ax in sst_map.getAxisList()]),
                          "shape1": "(sst_map) " + str(sst_map.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"newgrid", "missing", "order", "mask", "newgrid_name", "regridder", "regridTool",
                          "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            sst_map = Regrid(sst_map, None, region=box, **kwargs["regridding"])
            method += ", dataset regridded to " + str(kwargs["regridding"]["newgrid_name"])
            if debug is True:
                dict_debug = {"axes1": "(sst_map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "shape1": "(sst_map) " + str(sst_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Meridional average
        sst_lon, keyerror = AverageMeridional(sst_map)
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst_lon) " + str([ax.id for ax in sst_lon.getAxisList()]),
                          "shape1": "(sst_lon) " + str(sst_lon.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
        # 3.3 position of max(|SSTA|)
        if len(ln_years) > 0:
            # samples
            ln_sample = Event_selection(sst_lon, kwargs["frequency"], list_event_years=ln_years)
            # find the zonal position of the minimum SSTA for each selected event
            ln_lon = FindXYMinMaxInTs(ln_sample, return_val="mini", smooth=True, axis=0, window=19, method="triangle")
            method += ", zonal SSTA smoothed using a triangle shaped window of 19 points"
            if debug is True:
                dict_debug = {"line1": "longitude of the minimum SSTA: " + str(ln_lon)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after FindXYMinMaxInTs", 15, **dict_debug)
        else:
            ln_sample = MyEmpty(sst_lon[:5, 0], time=True, time_id="years")
            ln_lon = MyEmpty(sst_lon[:5, 0], time=True, time_id="years")
        if len(en_years) > 0:
            # samples
            en_sample = Event_selection(sst_lon, kwargs["frequency"], list_event_years=en_years)
            # find the zonal position of the maximum SSTA for each selected event
            en_lon = FindXYMinMaxInTs(en_sample, return_val="maxi", smooth=True, axis=0, window=19, method="triangle")
            if debug is True:
                dict_debug = {"line1": "longitude of the maximum SSTA: " + str(en_lon)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after FindXYMinMaxInTs", 15, **dict_debug)
        else:
            en_sample = MyEmpty(sst_lon[:5, 0], time=True, time_id="years")
            en_lon = MyEmpty(sst_lon[:5, 0], time=True, time_id="years")

        # ------------------------------------------------
        # 4. Statistical dispersion (diagnostic value and supplementary diagnostic values)
        # ------------------------------------------------
        if len(ln_years) > 0 and len(en_years) > 0:
            enso_sample = Concatenate(-1 * ln_sample, en_sample, events1=ln_years, events2=en_years)
            enso_lon = Concatenate(ln_lon, en_lon, events1=ln_years, events2=en_years)
            # 4.1 compute the spread of the distribution of both El Nino and La Nina events
            mv1 = statistical_dispersion(enso_lon, method="IQR")
            mv2 = statistical_dispersion(enso_lon, method="MAD")
            mv1_err, mv2_err = None, None
        else:
            if len(ln_years) > 0:
                enso_sample = -1 * deepcopy(ln_sample)
                enso_lon = deepcopy(ln_lon)
            else:
                enso_sample = deepcopy(en_sample)
                enso_lon = deepcopy(en_lon)
        if len(ln_years) > 0:
            # 4.2 compute the spread of the distribution of La Nina events
            mv3 = statistical_dispersion(ln_lon, method="IQR")
            mv4 = statistical_dispersion(ln_lon, method="MAD")
            mv3_err, mv4_err = None, None
        if len(en_years) > 0:
            # 4.3 compute the spread of the distribution of El Nino events
            mv5 = statistical_dispersion(en_lon, method="IQR")
            mv6 = statistical_dispersion(en_lon, method="MAD")
            mv5_err, mv6_err = None, None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "nina_years": str(ln_years), "nino_years": str(en_years),
                     "description": "zonal location of maximum SSTA (-1 * Nina sstA) during both El Nino and La " +
                                    "Nina; " + season_ev + " SSTA averaged in [" + str(lat[0]) + " ; " + str(lat[1]) +
                                    "]; Nino (Nina) events = " + region_ev + " SSTA > " + my_thresh + " (< -" +
                                    my_thresh + ") during " + season_ev + enso_method,
                     "diagnostic_value_" + dataset: mv1, "diagnostic_value_error_" + dataset: mv1_err,
                     "diagnostic_value2_" + dataset: mv2, "diagnostic_value_error2_" + dataset: mv2_err}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "nina_years": str(ln_years),
                     "description": "zonal location of maximum SSTA (-1 * Nina sstA) during La Nina; " + season_ev +
                                    " SSTA averaged in [" + str(lat[0]) + " ; " + str(lat[1]) + "]; Nina events = " +
                                    region_ev + " SSTA < -" + my_thresh + ") during " + season_ev + enso_method,
                     "diagnostic_value_" + dataset: mv3, "diagnostic_value_error_" + dataset: mv3_err,
                     "diagnostic_value2_" + dataset: mv4, "diagnostic_value_error2_" + dataset: mv4_err}
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "nino_years": str(en_years),
                     "description": "zonal location of maximum SSTA during El Nino; " + season_ev + " SSTA averaged " +
                                    "in [" + str(lat[0]) + " ; " + str(lat[1]) + "]; Nino events = " + region_ev +
                                    " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                     "diagnostic_value_" + dataset: mv5, "diagnostic_value_error_" + dataset: mv5_err,
                     "diagnostic_value2_" + dataset: mv6, "diagnostic_value_error2_" + dataset: mv6_err}
            dict4 = {"units": "C", "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "nina_years": str(ln_years), "nino_years": str(en_years),
                     "description": "zonal SSTA (-1 * Nina SSTA) during both El Nino and La Nina; " + season_ev +
                                    " SSTA averaged in [" + str(lat[0]) + " ; " + str(lat[1]) + "]; Nino (Nina) " +
                                    "events = " + region_ev + " sstA > " + my_thresh + " (< -" + my_thresh + ") during "
                                    + season_ev + enso_method}
            dict5 = {"units": "C", "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "nina_years": str(ln_years),
                     "description": "zonal SSTA (-1 * Nina SSTA) during La Nina; " + season_ev + " SSTA averaged in [" +
                                    str(lat[0]) + " ; " + str(lat[1]) + "]; Nina events = " + region_ev + " SSTA < -" +
                                    my_thresh + ") during " + season_ev + enso_method}
            dict6 = {"units": "C", "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "nino_years": str(en_years),
                     "description": "zonal SSTA during El Nino; " + season_ev + " SSTA averaged in [" + str(lat[0]) +
                                    " ; " + str(lat[1]) + "]; Nino " + "events = " + region_ev + " SSTA > " + my_thresh
                                    + " during " + season_ev + enso_method}
            dict7 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict7,
                       var1=enso_lon, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                       var2=enso_sample, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                       var3=ln_lon, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=ln_sample, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=en_lon, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=en_sample, var6_attributes=dict6, var6_name=ovar[5] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, file_name
    # Metric value
    if debug is True:
        dict_debug = {
            "line1": "diagnostic (IQR) value: " + str(mv1), "line2": "diagnostic (IQR) value_error: " + str(mv1_err),
            "line3": "diagnostic (MAD) value: " + str(mv2), "line4": "diagnostic (MAD) value_error: " + str(mv2_err)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv1, "value_error": mv1_err, "value2": mv2, "value_error2": mv2_err, "units": units,
        "method": method, "nyears": nbr_year, "events": ln_years + en_years, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoDuration(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
                 nbr_years_window, dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoDuration() function computes SSTA (sea surface temperature anomalies) life cycle associated with ENSO in a
    'box' (usually the nino3.4) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and
    lagging ENSO), the duration is then the period, around the peak, during which the life cycle is above 0.25.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Fri Nov 23 2018

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box (e.g. 'nino3.4') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoDuration_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        enso_method = " ("
        if isinstance(smooth_ev, dict) is True:
            enso_method += "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            if isinstance(length_ev, int) is True:
                enso_method += "; "
        if isinstance(length_ev, int) is True:
            enso_method += "threshold met during at least " + str(length_ev) + " consecutive months"
        enso_method += ")"
    else:
        enso_method = ""
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "ENSO duration based on life cyle SSTA pattern"
    units = "months"
    method = "sea surface temperature anomalies (SSTA) during " + str(nbr_years_window) + " years (centered on " + \
             "ENSO) regressed onto " + region_ev + " SSTA during " + season_ev + ", the duration is the number of " + \
             "consecutive months during which the regression is above 0.25"
    ref = "Using CDAT"
    metric = "EnsoDuration"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error = None, None
    dive_down_diag, actualtimebounds, keyerror, nbr_year = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, region_ev, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess ENSO SSTA
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        enso_sst, _, keyerror = PreProcessTS(
            sst, "", areacell=sst_areacell, average="horizontal", region=region_ev, **kwargs)
        kwargs["smoothing"] = smooth
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": str([ax.id for ax in enso_sst.getAxisList()]), "shape1": str(enso_sst.shape),
                          "time1": str(TimeBounds(enso_sst))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
        # 2.2 Seasonal mean and anomalies
        enso = SeasonalMean(enso_sst, season_ev, compute_anom=True)
        if debug is True:
            dict_debug = {"axes1": str([ax.id for ax in enso.getAxisList()]),
                          "shape1": str(enso.shape), "time1": str(TimeBounds(enso))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Preprocess temporal SSTA
        # ------------------------------------------------
        # 3.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=region_ev, **kwargs)
        del sst_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": str([ax.id for ax in sst.getAxisList()]), "shape1": str(sst.shape),
                          "time1": str(TimeBounds(sst))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Regression time series
        # ------------------------------------------------
        # 4.1 Regress SSTA time series onto ENSO SSTA
        sst_ts = LinearRegressionTsAgainstTs(
            sst, enso, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        if debug is True:
            dict_debug = {"axes1": str([ax.id for ax in sst_ts.getAxisList()]), "shape1": str(sst_ts.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Duration (diagnostic value)
        # ------------------------------------------------
        # 5.1 Count the number of consecutive month above a threshold
        mv = float(DurationEvent(sst_ts, 0.25, nino=True, debug=debug))
        mv_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Lists event years
            nina_years = DetectEvents(enso_sst, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                      compute_season=True, duration=length_ev)
            nino_years = DetectEvents(enso_sst, season_ev, thresh_ev, normalization=normalize, nino=True,
                                      compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "nbr(" + str(len(nina_years)) + "): " + str(nina_years),
                              "nino1": "nbr(" + str(len(nino_years)) + "): " + str(nino_years)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            lth = 0.9 * thresh_ev
            if normalize is True:
                lth = lth * float(Std(enso))
            if len(nina_years) > 0:
                sample = Event_selection(
                    sst, kwargs["frequency"], nbr_years_window=nbr_years_window, list_event_years=nina_years)
                # Count the number of consecutive month bellow a threshold
                nina_dur_all = DurationAllEvent(sample, -lth, nino=False, debug=debug)
                nina_dur_mean = float(nina_dur_all.mean())
                nina_dur_err = float(Std(nina_dur_all) / NUMPYsqrt(len(nina_dur_all)))
            else:
                nina_dur_all = MyEmpty(sst[:5], time=True, time_id="years")
                nina_dur_mean = None
                nina_dur_err = None
            if len(nino_years) > 0:
                sample = Event_selection(
                    sst, kwargs["frequency"], nbr_years_window=nbr_years_window, list_event_years=nino_years)
                # Count the number of consecutive month above a threshold
                nino_dur_all = DurationAllEvent(sample, lth, nino=True, debug=debug)
                nino_dur_mean = float(nino_dur_all.mean())
                nino_dur_err = float(Std(nino_dur_all) / NUMPYsqrt(len(nino_dur_all)))
            else:
                nino_dur_all = MyEmpty(sst[:5], time=True, time_id="years")
                nino_dur_mean = None
                nino_dur_err = None
            if debug is True:
                dict_debug = {"nina1": "mean(" + str(nina_dur_mean) + "): " + str(nina_dur_all),
                              "nino1": "mean(" + str(nino_dur_mean) + "): " + str(nino_dur_all)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DurationAllEvent: netcdf", 15, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            my_thresh = "std" if normalize is True else "C"
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "time series of SSTA during " + str(nbr_years_window) + " years (centered on " +
                                    "ENSO) regressed onto " + region_ev + " SSTA during " + season_ev,
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "nina_years": str(nina_years),
                     "description": "duration of La Nina events; Nina events = " + region_ev + " SSTA < -" + my_thresh +
                                    " during " + season_ev + enso_method + ", duration is the number of consecutive " +
                                    "months during which SSTA < -" + my_thresh,
                     "diagnostic_value": nina_dur_mean, "diagnostic_value_error": nina_dur_err}
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "nino_years": str(nino_years),
                     "description": "duration of El Nino events; Nino events = " + region_ev + " SSTA > " + my_thresh +
                                    " during " + season_ev + enso_method + ", duration is the number of consecutive " +
                                    "months during which SSTA > " + my_thresh,
                     "diagnostic_value": nino_dur_mean, "diagnostic_value_error": nino_dur_err}
            dict4 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict4,
                       var1=sst_ts, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                       var2=nina_dur_all, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                       var3=nino_dur_all, var3_attributes=dict3, var3_name=ovar[2] + dataset)
            del dict1, dict2, dict3, dict4
    # Metric value
    if debug is True:
        dict_debug = {
            "line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": nbr_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsodSstOce(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox,
                thffile, thfname, thfareafile, thfareaname, thflandmaskfile, thflandmaskname, thfbox,
                event_definition, dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsodSstOce() function computes an estimation of the SST (sea surface temperature) change caused by an anomalous
    ocean circulation (usually in nino3)
    For this, the net heat flux (THF) is integrated from June to December (representing SST change driven by heat
    fluxes) and subtracted to the SST change during this period. dSSToce = dSST - dSSTthf
    The net heat flux is the sum of four term:
         - net surface shortwave radiation,
         - net surface longwave radiation,
         - latent heat flux,
         - sensible heat flux

    The net heat flux is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into thffile and thfname of this
    function a list() of the four needed files and variable names (CMIP: rsds-rsus, rlds-rlus, hfls, hfss)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Jul 18 2019

    Based on:
    Bayr, T., C. Wengel, M. Latif, D. Dommenget, J. Lübbecke, W. Park (2018) Error compensation of ENSO atmospheric
    feedbacks in climate models and its influence on simulated ENSO dynamics. Clim. Dyn., doi:10.1007/s00382-018-4575-7

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param thffile: string
        path_to/filename of the file (NetCDF) of THF
    :param thfname: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffile'
    :param thfareafile: string
        path_to/filename of the file (NetCDF) of the areacell for THF
    :param thfareaname: string
        name of areacell variable (areacella, areacello) in 'thfareafile'
    :param thflandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for THF
    :param thflandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfile'
    :param thfbox: string
        name of box (nino3') for THF
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsodSstOce_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        enso_method = " ("
        if isinstance(smooth_ev, dict) is True:
            enso_method += "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            if isinstance(length_ev, int) is True:
                enso_method += "; "
        if isinstance(length_ev, int) is True:
            enso_method += "threshold met during at least " + str(length_ev) + " consecutive months"
        enso_method += ")"
    else:
        enso_method = ""
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "SST change caused by an anomalous ocean circulation (dSSToce)"
    units = "C/C"
    method = "Nino (Nina) events = " + region_ev + " SSTA > " + my_thresh + " (< -" + my_thresh + ") during " + \
             season_ev + enso_method + ", dSSToce = dSST - dSSTthf during ENSO events (relative difference between " + \
             sstbox + " SST change and heat flux-driven " + thfbox + " SST change from Jul to Dec)"
    ref = "Using CDAT"
    metric = "EnsodSstOce"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"value": None, "value2": None, "value3": None, "axis": None}
    nina_years, nino_years, actualtimebounds, keyerror, nbr_year = list(), list(), None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        enso, enso_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, region_ev, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        sst, sst_areacell, keyerror2 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        thf, thf_areacell, keyerror3 = Read_data_mask_area_multifile(
            thffile, thfname, "heat flux", "thf", metric, thfbox, file_area=thfareafile, name_area=thfareaname,
            file_mask=thflandmaskfile, name_mask=thflandmaskname, maskland=True, maskocean=False, debug=debug,
            interpreter="project_interpreter_var2", **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst, thf, keyerror1 = CheckTime(sst, thf, metric_name=metric, debug=debug, **kwargs)
        sst, enso, keyerror2 = CheckTime(sst, enso, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.3 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.4 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        enso_sst, _, keyerror1 = PreProcessTS(
            enso, "", areacell=enso_areacell, average="horizontal", region=region_ev, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        # 2.2 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror2 = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        # 2.3 THF averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        thf, _, keyerror3 = PreProcessTS(
            thf, "", areacell=thf_areacell, average="horizontal", compute_anom=True, region=thfbox, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
        del enso_areacell, smooth, sst_areacell, thf_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                          "axes2": "(thf) " + str([ax.id for ax in thf.getAxisList()]),
                          "axes3": "(enso) " + str([ax.id for ax in enso_sst.getAxisList()]),
                          "shape1": "(sst) " + str(sst.shape), "shape2": "(thf) " + str(thf.shape),
                          "shape3": "(thf) " + str(enso_sst.shape), "time1": "(sst) " + str(TimeBounds(sst)),
                          "time2": "(thf) " + str(TimeBounds(thf)), "time3": "(enso) " + str(TimeBounds(enso_sst))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Detect ENSO events
        # ------------------------------------------------
        # 3.1 Lists event years
        nina_years = DetectEvents(enso_sst, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                  compute_season=True, duration=length_ev)
        nino_years = DetectEvents(enso_sst, season_ev, thresh_ev, normalization=normalize, nino=True,
                                  compute_season=True, duration=length_ev)
        if debug is True:
            dict_debug = {"nina1": str(nina_years), "nino1": str(nino_years)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Compute SST change (diagnostic value)
        # ------------------------------------------------
        # 4.1 SST change
        dSST, dSSTthf, dSSToce = SlabOcean(
            sst, thf, "JUN", "DEC", nina_years + nino_years, frequency=kwargs["frequency"], debug=debug)
        # 4.2 Mean SST change caused by an anomalous ocean circulation during ENSO events
        mv = float(dSSToce[-1])
        mv_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            thf_map, thf_map_areacell, keyerror2 = Read_data_mask_area_multifile(
                thffile, thfname, "heat flux", "thf", metric, "equatorial_pacific", file_area=thfareafile,
                name_area=thfareaname, file_mask=thflandmaskfile, name_mask=thflandmaskname, maskland=True,
                maskocean=False, debug=debug, interpreter="project_interpreter_var2", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_map, thf_map, keyerror = CheckTime(sst_map, thf_map, metric_name=metric, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
            sst_map, _, keyerror1 = PreProcessTS(
                sst_map, "", areacell=sst_map_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            thf_map, _, keyerror2 = PreProcessTS(
                thf_map, "", areacell=thf_map_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            del thf_map_areacell, sst_map_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst_map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(thf_map) " + str([ax.id for ax in thf_map.getAxisList()]),
                              "shape1": "(sst_map) " + str(sst_map.shape), "shape2": "(thf_map) " + str(thf_map.shape),
                              "time1": "(sst_map) " + str(TimeBounds(sst_map)),
                              "time2": "(thf_map) " + str(TimeBounds(thf_map))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
            thf_map = Regrid(thf_map, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(sst_map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(thf_map) " + str([ax.id for ax in thf_map.getAxisList()]),
                              "shape1": "(sst_map) " + str(sst_map.shape), "shape2": "(thf_map) " + str(thf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            sst_map, keyerror1 = AverageMeridional(sst_map)
            thf_map, keyerror2 = AverageMeridional(thf_map)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst_map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(thf_map) " + str([ax.id for ax in thf_map.getAxisList()]),
                              "shape1": "(sst_map) " + str(sst_map.shape), "shape2": "(thf_map) " + str(thf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Zonal smoothing
            sst_map, _ = Smoothing(sst_map, "", axis=1, window=51, method="square")
            thf_map, _ = Smoothing(thf_map, "", axis=1, window=51, method="square")
            if debug is True:
                dict_debug = {"axes1": "(sst_map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(thf_map) " + str([ax.id for ax in thf_map.getAxisList()]),
                              "shape1": "(sst_map) " + str(sst_map.shape), "shape2": "(thf_map) " + str(thf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # SST change
            lth = deepcopy(thresh_ev)
            if normalize is True:
                enso = SeasonalMean(enso_sst, season_ev, compute_anom=True)
                lth = lth * float(Std(enso))
            hovdSST, hovdSSTthf, hovdSSToce = SlabOcean(sst_map, thf_map, "JUN", "DEC", nina_years + nino_years,
                                                        frequency=kwargs["frequency"], tmin=lth, debug=debug)
            curdSSTthf, curdSSToce = hovdSSTthf[-1], hovdSSToce[-1]
            if debug is True:
                dict_debug = {"axes1": "(zonal curdSSTthf) " + str([ax.id for ax in curdSSTthf.getAxisList()]),
                              "axes2": "(hovtx hovdSST) " + str([ax.id for ax in hovdSST.getAxisList()]),
                              "shape1": "(zonal curdSSTthf) " + str(curdSSTthf.shape),
                              "shape2": "(hovtx hovdSST) " + str(hovdSST.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SlabOcean: netcdf", 15, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "evolution of " + sstbox + " SST change from Jul to Dec prior ENSO events",
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error,
                     "nina_years": str(nina_years), "nino_years": str(nino_years)}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "evolution of heat flux-driven " + sstbox + " SST change from Jul to Dec prior " +
                                    "ENSO events",
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error,
                     "nina_years": str(nina_years), "nino_years": str(nino_years)}
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "evolution of " + sstbox + " SST change caused by an anomalous ocean circulation " +
                                    "from Jul to Dec prior ENSO events",
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error,
                     "nina_years": str(nina_years), "nino_years": str(nino_years)}
            dict4 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "zonal slice during Dec of heat flux-driven " + sstbox + " SST change from Jul to" +
                                    " Dec prior ENSO events",
                     "nina_years": str(nina_years), "nino_years": str(nino_years)}
            dict5 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "zonal slice during Dec of " + sstbox + " SST change caused by an anomalous ocean" +
                                    " circulation from Jul to Dec prior ENSO events",
                     "nina_years": str(nina_years), "nino_years": str(nino_years)}
            dict6 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of " + sstbox + " SST change from Jul to Dec" +
                                    " prior ENSO events",
                     "nina_years": str(nina_years), "nino_years": str(nino_years)}
            dict7 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of heat flux-driven " + sstbox + " SST " +
                                    "change from Jul to Dec prior ENSO events",
                     "nina_years": str(nina_years), "nino_years": str(nino_years)}
            dict8 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of " + sstbox + " SST change caused by an " +
                                    "anomalous ocean circulation from Jul to Dec prior ENSO events",
                     "nina_years": str(nina_years), "nino_years": str(nino_years)}
            dict9 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict9,
                       var1=dSST, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                       var2=dSSTthf, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                       var3=dSSToce, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=curdSSTthf, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=curdSSToce, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=hovdSST, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                       var7=hovdSSTthf, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                       var8=hovdSSToce, var8_attributes=dict8, var8_name=ovar[7] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": nbr_year,
        "events": sorted(nina_years + nino_years), "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoPrLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox,
                  prbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False,
                  netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrLonRmse() function computes PRA (precipitation anomalies) pattern associated with ENSO in a 'prbox'
    (usually the equatorial_pacific).
    It is the regression of 'prbox' averaged PRA time series onto 'region_ev' averaged SSTA (sea surface temperature
    anomalies) (usually the regression of equatorial_pacific PRA onto nino3.4 SSTA).

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr, prec, precip, rain) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, prec, precip, rain) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param prbox: string
        name of box (e.g. 'equatorial_pacific') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'ERA-Interim',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(prbox)["latitude"]
    name = "ENSO zonal PRA pattern"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "mm/day/C"
    method = "zonal curve of " + prbox + " precipitation anomalies (PRA; meridional averaged [" + str(lat[0]) + \
             " ; " + str(lat[1]) + "]) regressed onto " + region_ev + " averaged sea surface temperature anomalies " + \
             "(SSTA) during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoPrLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, keyerror, nbr_year_mod, nbr_year_obs = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            prfilemod, prnamemod, "precipitation", metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
            file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            prfileobs, prnameobs, "precipitation", metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
            file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst_mod, pr_mod, keyerror_mod = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
        sst_obs, pr_obs, keyerror_obs = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.4 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        # 2.2 PR in 'prbox' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, method, keyerror_mod2 = PreProcessTS(
            pr_mod, method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        del pr_mod_areacell, pr_obs_areacell, smooth, sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                "axes4": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                "shape3": "(mod pr) " + str(pr_mod.shape), "shape4": "(obs pr) " + str(pr_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod pr) " + str(TimeBounds(pr_mod)), "time4": "(obs pr) " + str(TimeBounds(pr_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
        # 3.2 map of SST
        pr_mod = SeasonalMean(pr_mod, season_ev, compute_anom=True)
        pr_obs = SeasonalMean(pr_obs, season_ev, compute_anom=True)
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                "axes3": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                "axes4": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                "shape3": "(mod pr) " + str(pr_mod.shape), "shape4": "(obs pr) " + str(pr_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                "time2": "(obs sst) " + str(TimeBounds(enso_obs)),
                "time3": "(mod pr) " + str(TimeBounds(pr_mod)), "time4": "(obs pr) " + str(TimeBounds(pr_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Regression map
        # ------------------------------------------------
        # 4.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            pr_mod, pr_obs, method = TwoVarRegrid(pr_mod, pr_obs, method, region=prbox, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod pr) " + str(pr_mod.shape), "shape2": "(obs pr) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.2 Meridional average
        pr_lon_mod, keyerror_mod = AverageMeridional(pr_mod)
        pr_lon_obs, keyerror_obs = AverageMeridional(pr_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                          "axes2": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                          "shape1": "(mod pr) " + str(pr_mod.shape), "shape2": "(obs pr) " + str(pr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.3 Regression map
        pr_slope_mod = LinearRegressionTsAgainstMap(pr_lon_mod, enso_mod, return_stderr=False)
        pr_slope_obs = LinearRegressionTsAgainstMap(pr_lon_obs, enso_obs, return_stderr=False)
        if debug is True:
            dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_slope_mod.getAxisList()]),
                          "axes2": "(obs pr) " + str([ax.id for ax in pr_slope_obs.getAxisList()]),
                          "shape1": "(mod pr) " + str(pr_slope_mod.shape),
                          "shape2": "(obs pr) " + str(pr_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(pr_slope_mod, pr_slope_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(pr_slope_mod, pr_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(pr_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(pr_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            pr_mod, pr_mod_areacell, keyerror_mod = Read_data_mask_area(
                prfilemod, prnamemod, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafilemod,
                name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            pr_obs, pr_obs_areacell, keyerror_obs = Read_data_mask_area(
                prfileobs, prnameobs, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafileobs,
                name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_mod, pr_mod, keyerror_mod = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
            sst_obs, pr_obs, keyerror_obs = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # PR in 'equatorial_pacific_LatExt2' are normalized / detrended / smoothed (running average) if applicable
            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
            kwargs["smoothing"] = deepcopy(smooth_ev)
            pr_mod, _, keyerror_mod = PreProcessTS(
                pr_mod, "", areacell=pr_mod_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            pr_obs, _, keyerror_obs = PreProcessTS(
                pr_obs, "", areacell=pr_obs_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            kwargs["smoothing"] = deepcopy(smooth)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del pr_mod_areacell, pr_obs_areacell, smooth
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod pr) " + str(pr_mod.shape), "shape2": "(obs pr) " + str(pr_obs.shape),
                              "time1": "(mod pr) " + str(TimeBounds(pr_mod)),
                              "time2": "(obs pr) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Seasonal mean and anomalies
            pr_mod = SeasonalMean(pr_mod, season_ev, compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod pr) " + str(pr_mod.shape), "shape2": "(obs pr) " + str(pr_obs.shape),
                              "time1": "(mod pr) " + str(TimeBounds(pr_mod)),
                              "time2": "(obs pr) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                pr_mod, pr_obs, _ = TwoVarRegrid(
                    pr_mod, pr_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(mod pr) " + str(pr_mod.shape), "shape2": "(obs pr) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Linear regression
            pr_map_slope_mod = LinearRegressionTsAgainstMap(pr_mod, enso_mod, return_stderr=False)
            pr_map_slope_obs = LinearRegressionTsAgainstMap(pr_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_map_slope_mod.getAxisList()]),
                              "axes2": "(obs pr) " + str([ax.id for ax in pr_map_slope_obs.getAxisList()]),
                              "shape1": "(mod pr) " + str(pr_map_slope_mod.shape),
                              "shape2": "(obs pr) " + str(pr_map_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionTsAgainstMap: netcdf", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_pr_lon_mod = Composite(pr_lon_mod, ln_years_mod, kwargs["frequency"])
                ln_pr_map_mod = Composite(pr_mod, ln_years_mod, kwargs["frequency"])
            else:
                ln_pr_lon_mod = MyEmpty(pr_lon_mod[0], time=False)
                ln_pr_map_mod = MyEmpty(pr_mod[0], time=False)
            if len(en_years_mod) > 0:
                en_pr_lon_mod = Composite(pr_lon_mod, en_years_mod, kwargs["frequency"])
                en_pr_map_mod = Composite(pr_mod, en_years_mod, kwargs["frequency"])
            else:
                en_pr_lon_mod = MyEmpty(pr_lon_mod[0], time=False)
                en_pr_map_mod = MyEmpty(pr_mod[0], time=False)
            if len(ln_years_obs) > 0:
                ln_pr_lon_obs = Composite(pr_lon_obs, ln_years_obs, kwargs["frequency"])
                ln_pr_map_obs = Composite(pr_obs, ln_years_obs, kwargs["frequency"])
            else:
                ln_pr_lon_obs = MyEmpty(pr_lon_obs[0], time=False)
                ln_pr_map_obs = MyEmpty(pr_obs[0], time=False)
            if len(en_years_obs) > 0:
                en_pr_lon_obs = Composite(pr_lon_obs, en_years_obs, kwargs["frequency"])
                en_pr_map_obs = Composite(pr_obs, en_years_obs, kwargs["frequency"])
            else:
                en_pr_lon_obs = MyEmpty(pr_lon_obs[0], time=False)
                en_pr_map_obs = MyEmpty(pr_obs[0], time=False)
            # Supplementary metrics
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "mm/day"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = ["zonal curve of " + prbox + " PRA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) +
                     "]) of La Nina events composite during " + season_ev + "; Nina = " + region_ev + " SSTA < -" +
                     my_thresh + " during " + season_ev + enso_method,
                     "zonal curve of " + prbox + " PRA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) +
                     "]) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                     my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific PRA regressed onto " + region_ev + " averaged SSTA during " + season_ev +
                     enso_method3,
                     "map of tropical Pacific PRA of La Nina events composite during " + season_ev + "; Nina = " +
                     region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific PRA of El Nino events composite during " + season_ev + "; Nino = " +
                     region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_pr_lon_mod, en_pr_lon_mod, pr_map_slope_mod, ln_pr_map_mod, en_pr_map_mod]
            my_ob = [ln_pr_lon_obs, en_pr_lon_obs, pr_map_slope_obs, ln_pr_map_obs, en_pr_map_obs]
            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
            my_ax = ["0", "0", "xy", "xy", "xy"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": method.split(", ")[0], "arraySTD_" + dataset1: sm_std_mod}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": method.split(", ")[0], "arraySTD_" + dataset2: sm_std_obs}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                     "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=pr_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=pr_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSshLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                   sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs,
                   sstbox, sshbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                   debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSshLonRmse() function computes dynamic SSHA (sea surface height anomalies) pattern associated with ENSO in a
    'sshbox' (usually the equatorial_pacific).
    It is the regression of 'sshbox' averaged dynamic SSHA time series onto 'region_ev' averaged SSTA (sea surface
    temperature anomalies) (usually the regression of equatorial_pacific dynamic SSHA onto nino3.4 SSTA).

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May  3 2021

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SSH areacell
    :param sshareanamemod: string, optional
        name of areacell for the SSH variable (areacella, areacello,...) in 'sshareafilemod'
    :param sshlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SSH landmask
    :param sshlandmasknamemod: string, optional
        name of landmask for the SSH variable (sftlf,...) in 'sshlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SSH areacell
    :param sshareanameobs: string, optional
        name of areacell for the SSH variable (areacella, areacello,...) in 'sshareafileobs'
    :param sshlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SSH landmask
    :param sshlandmasknameobs: string, optional
        name of landmask for the SSH variable (sftlf,...) in 'sshlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param sshbox: string
        name of box (e.g. 'equatorial_pacific') for SSH
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'ORAS5',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSshLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(sshbox)["latitude"]
    name = "ENSO zonal dynamic SSHA pattern"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "cm/C"
    method = "zonal curve of " + sshbox + " dynamic sea surface height anomalies (SSHA; meridional averaged [" + \
             str(lat[0]) + " ; " + str(lat[1]) + "]) regressed onto " + region_ev + " averaged sea surface " + \
             "temperature (SSTA) during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoSshLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        ssh_mod, ssh_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, sshbox, file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_mod_glo, ssh_mod_glo_areacell, keyerror_mod3 = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, "global2", file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_obs, ssh_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, sshbox, file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        ssh_obs_glo, ssh_obs_glo_areacell, keyerror_obs3 = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, "global2", file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors(
            [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst_mod, ssh_mod, keyerror_mod1 = CheckTime(sst_mod, ssh_mod, metric_name=metric, debug=debug, **kwargs)
        sst_mod, ssh_mod_glo, keyerror_mod2 = CheckTime(sst_mod, ssh_mod_glo, metric_name=metric, debug=debug, **kwargs)
        sst_obs, ssh_obs, keyerror_obs1 = CheckTime(sst_obs, ssh_obs, metric_name=metric, debug=debug, **kwargs)
        sst_obs, ssh_obs_glo, keyerror_obs2 = CheckTime(sst_obs, ssh_obs_glo, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.4 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        my_smo = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = False
        ssh_mod_glo, _, keyerror_mod = PreProcessTS(
            ssh_mod_glo, "", areacell=ssh_mod_glo_areacell, average="horizontal", region="global2", **kwargs)
        ssh_obs_glo, _, keyerror_obs = PreProcessTS(
            ssh_obs_glo, "", areacell=ssh_obs_glo_areacell, average="horizontal", region="global2", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_mod])
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        ssh_mod, method, keyerror_mod = remove_global_mean(ssh_mod, ssh_mod_glo, "SSH", method)
        ssh_obs, _, keyerror_obs = remove_global_mean(ssh_obs, ssh_obs_glo, "", "")
        keyerror = add_up_errors([keyerror_mod, keyerror_mod])
        if keyerror is not None:
            break
        # 2.3 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        # 2.4 SSH in 'sshbox' are normalized / detrended / smoothed (running average) if applicable
        ssh_mod, method, keyerror_mod2 = PreProcessTS(
            ssh_mod, method, areacell=ssh_mod_areacell, region=sshbox, **kwargs)
        ssh_obs, _, keyerror_obs2 = PreProcessTS(ssh_obs, "", areacell=ssh_obs_areacell, region=sshbox, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        kwargs["smoothing"] = deepcopy(my_smo)
        del my_det, my_smo, ssh_mod_areacell, ssh_mod_glo_areacell, ssh_obs_areacell, ssh_obs_glo_areacell, \
            sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                "axes4": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                "shape3": "(mod ssh) " + str(ssh_mod.shape), "shape4": "(obs ssh) " + str(ssh_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod ssh) " + str(TimeBounds(ssh_mod)), "time4": "(obs ssh) " + str(TimeBounds(ssh_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
        # 3.2 map of SST
        ssh_mod = SeasonalMean(ssh_mod, season_ev, compute_anom=True)
        ssh_obs = SeasonalMean(ssh_obs, season_ev, compute_anom=True)
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                "axes3": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                "axes4": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                "shape3": "(mod ssh) " + str(ssh_mod.shape), "shape4": "(obs ssh) " + str(ssh_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(enso_mod)), "time2": "(obs sst) " + str(TimeBounds(enso_obs)),
                "time3": "(mod ssh) " + str(TimeBounds(ssh_mod)), "time4": "(obs ssh) " + str(TimeBounds(ssh_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Regression
        # ------------------------------------------------
        # 4.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            ssh_mod, ssh_obs, method = TwoVarRegrid(ssh_mod, ssh_obs, method, region=sshbox, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod ssh) " + str(ssh_mod.shape), "shape2": "(obs ssh) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.2 Meridional average
        ssh_lon_mod, keyerror_mod = AverageMeridional(ssh_mod)
        ssh_lon_obs, keyerror_obs = AverageMeridional(ssh_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod ssh) " + str(ssh_mod.shape), "shape2": "(obs ssh) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.3 Regression
        ssh_slope_mod = LinearRegressionTsAgainstMap(ssh_lon_mod, enso_mod, return_stderr=False)
        ssh_slope_obs = LinearRegressionTsAgainstMap(ssh_lon_obs, enso_obs, return_stderr=False)
        if debug is True:
            dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_slope_mod.getAxisList()]),
                          "axes2": "(obs ssh) " + str([ax.id for ax in ssh_slope_obs.getAxisList()]),
                          "shape1": "(mod ssh) " + str(ssh_slope_mod.shape),
                          "shape2": "(obs ssh) " + str(ssh_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(ssh_slope_mod, ssh_slope_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(ssh_slope_mod, ssh_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(ssh_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(ssh_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            ssh_mod, ssh_mod_map_areacell, keyerror_mod = Read_data_mask_area(
                sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific_LatExt2",
                file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                name_mask=sshlandmasknamemod, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, **kwargs)
            ssh_obs, ssh_obs_map_areacell, keyerror_obs = Read_data_mask_area(
                sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific_LatExt2",
                file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                name_mask=sshlandmasknameobs, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_mod, ssh_mod, keyerror_mod = CheckTime(sst_mod, ssh_mod, metric_name=metric, debug=debug, **kwargs)
            sst_obs, ssh_obs, keyerror_obs = CheckTime(sst_obs, ssh_obs, metric_name=metric, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Remove global mean SSH to local SSH at every time step
            ssh_mod, _, keyerror_mod = remove_global_mean(ssh_mod, ssh_mod_glo, "", "")
            ssh_obs, _, keyerror_obs = remove_global_mean(ssh_obs, ssh_obs_glo, "", "")
            keyerror = add_up_errors([keyerror_mod, keyerror_mod])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod ssh) " + str(ssh_mod.shape), "shape2": "(obs ssh) " + str(ssh_obs.shape),
                              "time1": "(mod ssh) " + str(TimeBounds(ssh_mod)),
                              "time2": "(obs ssh) " + str(TimeBounds(ssh_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean: netcdf", 15, **dict_debug)
            # Preprocess SSH (computes anomalies, normalizes, detrends, smoothes, averages,...)
            my_smo = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
            kwargs["smoothing"] = deepcopy(smooth_ev)
            ssh_mod, _, keyerror_mod = PreProcessTS(
                ssh_mod, "", areacell=ssh_mod_map_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            ssh_obs, _, keyerror_obs = PreProcessTS(
                ssh_obs, "", areacell=ssh_obs_map_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            kwargs["smoothing"] = deepcopy(my_smo)
            del my_smo, ssh_mod_map_areacell, ssh_obs_map_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod ssh) " + str(ssh_mod.shape), "shape2": "(obs ssh) " + str(ssh_obs.shape),
                              "time1": "(mod ssh) " + str(TimeBounds(ssh_mod)),
                              "time2": "(obs ssh) " + str(TimeBounds(ssh_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Seasonal mean and anomalies
            ssh_mod = SeasonalMean(ssh_mod, season_ev, compute_anom=True)
            ssh_obs = SeasonalMean(ssh_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod ssh) " + str(ssh_mod.shape), "shape2": "(obs ssh) " + str(ssh_obs.shape),
                              "time1": "(mod ssh) " + str(TimeBounds(ssh_mod)),
                              "time2": "(obs ssh) " + str(TimeBounds(ssh_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                ssh_mod, ssh_obs, _ = TwoVarRegrid(ssh_mod, ssh_obs, "", region="equatorial_pacific_LatExt2",
                                                   **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                  "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                  "shape1": "(mod ssh) " + str(ssh_mod.shape),
                                  "shape2": "(obs ssh) " + str(ssh_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Linear regression
            ssh_map_slope_mod = LinearRegressionTsAgainstMap(ssh_mod, enso_mod, return_stderr=False)
            ssh_map_slope_obs = LinearRegressionTsAgainstMap(ssh_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_map_slope_mod.getAxisList()]),
                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_map_slope_obs.getAxisList()]),
                              "shape1": "(mod ssh) " + str(ssh_map_slope_mod.shape),
                              "shape2": "(obs ssh) " + str(ssh_map_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionTsAgainstMap: netcdf", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_ssh_lon_mod = Composite(ssh_lon_mod, ln_years_mod, kwargs["frequency"])
                ln_ssh_map_mod = Composite(ssh_mod, ln_years_mod, kwargs["frequency"])
            else:
                ln_ssh_lon_mod, ln_ssh_map_mod = MyEmpty(ssh_lon_mod[0], time=False), MyEmpty(ssh_mod[0], time=False)
            if len(en_years_mod) > 0:
                en_ssh_lon_mod = Composite(ssh_lon_mod, en_years_mod, kwargs["frequency"])
                en_ssh_map_mod = Composite(ssh_mod, en_years_mod, kwargs["frequency"])
            else:
                en_ssh_lon_mod, en_ssh_map_mod = MyEmpty(ssh_lon_mod[0], time=False), MyEmpty(ssh_mod[0], time=False)
            if len(ln_years_obs) > 0:
                ln_ssh_lon_obs = Composite(ssh_lon_obs, ln_years_obs, kwargs["frequency"])
                ln_ssh_map_obs = Composite(ssh_obs, ln_years_obs, kwargs["frequency"])
            else:
                ln_ssh_lon_obs, ln_ssh_map_obs = MyEmpty(ssh_lon_obs[0], time=False), MyEmpty(ssh_obs[0], time=False)
            if len(en_years_obs) > 0:
                en_ssh_lon_obs = Composite(ssh_lon_obs, en_years_obs, kwargs["frequency"])
                en_ssh_map_obs = Composite(ssh_obs, en_years_obs, kwargs["frequency"])
            else:
                en_ssh_lon_obs, en_ssh_map_obs = MyEmpty(ssh_lon_obs[0], time=False), MyEmpty(ssh_obs[0], time=False)
            # Supplementary metrics
            my_units = "" if kwargs["normalization"] is True else "cm"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = ["zonal curve of " + sshbox + " dynamic SSHA (meridional averaged [" + str(lat[0]) + " ; " +
                     str(lat[1]) + "]) of La Nina events composite during " + season_ev + "; Nina = " + region_ev +
                     " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "zonal curve of " + sshbox + " dynamic SSHA (meridional averaged [" + str(lat[0]) + " ; " +
                     str(lat[1]) + "]) of El Nino events composite during " + season_ev + "; Nino = " + region_ev +
                     " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific dynamic SSHA regressed onto " + region_ev + " averaged SSTA during " +
                     season_ev + enso_method3,
                     "map of tropical Pacific dynamic SSHA of La Nina events composite during " + season_ev + "; Nina" +
                     " = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific dynamic SSHA of El Nino events composite during " + season_ev + "; Nino" +
                     " = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_ssh_lon_mod, en_ssh_lon_mod, ssh_map_slope_mod, ln_ssh_map_mod, en_ssh_map_mod]
            my_ob = [ln_ssh_lon_obs, en_ssh_lon_obs, ssh_map_slope_obs, ln_ssh_map_obs, en_ssh_map_obs]
            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
            my_ax = ["0", "0", "xy", "xy", "xy"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "arraySTD_" + dataset1: sm_std_mod, "description": method.split(", ")[0]}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "arraySTD_" + dataset2: sm_std_obs, "description": method.split(", ")[0]}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                     "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=ssh_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=ssh_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSstLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                   box, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False,
                   netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstLonRmse() function computes SSTA (sea surface temperature anomalies) pattern associated with ENSO in a
    'box' (usually the equatorial_pacific)
    It is the regression of meridionally averaged 'box' SSTA onto 'region_ev' averaged SSTA during boreal winter
    (usually the regression of equatorial_pacific SSTA onto nino3.4 SSTA).

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param box: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(box)["latitude"]
    name = "ENSO zonal SSTA pattern"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "C/C"
    method = "zonal curve of " + box + " sea surface temperature anomalies (SSTA; meridional averaged [" + \
             str(lat[0]) + " ; " + str(lat[1]) + "]) regressed onto " + region_ev + " averaged SSTA during " + \
             season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoSstLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        sst_map_mod, sst_map_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
            file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_map_obs, sst_map_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
            file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        # 2.2 SST in 'box' are normalized / detrended / smoothed (running average) if applicable
        sst_map_mod, method, keyerror_mod2 = PreProcessTS(
            sst_map_mod, method, areacell=sst_map_mod_areacell, region=box, **kwargs)
        sst_map_obs, _, keyerror_obs2 = PreProcessTS(
            sst_map_obs, "", areacell=sst_map_obs_areacell, region=box, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        del smooth, sst_map_mod_areacell, sst_map_obs_areacell, sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                "axes4": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                "shape1": "(mod) " + str(sst_box_mod.shape), "shape2": "(obs) " + str(sst_box_obs.shape),
                "shape3": "(mod map) " + str(sst_map_mod.shape), "shape4": "(obs map) " + str(sst_map_obs.shape),
                "time1": "(mod) " + str(TimeBounds(sst_box_mod)), "time2": "(obs) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod map) " + str(TimeBounds(sst_map_mod)),
                "time4": "(obs map) " + str(TimeBounds(sst_map_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
        # 3.2 map of SST
        sst_map_mod = SeasonalMean(sst_map_mod, season_ev, compute_anom=True)
        sst_map_obs = SeasonalMean(sst_map_obs, season_ev, compute_anom=True)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                          "axes3": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                          "axes4": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                          "shape1": "(mod) " + str(enso_mod.shape), "shape2": "(obs) " + str(enso_obs.shape),
                          "shape3": "(mod map) " + str(sst_map_mod.shape),
                          "shape4": "(obs map) " + str(sst_map_obs.shape),
                          "time1": "(mod) " + str(TimeBounds(enso_mod)), "time2": "(obs) " + str(TimeBounds(enso_obs)),
                          "time3": "(mod map) " + str(TimeBounds(sst_map_mod)),
                          "time4": "(obs map) " + str(TimeBounds(sst_map_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Regression map
        # ------------------------------------------------
        # 4.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            sst_map_mod, sst_map_obs, method = TwoVarRegrid(
                sst_map_mod, sst_map_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                              "axes2": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                              "shape1": "(mod map) " + str(sst_map_mod.shape),
                              "shape2": "(obs map) " + str(sst_map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.2 Meridional average
        sst_lon_mod, keyerror_mod = AverageMeridional(sst_map_mod)
        sst_lon_obs, keyerror_obs = AverageMeridional(sst_map_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod lon) " + str([ax.id for ax in sst_lon_mod.getAxisList()]),
                          "axes2": "(obs lon) " + str([ax.id for ax in sst_lon_obs.getAxisList()]),
                          "shape1": "(mod lon) " + str(sst_lon_mod.shape),
                          "shape2": "(obs lon) " + str(sst_lon_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
        # 4.3 Regression
        sst_slope_mod = LinearRegressionTsAgainstMap(sst_lon_mod, enso_mod, return_stderr=False)
        sst_slope_obs = LinearRegressionTsAgainstMap(sst_lon_obs, enso_obs, return_stderr=False)
        if debug is True:
            dict_debug = {"axes1": "(mod lon) " + str([ax.id for ax in sst_slope_mod.getAxisList()]),
                          "axes2": "(obs lon) " + str([ax.id for ax in sst_slope_obs.getAxisList()]),
                          "shape1": "(mod lon) " + str(sst_slope_mod.shape),
                          "shape2": "(obs lon) " + str(sst_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(sst_slope_mod, sst_slope_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(sst_slope_mod, sst_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = Std(sst_slope_mod, weights=None, axis=0, centered=1, biased=1)
        sm_std_obs = Std(sst_slope_obs, weights=None, axis=0, centered=1, biased=1)
        sm_std = float(sm_std_mod) / float(sm_std_obs)
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map_mod, sst_map_mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafilemod,
                name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            sst_map_obs, sst_map_obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafileobs,
                name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess SST (computes anomalies, normalizes, detrends, smoothes, averages,...)
            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
            kwargs["smoothing"] = deepcopy(smooth_ev)
            sst_map_mod, _, keyerror_mod = PreProcessTS(
                sst_map_mod, "", areacell=sst_map_mod_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            sst_map_obs, _, keyerror_obs = PreProcessTS(
                sst_map_obs, "", areacell=sst_map_obs_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            kwargs["smoothing"] = deepcopy(smooth)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del smooth, sst_map_mod_areacell, sst_map_obs_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                              "axes2": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                              "shape1": "(mod map) " + str(sst_map_mod.shape),
                              "shape2": "(obs map) " + str(sst_map_obs.shape),
                              "time1": "(mod map) " + str(TimeBounds(sst_map_mod)),
                              "time2": "(obs map) " + str(TimeBounds(sst_map_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Seasonal mean and anomalies
            sst_map_mod = SeasonalMean(sst_map_mod, season_ev, compute_anom=True)
            sst_map_obs = SeasonalMean(sst_map_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {"axes1": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                              "axes2": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                              "shape1": "(mod map) " + str(sst_map_mod.shape),
                              "shape2": "(obs map) " + str(sst_map_obs.shape),
                              "time1": "(mod map) " + str(TimeBounds(sst_map_mod)),
                              "time2": "(obs map) " + str(TimeBounds(sst_map_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                sst_map_mod, sst_map_obs, _ = TwoVarRegrid(
                    sst_map_mod, sst_map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                                  "axes2": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                                  "shape1": "(mod map) " + str(sst_map_mod.shape),
                                  "shape2": "(obs map) " + str(sst_map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Linear regression
            sst_map_slope_mod = LinearRegressionTsAgainstMap(sst_map_mod, enso_mod, return_stderr=False)
            sst_map_slope_obs = LinearRegressionTsAgainstMap(sst_map_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(mod map) " + str([ax.id for ax in sst_map_slope_mod.getAxisList()]),
                              "axes2": "(obs map) " + str([ax.id for ax in sst_map_slope_obs.getAxisList()]),
                              "shape1": "(mod map) " + str(sst_map_slope_mod.shape),
                              "shape2": "(obs map) " + str(sst_map_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionTsAgainstMap: netcdf", 15, **dict_debug)
            # Lists event years
            nina_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                          compute_season=True, duration=length_ev)
            nino_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                          compute_season=True, duration=length_ev)
            nina_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                          compute_season=True, duration=length_ev)
            nino_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                          compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(nina_years_mod)) + "): " + str(nina_years_mod),
                              "nina2": "(obs) nbr(" + str(len(nina_years_obs)) + "): " + str(nina_years_obs),
                              "nino1": "(mod) nbr(" + str(len(nino_years_mod)) + "): " + str(nino_years_mod),
                              "nino2": "(obs) nbr(" + str(len(nino_years_obs)) + "): " + str(nino_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(nina_years_mod) > 0:
                nina_sst_lon_mod = Composite(sst_lon_mod, nina_years_mod, kwargs["frequency"])
                nina_sst_map_mod = Composite(sst_map_mod, nina_years_mod, kwargs["frequency"])
            else:
                nina_sst_lon_mod = MyEmpty(sst_lon_mod[0], time=False)
                nina_sst_map_mod = MyEmpty(sst_map_mod[0], time=False)
            if len(nino_years_mod) > 0:
                nino_sst_lon_mod = Composite(sst_lon_mod, nino_years_mod, kwargs["frequency"])
                nino_sst_map_mod = Composite(sst_map_mod, nino_years_mod, kwargs["frequency"])
            else:
                nino_sst_lon_mod = MyEmpty(sst_lon_mod[0], time=False)
                nino_sst_map_mod = MyEmpty(sst_map_mod[0], time=False)
            if len(nina_years_obs) > 0:
                nina_sst_lon_obs = Composite(sst_lon_obs, nina_years_obs, kwargs["frequency"])
                nina_sst_map_obs = Composite(sst_map_obs, nina_years_obs, kwargs["frequency"])
            else:
                nina_sst_lon_obs = MyEmpty(sst_lon_obs[0], time=False)
                nina_sst_map_obs = MyEmpty(sst_map_obs[0], time=False)
            if len(nino_years_obs) > 0:
                nino_sst_lon_obs = Composite(sst_lon_obs, nino_years_obs, kwargs["frequency"])
                nino_sst_map_obs = Composite(sst_map_obs, nino_years_obs, kwargs["frequency"])
            else:
                nino_sst_lon_obs = MyEmpty(sst_lon_obs[0], time=False)
                nino_sst_map_obs = MyEmpty(sst_map_obs[0], time=False)
            # Supplementary metrics
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "C"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [nina_years_mod, nino_years_mod, None, nina_years_mod, nino_years_mod]
            my_eo = [nina_years_obs, nino_years_obs, None, nina_years_obs, nino_years_obs]
            my_de = ["zonal curve of " + box + " SSTA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) +
                     "]) of La Nina events composite during " + season_ev + "; Nina = " + region_ev + " SSTA < -" +
                     my_thresh + " during " + season_ev + enso_method,
                     "zonal curve of " + box + " SSTA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) +
                     "]) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                     my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific SSTA regressed onto " + region_ev + " averaged SSTA during " + season_ev +
                     enso_method3,
                     "map of tropical Pacific SSTA of La Nina events composite during " + season_ev + "; Nina = " +
                     region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific SSTA of El Nino events composite during " + season_ev + "; Nino = " +
                     region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [nina_sst_lon_mod, nino_sst_lon_mod, sst_map_slope_mod, nina_sst_map_mod, nino_sst_map_mod]
            my_ob = [nina_sst_lon_obs, nino_sst_lon_obs, sst_map_slope_obs, nina_sst_map_obs, nino_sst_map_obs]
            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
            my_ax = ["0", "0", "xy", "xy", "xy"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": method.split(", ")[0], "arraySTD_" + dataset1: sm_std_mod}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": method.split(", ")[0], "arraySTD_" + dataset2: sm_std_obs}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                     "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sst_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=sst_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoTauxLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                    taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                    sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                    taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs,
                    sstbox, taubox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                    debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoTauxLonRmse() function computes TauxA (zonal wind stress anomalies) pattern associated with ENSO in a
    'taubox' (usually the equatorial_pacific).
    It is the regression of 'taubox' averaged TauxA time series onto 'region_ev' averaged SSTA (sea surface temperature
    anomalies) (usually the regression of equatorial_pacific TauxA onto nino3.4 SSTA).

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Taux
    :param taunamemod: string
        name of Taux variable (tauu, tauuo, taux) in 'taufilemod'
    :param tauareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled Taux areacell
    :param tauareanamemod: string, optional
        name of areacell for the Taux variable (areacella, areacello,...) in 'tauareafilemod'
    :param taulandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled Taux landmask
    :param taulandmasknamemod: string, optional
        name of landmask for the Taux variable (sftlf,...) in 'taulandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Taux
    :param taunameobs: string
        name of Taux variable (tauu, tauuo, taux) in 'taufileobs'
    :param tauareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed Taux areacell
    :param tauareanameobs: string, optional
        name of areacell for the Taux variable (areacella, areacello,...) in 'tauareafileobs'
    :param taulandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed Taux landmask
    :param taulandmasknameobs: string, optional
        name of landmask for the Taux variable (sftlf,...) in 'taulandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param taubox: string
        name of box (e.g. 'equatorial_pacific') for Taux
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'ERA-Interim',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoTauxLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(taubox)["latitude"]
    name = "ENSO zonal TauxA pattern"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "1e-3 N/m2/C"
    method = "zonal curve of " + taubox + " zonal wind stress anomalies (TauxA; meridional averaged [" + str(lat[0]) + \
             " ; " + str(lat[1]) + "]) regressed onto " + region_ev + " averaged sea surface temperature anomalies " + \
             "(SSTA) during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoTauxLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, keyerror, nbr_year_mod, nbr_year_obs = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        tau_mod, tau_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, taubox, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, tau_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, taubox, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst_mod, tau_mod, keyerror_mod = CheckTime(sst_mod, tau_mod, metric_name=metric, debug=debug, **kwargs)
        sst_obs, tau_obs, keyerror_obs = CheckTime(sst_obs, tau_obs, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.4 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        # 2.2 Taux in 'taubox' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod2 = PreProcessTS(
            tau_mod, method, areacell=tau_mod_areacell, compute_anom=False, region=taubox, **kwargs)
        tau_obs, _, keyerror_obs2 = PreProcessTS(
            tau_obs, "", areacell=tau_obs_areacell, compute_anom=False, region=taubox, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        del tau_mod_areacell, tau_obs_areacell, smooth, sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                "axes4": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                "shape3": "(mod Tau) " + str(tau_mod.shape), "shape4": "(obs Tau) " + str(tau_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod Tau) " + str(TimeBounds(tau_mod)), "time4": "(obs Tau) " + str(TimeBounds(tau_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
        # 3.2 map of SST
        tau_mod = SeasonalMean(tau_mod, season_ev, compute_anom=True)
        tau_obs = SeasonalMean(tau_obs, season_ev, compute_anom=True)
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                "axes3": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                "axes4": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                "shape3": "(mod Tau) " + str(tau_mod.shape), "shape4": "(obs Tau) " + str(tau_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(enso_mod)), "time2": "(obs sst) " + str(TimeBounds(enso_obs)),
                "time3": "(mod Tau) " + str(TimeBounds(tau_mod)), "time4": "(obs Tau) " + str(TimeBounds(tau_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Regression map
        # ------------------------------------------------
        # 4.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tau_mod, tau_obs, method = TwoVarRegrid(tau_mod, tau_obs, method, region=taubox, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_mod.shape), "shape2": "(obs Tau) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.2 Meridional average
        tau_lon_mod, keyerror_mod = AverageMeridional(tau_mod)
        tau_lon_obs, keyerror_obs = AverageMeridional(tau_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod Tau) " + str(tau_mod.shape), "shape2": "(obs Tau) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.3 Regression map
        tau_slope_mod = LinearRegressionTsAgainstMap(tau_lon_mod, enso_mod, return_stderr=False)
        tau_slope_obs = LinearRegressionTsAgainstMap(tau_lon_obs, enso_obs, return_stderr=False)
        if debug is True:
            dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_slope_mod.getAxisList()]),
                          "axes2": "(obs Tau) " + str([ax.id for ax in tau_slope_obs.getAxisList()]),
                          "shape1": "(mod pr) " + str(tau_slope_mod.shape),
                          "shape2": "(obs Tau) " + str(tau_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(tau_slope_mod, tau_slope_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(tau_slope_mod, tau_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tau_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tau_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            tau_mod, tau_mod_areacell, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            tau_obs, tau_obs_areacell, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_mod, tau_mod, keyerror_mod = CheckTime(sst_mod, tau_mod, metric_name=metric, debug=debug, **kwargs)
            sst_obs, tau_obs, keyerror_obs = CheckTime(sst_obs, tau_obs, metric_name=metric, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Taux in 'equatorial_pacific_LatExt2' are normalized / detrended / smoothed (running average) if applicable
            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
            kwargs["smoothing"] = deepcopy(smooth_ev)
            tau_mod, _, keyerror_mod = PreProcessTS(
                tau_mod, "", areacell=tau_mod_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            tau_obs, _, keyerror_obs = PreProcessTS(
                tau_obs, "", areacell=tau_obs_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            kwargs["smoothing"] = deepcopy(smooth)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del tau_mod_areacell, tau_obs_areacell, smooth
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_mod.shape), "shape2": "(obs Tau) " + str(tau_obs.shape),
                              "time1": "(mod Tau) " + str(TimeBounds(tau_mod)),
                              "time2": "(obs Tau) " + str(TimeBounds(tau_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Seasonal mean and anomalies
            tau_mod = SeasonalMean(tau_mod, season_ev, compute_anom=True)
            tau_obs = SeasonalMean(tau_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_mod.shape), "shape2": "(obs Tau) " + str(tau_obs.shape),
                              "time1": "(mod Tau) " + str(TimeBounds(tau_mod)),
                              "time2": "(obs Tau) " + str(TimeBounds(tau_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                tau_mod, tau_obs, _ = TwoVarRegrid(
                    tau_mod, tau_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                                  "axes2": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                                  "shape1": "(mod Tau) " + str(tau_mod.shape),
                                  "shape2": "(obs Tau) " + str(tau_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Linear regression
            tau_map_slope_mod = LinearRegressionTsAgainstMap(tau_mod, enso_mod, return_stderr=False)
            tau_map_slope_obs = LinearRegressionTsAgainstMap(tau_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_map_slope_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_map_slope_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_map_slope_mod.shape),
                              "shape2": "(obs Tau) " + str(tau_map_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionTsAgainstMap: netcdf", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_tau_lon_mod = Composite(tau_lon_mod, ln_years_mod, kwargs["frequency"])
                ln_tau_map_mod = Composite(tau_mod, ln_years_mod, kwargs["frequency"])
            else:
                ln_tau_lon_mod = MyEmpty(tau_lon_mod[0], time=False)
                ln_tau_map_mod = MyEmpty(tau_mod[0], time=False)
            if len(en_years_mod) > 0:
                en_tau_lon_mod = Composite(tau_lon_mod, en_years_mod, kwargs["frequency"])
                en_tau_map_mod = Composite(tau_mod, en_years_mod, kwargs["frequency"])
            else:
                en_tau_lon_mod = MyEmpty(tau_lon_mod[0], time=False)
                en_tau_map_mod = MyEmpty(tau_mod[0], time=False)
            if len(ln_years_obs) > 0:
                ln_tau_lon_obs = Composite(tau_lon_obs, ln_years_obs, kwargs["frequency"])
                ln_tau_map_obs = Composite(tau_obs, ln_years_obs, kwargs["frequency"])
            else:
                ln_tau_lon_obs = MyEmpty(tau_lon_obs[0], time=False)
                ln_tau_map_obs = MyEmpty(tau_obs[0], time=False)
            if len(en_years_obs) > 0:
                en_tau_lon_obs = Composite(tau_lon_obs, en_years_obs, kwargs["frequency"])
                en_tau_map_obs = Composite(tau_obs, en_years_obs, kwargs["frequency"])
            else:
                en_tau_lon_obs = MyEmpty(tau_lon_obs[0], time=False)
                en_tau_map_obs = MyEmpty(tau_obs[0], time=False)
            # Supplementary metrics
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "1e-3 N/m2"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = ["zonal curve of " + taubox + " TauxA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) +
                     "]) of La Nina events composite during " + season_ev + "; Nina = " + region_ev + " SSTA < -" +
                     my_thresh + " during " + season_ev + enso_method,
                     "zonal curve of " + taubox + " TauxA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) +
                     "]) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                     my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific TauxA regressed onto " + region_ev + " averaged SSTA during " +
                     season_ev + enso_method3,
                     "map of tropical Pacific TauxA of La Nina events composite during " + season_ev + "; Nina = " +
                     region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific TauxA of El Nino events composite during " + season_ev + "; Nino = " +
                     region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_tau_lon_mod, en_tau_lon_mod, tau_map_slope_mod, ln_tau_map_mod, en_tau_map_mod]
            my_ob = [ln_tau_lon_obs, en_tau_lon_obs, tau_map_slope_obs, ln_tau_map_obs, en_tau_map_obs]
            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
            my_ax = ["0", "0", "xy", "xy", "xy"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": method.split(", ")[0], "arraySTD_" + dataset1: sm_std_mod}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": method.split(", ")[0], "arraySTD_" + dataset2: sm_std_obs}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                     "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tau_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tau_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoTauyLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                    taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                    sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                    taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs,
                    sstbox, taubox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                    debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoTauyLonRmse() function computes TauyA (meridional wind stress anomalies) pattern associated with ENSO in a
    'taubox' (usually the equatorial_pacific).
    It is the regression of 'taubox' averaged TauyA time series onto 'region_ev' averaged SSTA (sea surface temperature
    anomalies) (usually the regression of equatorial_pacific TauyA onto nino3.4 SSTA).

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Wed Dec  9 2020

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Tauy
    :param taunamemod: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufilemod'
    :param tauareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled Tauy areacell
    :param tauareanamemod: string, optional
        name of areacell for the Tauy variable (areacella, areacello,...) in 'tauareafilemod'
    :param taulandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled Tauy landmask
    :param taulandmasknamemod: string, optional
        name of landmask for the Tauy variable (sftlf,...) in 'taulandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Tauy
    :param taunameobs: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufileobs'
    :param tauareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed Tauy areacell
    :param tauareanameobs: string, optional
        name of areacell for the Tauy variable (areacella, areacello,...) in 'tauareafileobs'
    :param taulandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed Tauy landmask
    :param taulandmasknameobs: string, optional
        name of landmask for the Tauy variable (sftlf,...) in 'taulandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param taubox: string
        name of box (e.g. 'equatorial_pacific') for Tauy
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'ERA-Interim',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoTauyLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(taubox)["latitude"]
    name = "ENSO zonal TauyA pattern"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "1e-3 N/m2/C"
    method = "zonal curve of " + taubox + " meridional wind stress anomalies (TauyA; meridional averaged [" + \
             str(lat[0]) + " ; " + str(lat[1]) + "]) regressed onto " + region_ev + " averaged sea surface " + \
             "temperature anomalies (SSTA) during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoTauyLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, keyerror, nbr_year_mod, nbr_year_obs = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        tau_mod, tau_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, taubox, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, tau_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, taubox, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst_mod, tau_mod, keyerror_mod = CheckTime(sst_mod, tau_mod, metric_name=metric, debug=debug, **kwargs)
        sst_obs, tau_obs, keyerror_obs = CheckTime(sst_obs, tau_obs, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.4 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        # 2.2 Tauy in 'taubox' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod2 = PreProcessTS(
            tau_mod, method, areacell=tau_mod_areacell, compute_anom=False, region=taubox, **kwargs)
        tau_obs, _, keyerror_obs2 = PreProcessTS(
            tau_obs, "", areacell=tau_obs_areacell, compute_anom=False, region=taubox, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        del tau_mod_areacell, tau_obs_areacell, smooth, sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                "axes4": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                "shape3": "(mod Tau) " + str(tau_mod.shape), "shape4": "(obs Tau) " + str(tau_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod Tau) " + str(TimeBounds(tau_mod)), "time4": "(obs Tau) " + str(TimeBounds(tau_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
        # 3.2 map of SST
        tau_mod = SeasonalMean(tau_mod, season_ev, compute_anom=True)
        tau_obs = SeasonalMean(tau_obs, season_ev, compute_anom=True)
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                "axes3": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                "axes4": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                "shape3": "(mod Tau) " + str(tau_mod.shape), "shape4": "(obs Tau) " + str(tau_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(enso_mod)), "time2": "(obs sst) " + str(TimeBounds(enso_obs)),
                "time3": "(mod Tau) " + str(TimeBounds(tau_mod)), "time4": "(obs Tau) " + str(TimeBounds(tau_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Regression map
        # ------------------------------------------------
        # 4.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tau_mod, tau_obs, method = TwoVarRegrid(tau_mod, tau_obs, method, region=taubox, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_mod.shape), "shape2": "(obs Tau) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.2 Meridional average
        tau_lon_mod, keyerror_mod = AverageMeridional(tau_mod)
        tau_lon_obs, keyerror_obs = AverageMeridional(tau_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod Tau) " + str(tau_mod.shape), "shape2": "(obs Tau) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.3 Regression map
        tau_slope_mod = LinearRegressionTsAgainstMap(tau_lon_mod, enso_mod, return_stderr=False)
        tau_slope_obs = LinearRegressionTsAgainstMap(tau_lon_obs, enso_obs, return_stderr=False)
        if debug is True:
            dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_slope_mod.getAxisList()]),
                          "axes2": "(obs Tau) " + str([ax.id for ax in tau_slope_obs.getAxisList()]),
                          "shape1": "(mod Tau) " + str(tau_slope_mod.shape),
                          "shape2": "(obs Tau) " + str(tau_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(tau_slope_mod, tau_slope_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(tau_slope_mod, tau_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tau_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tau_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            tau_mod, tau_mod_areacell, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            tau_obs, tau_obs_areacell, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_mod, tau_mod, keyerror_mod = CheckTime(sst_mod, tau_mod, metric_name=metric, debug=debug, **kwargs)
            sst_obs, tau_obs, keyerror_obs = CheckTime(sst_obs, tau_obs, metric_name=metric, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Tauy in 'equatorial_pacific_LatExt2' are normalized / detrended / smoothed (running average) if applicable
            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
            kwargs["smoothing"] = deepcopy(smooth_ev)
            tau_mod, _, keyerror_mod = PreProcessTS(
                tau_mod, "", areacell=tau_mod_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            tau_obs, _, keyerror_obs = PreProcessTS(
                tau_obs, "", areacell=tau_obs_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            kwargs["smoothing"] = deepcopy(smooth)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del tau_mod_areacell, tau_obs_areacell, smooth
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_mod.shape), "shape2": "(obs Tau) " + str(tau_obs.shape),
                              "time1": "(mod Tau) " + str(TimeBounds(tau_mod)),
                              "time2": "(obs Tau) " + str(TimeBounds(tau_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Seasonal mean and anomalies
            tau_mod = SeasonalMean(tau_mod, season_ev, compute_anom=True)
            tau_obs = SeasonalMean(tau_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_mod.shape), "shape2": "(obs Tau) " + str(tau_obs.shape),
                              "time1": "(mod Tau) " + str(TimeBounds(tau_mod)),
                              "time2": "(obs Tau) " + str(TimeBounds(tau_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                tau_mod, tau_obs, _ = TwoVarRegrid(
                    tau_mod, tau_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                                  "axes2": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                                  "shape1": "(mod Tau) " + str(tau_mod.shape),
                                  "shape2": "(obs Tau) " + str(tau_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Linear regression
            tau_map_slope_mod = LinearRegressionTsAgainstMap(tau_mod, enso_mod, return_stderr=False)
            tau_map_slope_obs = LinearRegressionTsAgainstMap(tau_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_map_slope_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_map_slope_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_map_slope_mod.shape),
                              "shape2": "(obs Tau) " + str(tau_map_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionTsAgainstMap: netcdf", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_tau_lon_mod = Composite(tau_lon_mod, ln_years_mod, kwargs["frequency"])
                ln_tau_map_mod = Composite(tau_mod, ln_years_mod, kwargs["frequency"])
            else:
                ln_tau_lon_mod = MyEmpty(tau_lon_mod[0], time=False)
                ln_tau_map_mod = MyEmpty(tau_mod[0], time=False)
            if len(en_years_mod) > 0:
                en_tau_lon_mod = Composite(tau_lon_mod, en_years_mod, kwargs["frequency"])
                en_tau_map_mod = Composite(tau_mod, en_years_mod, kwargs["frequency"])
            else:
                en_tau_lon_mod = MyEmpty(tau_lon_mod[0], time=False)
                en_tau_map_mod = MyEmpty(tau_mod[0], time=False)
            if len(ln_years_obs) > 0:
                ln_tau_lon_obs = Composite(tau_lon_obs, ln_years_obs, kwargs["frequency"])
                ln_tau_map_obs = Composite(tau_obs, ln_years_obs, kwargs["frequency"])
            else:
                ln_tau_lon_obs = MyEmpty(tau_lon_obs[0], time=False)
                ln_tau_map_obs = MyEmpty(tau_obs[0], time=False)
            if len(en_years_obs) > 0:
                en_tau_lon_obs = Composite(tau_lon_obs, en_years_obs, kwargs["frequency"])
                en_tau_map_obs = Composite(tau_obs, en_years_obs, kwargs["frequency"])
            else:
                en_tau_lon_obs = MyEmpty(tau_lon_obs[0], time=False)
                en_tau_map_obs = MyEmpty(tau_obs[0], time=False)
            # Supplementary metrics
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "1e-3 N/m2"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = ["zonal curve of " + taubox + " TauyA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) +
                     "]) of La Nina events composite during " + season_ev + "; Nina = " + region_ev + " SSTA < -" +
                     my_thresh + " during " + season_ev + enso_method,
                     "zonal curve of " + taubox + " TauyA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) +
                     "]) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                     my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific TauyA regressed onto " + region_ev + " averaged SSTA during " +
                     season_ev + enso_method3,
                     "map of tropical Pacific TauyA of La Nina events composite during " + season_ev + "; Nina = " +
                     region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific TauyA of El Nino events composite during " + season_ev + "; Nino = " +
                     region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_tau_lon_mod, en_tau_lon_mod, tau_map_slope_mod, ln_tau_map_mod, en_tau_map_mod]
            my_ob = [ln_tau_lon_obs, en_tau_lon_obs, tau_map_slope_obs, ln_tau_map_obs, en_tau_map_obs]
            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
            my_ax = ["0", "0", "xy", "xy", "xy"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": method.split(", ")[0], "arraySTD_" + dataset1: sm_std_mod}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": method.split(", ")[0], "arraySTD_" + dataset2: sm_std_obs}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                     "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tau_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tau_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoThfLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   thffilemod, thfnamemod, thfareafilemod, thfareanamemod, thflandmaskfilemod, thflandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                   thffileobs, thfnameobs, thfareafileobs, thfareanameobs, thflandmaskfileobs, thflandmasknameobs,
                   sstbox, thfbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                   debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoThfLonRmse() function computes THFA (net heat flux anomalies) pattern associated with ENSO in a 'thfbox'
    (usually the equatorial_pacific).
    It is the regression of 'thfbox' averaged THFA time series onto 'region_ev' averaged SSTA (sea surface temperature
    anomalies) (usually the regression of equatorial_pacific THFA onto nino3.4 SSTA).
    The net heat flux is the sum of four term:
         - net surface shortwave radiation,
         - net surface longwave radiation,
         - latent heat flux,
         - sensible heat flux

    The net heat flux is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into thffile and thfname of this
    function a list() of the four needed files and variable names (CMIP: rsds-rsus, rlds-rlus, hfls, hfss)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May 10 2021

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param thffilemod: string
        path_to/filename of the file (NetCDF) of the modeled THF
    :param thfnamemod: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffilemod'
    :param thfareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled THF areacell
    :param thfareanamemod: string, optional
        name of areacell for the THF variable (areacella, areacello,...) in 'thfareafilemod'
    :param thflandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled THF landmask
    :param thflandmasknamemod: string, optional
        name of landmask for the THF variable (sftlf,...) in 'thflandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param thffileobs: string
        path_to/filename of the file (NetCDF) of the observed THF
    :param thfnameobs: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffileobs'
    :param thfareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed THF areacell
    :param thfareanameobs: string, optional
        name of areacell for the THF variable (areacella, areacello,...) in 'thfareafileobs'
    :param thflandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed THF landmask
    :param thflandmasknameobs: string, optional
        name of landmask for the THF variable (sftlf,...) in 'thflandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param thfbox: string
        name of box (e.g. 'equatorial_pacific') for THF
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'ERA-Interim',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoThfLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(thfbox)["latitude"]
    name = "ENSO zonal THFA pattern"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "W/m2/C"
    method = "zonal curve of " + thfbox + " net heat flux anomalies (THFA; meridional averaged [" + str(lat[0]) + \
             " ; " + str(lat[1]) + "]) regressed onto " + region_ev + " averaged sea surface temperature anomalies " + \
             "(SSTA) during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoThfLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, keyerror, nbr_year_mod, nbr_year_obs = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        thf_mod, thf_mod_areacell, keyerror_mod2 = Read_data_mask_area_multifile(
            thffilemod, thfnamemod, "heat flux", "thf", metric, thfbox, file_area=thfareafilemod,
            name_area=thfareanamemod, file_mask=thflandmaskfilemod, name_mask=thflandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug,
            interpreter="project_interpreter_mod_var2", **kwargs)
        thf_obs, thf_obs_areacell, keyerror_obs2 = Read_data_mask_area_multifile(
            thffileobs, thfnameobs, "heat flux", "thf", metric, thfbox, file_area=thfareafileobs,
            name_area=thfareanameobs, file_mask=thflandmaskfileobs, name_mask=thflandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug,
            interpreter="project_interpreter_obs_var2", **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst_mod, thf_mod, keyerror_mod = CheckTime(sst_mod, thf_mod, metric_name=metric, debug=debug, **kwargs)
        sst_obs, thf_obs, keyerror_obs = CheckTime(sst_obs, thf_obs, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.4 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        # 2.2 THF in 'thfbox' are normalized / detrended / smoothed (running average) if applicable
        thf_mod, method, keyerror_mod2 = PreProcessTS(
            thf_mod, method, areacell=thf_mod_areacell, compute_anom=False, region=thfbox, **kwargs)
        thf_obs, _, keyerror_obs2 = PreProcessTS(
            thf_obs, "", areacell=thf_obs_areacell, compute_anom=False, region=thfbox, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        del thf_mod_areacell, thf_obs_areacell, smooth, sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod thf) " + str([ax.id for ax in thf_mod.getAxisList()]),
                "axes4": "(obs thf) " + str([ax.id for ax in thf_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                "shape3": "(mod thf) " + str(thf_mod.shape), "shape4": "(obs thf) " + str(thf_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod thf) " + str(TimeBounds(thf_mod)), "time4": "(obs thf) " + str(TimeBounds(thf_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
        # 3.2 map of SST
        thf_mod = SeasonalMean(thf_mod, season_ev, compute_anom=True)
        thf_obs = SeasonalMean(thf_obs, season_ev, compute_anom=True)
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                "axes3": "(mod thf) " + str([ax.id for ax in thf_mod.getAxisList()]),
                "axes4": "(obs thf) " + str([ax.id for ax in thf_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                "shape3": "(mod thf) " + str(thf_mod.shape), "shape4": "(obs thf) " + str(thf_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                "time2": "(obs sst) " + str(TimeBounds(enso_obs)),
                "time3": "(mod thf) " + str(TimeBounds(thf_mod)), "time4": "(obs thf) " + str(TimeBounds(thf_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Regression map
        # ------------------------------------------------
        # 4.1 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            thf_mod, thf_obs, method = TwoVarRegrid(thf_mod, thf_obs, method, region=thfbox, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in thf_mod.getAxisList()]),
                              "axes2": "(obs thf) " + str([ax.id for ax in thf_obs.getAxisList()]),
                              "shape1": "(mod thf) " + str(thf_mod.shape), "shape2": "(obs thf) " + str(thf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.2 Meridional average
        thf_lon_mod, keyerror_mod = AverageMeridional(thf_mod)
        thf_lon_obs, keyerror_obs = AverageMeridional(thf_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in thf_mod.getAxisList()]),
                          "axes2": "(obs thf) " + str([ax.id for ax in thf_obs.getAxisList()]),
                          "shape1": "(mod thf) " + str(thf_mod.shape), "shape2": "(obs thf) " + str(thf_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 4.3 Regression map
        thf_slope_mod = LinearRegressionTsAgainstMap(thf_lon_mod, enso_mod, return_stderr=False)
        thf_slope_obs = LinearRegressionTsAgainstMap(thf_lon_obs, enso_obs, return_stderr=False)
        if debug is True:
            dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in thf_slope_mod.getAxisList()]),
                          "axes2": "(obs thf) " + str([ax.id for ax in thf_slope_obs.getAxisList()]),
                          "shape1": "(mod thf) " + str(thf_slope_mod.shape),
                          "shape2": "(obs thf) " + str(thf_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(thf_slope_mod, thf_slope_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(thf_slope_mod, thf_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(thf_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(thf_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            thf_mod, thf_mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                thffilemod, thfnamemod, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                file_area=thfareafilemod, name_area=thfareanamemod, file_mask=thflandmaskfilemod,
                name_mask=thflandmasknamemod, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, interpreter="project_interpreter_mod_var2", **kwargs)
            thf_obs, thf_obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                thffileobs, thfnameobs, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                file_area=thfareafileobs, name_area=thfareanameobs, file_mask=thflandmaskfileobs,
                name_mask=thflandmasknameobs, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, interpreter="project_interpreter_obs_var2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_mod, thf_mod, keyerror_mod = CheckTime(sst_mod, thf_mod, metric_name=metric, debug=debug, **kwargs)
            sst_obs, thf_obs, keyerror_obs = CheckTime(sst_obs, thf_obs, metric_name=metric, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # THF in 'equatorial_pacific_LatExt2' are normalized / detrended / smoothed (running average) if applicable
            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
            kwargs["smoothing"] = deepcopy(smooth_ev)
            thf_mod, _, keyerror_mod = PreProcessTS(
                thf_mod, "", areacell=thf_mod_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            thf_obs, _, keyerror_obs = PreProcessTS(
                thf_obs, "", areacell=thf_obs_areacell, region="equatorial_pacific_LatExt2", **kwargs)
            kwargs["smoothing"] = deepcopy(smooth)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del thf_mod_areacell, thf_obs_areacell, smooth
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in thf_mod.getAxisList()]),
                              "axes2": "(obs thf) " + str([ax.id for ax in thf_obs.getAxisList()]),
                              "shape1": "(mod thf) " + str(thf_mod.shape), "shape2": "(obs thf) " + str(thf_obs.shape),
                              "time1": "(mod thf) " + str(TimeBounds(thf_mod)),
                              "time2": "(obs thf) " + str(TimeBounds(thf_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Seasonal mean and anomalies
            thf_mod = SeasonalMean(thf_mod, season_ev, compute_anom=True)
            thf_obs = SeasonalMean(thf_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in thf_mod.getAxisList()]),
                              "axes2": "(obs thf) " + str([ax.id for ax in thf_obs.getAxisList()]),
                              "shape1": "(mod thf) " + str(thf_mod.shape), "shape2": "(obs thf) " + str(thf_obs.shape),
                              "time1": "(mod thf) " + str(TimeBounds(thf_mod)),
                              "time2": "(obs thf) " + str(TimeBounds(thf_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                thf_mod, thf_obs, _ = TwoVarRegrid(
                    thf_mod, thf_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in thf_mod.getAxisList()]),
                                  "axes2": "(obs thf) " + str([ax.id for ax in thf_obs.getAxisList()]),
                                  "shape1": "(mod thf) " + str(thf_mod.shape),
                                  "shape2": "(obs thf) " + str(thf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Linear regression
            thf_map_slope_mod = LinearRegressionTsAgainstMap(thf_mod, enso_mod, return_stderr=False)
            thf_map_slope_obs = LinearRegressionTsAgainstMap(thf_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in thf_map_slope_mod.getAxisList()]),
                              "axes2": "(obs thf) " + str([ax.id for ax in thf_map_slope_obs.getAxisList()]),
                              "shape1": "(mod thf) " + str(thf_map_slope_mod.shape),
                              "shape2": "(obs thf) " + str(thf_map_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionTsAgainstMap: netcdf", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_thf_lon_mod = Composite(thf_lon_mod, ln_years_mod, kwargs["frequency"])
                ln_thf_map_mod = Composite(thf_mod, ln_years_mod, kwargs["frequency"])
            else:
                ln_thf_lon_mod = MyEmpty(thf_lon_mod[0], time=False)
                ln_thf_map_mod = MyEmpty(thf_mod[0], time=False)
            if len(en_years_mod) > 0:
                en_thf_lon_mod = Composite(thf_lon_mod, en_years_mod, kwargs["frequency"])
                en_thf_map_mod = Composite(thf_mod, en_years_mod, kwargs["frequency"])
            else:
                en_thf_lon_mod = MyEmpty(thf_lon_mod[0], time=False)
                en_thf_map_mod = MyEmpty(thf_mod[0], time=False)
            if len(ln_years_obs) > 0:
                ln_thf_lon_obs = Composite(thf_lon_obs, ln_years_obs, kwargs["frequency"])
                ln_thf_map_obs = Composite(thf_obs, ln_years_obs, kwargs["frequency"])
            else:
                ln_thf_lon_obs = MyEmpty(thf_lon_obs[0], time=False)
                ln_thf_map_obs = MyEmpty(thf_obs[0], time=False)
            if len(en_years_obs) > 0:
                en_thf_lon_obs = Composite(thf_lon_obs, en_years_obs, kwargs["frequency"])
                en_thf_map_obs = Composite(thf_obs, en_years_obs, kwargs["frequency"])
            else:
                en_thf_lon_obs = MyEmpty(thf_lon_obs[0], time=False)
                en_thf_map_obs = MyEmpty(thf_obs[0], time=False)
            # Supplementary metrics
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "W/m2"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = ["zonal curve of " + thfbox + " THFA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) +
                     "]) of La Nina events composite during " + season_ev + "; Nina = " + region_ev + " SSTA < -" +
                     my_thresh + " during " + season_ev + enso_method,
                     "zonal curve of " + thfbox + " THFA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) +
                     "]) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                     my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific THFA regressed onto " + region_ev + " averaged SSTA during " + season_ev +
                     enso_method3,
                     "map of tropical Pacific THFA of La Nina events composite during " + season_ev + "; Nina = " +
                     region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "map of tropical Pacific THFA of El Nino events composite during " + season_ev + "; Nino = " +
                     region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_thf_lon_mod, en_thf_lon_mod, thf_map_slope_mod, ln_thf_map_mod, en_thf_map_mod]
            my_ob = [ln_thf_lon_obs, en_thf_lon_obs, thf_map_slope_obs, ln_thf_map_obs, en_thf_map_obs]
            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
            my_ax = ["0", "0", "xy", "xy", "xy"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "description": method.split(", ")[0], "arraySTD_" + dataset1: sm_std_mod}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "description": method.split(", ")[0], "arraySTD_" + dataset2: sm_std_obs}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                     "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=thf_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=thf_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoPrTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                 prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                 sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                 prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox,
                 prbox, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                 debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrTsRmse() function computes PRA (precipitation anomalies) life cycle associated with ENSO in a 'prbox'
    (usually the nino3) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and lagging
    ENSO).
    It is the regression of 'prbox' averaged PRA time series onto 'region_ev' averaged SSTA (sea surface temperature
    anomalies) (usually the regression of nino3 PRA onto nino3.4 SSTA).

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr, prec, precip, rain) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, prec, precip, rain) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'nino3.4') for SST
    :param prbox: string
        name of box (e.g. 'nino3') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'ERA-Interim',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "ENSO life cyle PRA"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "mm/day/C"
    method = "temporal curve of " + prbox + " averaged precipitation anomalies (PRA) during " + \
             str(nbr_years_window) + " years (centered on ENSO) regressed onto " + region_ev + " averaged sea " + \
             "surface temperature anomalies (SSTA) during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoPrTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, keyerror, nbr_year_mod, nbr_year_obs = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            prfilemod, prnamemod, "precipitation", metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
            file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            prfileobs, prnameobs, "precipitation", metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
            file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst_mod, pr_mod, keyerror_mod = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
        sst_obs, pr_obs, keyerror_obs = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.4 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        # 2.2 PR in 'prbox' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, method, keyerror_mod2 = PreProcessTS(
            pr_mod, method, areacell=pr_mod_areacell, average="horizontal", compute_anom=True, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, "", areacell=pr_obs_areacell, average="horizontal", compute_anom=True, region=prbox, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        del pr_mod_areacell, pr_obs_areacell, smooth, sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                "axes4": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                "shape3": "(mod pr) " + str(pr_mod.shape), "shape4": "(obs pr) " + str(pr_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod pr) " + str(TimeBounds(pr_mod)), "time4": "(obs pr) " + str(TimeBounds(pr_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)

        # ------------------------------------------------
        # 4. Regression time series
        # ------------------------------------------------
        # 4.1 Regression time series
        pr_slope_mod = LinearRegressionTsAgainstTs(
            pr_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        pr_slope_obs = LinearRegressionTsAgainstTs(
            pr_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        if debug is True:
            dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_slope_mod.getAxisList()]),
                          "axes2": "(obs pr) " + str([ax.id for ax in pr_slope_obs.getAxisList()]),
                          "shape1": "(mod pr) " + str(pr_slope_mod.shape),
                          "shape2": "(obs pr) " + str(pr_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsAxis(pr_slope_mod, pr_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(pr_slope_mod, pr_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(pr_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(pr_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, map_mod_areacell, keyerror_mod = Read_data_mask_area(
                prfilemod, prnamemod, "precipitation", metric, "equatorial_pacific", file_area=prareafilemod,
                name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, map_obs_areacell, keyerror_obs = Read_data_mask_area(
                prfileobs, prnameobs, "precipitation", metric, "equatorial_pacific", file_area=prareafileobs,
                name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_mod, map_mod, keyerror_mod = CheckTime(sst_mod, map_mod, metric_name=metric, debug=debug, **kwargs)
            sst_obs, map_obs, keyerror_obs = CheckTime(sst_obs, map_obs, metric_name=metric, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess PR (computes anomalies, normalizes, detrends, smoothes, averages,...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=map_mod_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=map_obs_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del map_mod_areacell, map_obs_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs pr) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod pr) " + str(map_mod.shape), "shape2": "(obs pr) " + str(map_obs.shape),
                              "time1": "(mod pr) " + str(TimeBounds(map_mod)),
                              "time2": "(obs pr) " + str(TimeBounds(map_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs pr) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod pr) " + str(map_mod.shape),
                                  "shape2": "(obs pr) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Meridional average
            pr_hov_mod, keyerror_mod = AverageMeridional(map_mod)
            pr_hov_obs, keyerror_obs = AverageMeridional(map_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_hov_mod.getAxisList()]),
                              "axes2": "(obs pr) " + str([ax.id for ax in pr_hov_obs.getAxisList()]),
                              "shape1": "(mod pr) " + str(pr_hov_mod.shape),
                              "shape2": "(obs pr) " + str(pr_hov_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Linear regression
            pr_hov_slope_mod = LinearRegressionTsAgainstTs(
                pr_hov_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
            pr_hov_slope_obs = LinearRegressionTsAgainstTs(
                pr_hov_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_hov_slope_mod.getAxisList()]),
                              "axes2": "(obs pr) " + str([ax.id for ax in pr_hov_slope_obs.getAxisList()]),
                              "shape1": "(mod pr) " + str(pr_hov_slope_mod.shape),
                              "shape2": "(obs pr) " + str(pr_hov_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs: netcdf", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_pr_ts_mod = Composite(pr_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_pr_hov_mod = Composite(
                    pr_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_pr_ts_mod = MyEmpty(pr_mod[:12 * nbr_years_window], time=True, time_id="months")
                ln_pr_hov_mod = MyEmpty(pr_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_mod) > 0:
                en_pr_ts_mod = Composite(pr_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_pr_hov_mod = Composite(
                    pr_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_pr_ts_mod = MyEmpty(pr_mod[:12 * nbr_years_window], time=True, time_id="months")
                en_pr_hov_mod = MyEmpty(pr_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(ln_years_obs) > 0:
                ln_pr_ts_obs = Composite(pr_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_pr_hov_obs = Composite(
                    pr_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_pr_ts_obs = MyEmpty(pr_obs[:12 * nbr_years_window], time=True, time_id="months")
                ln_pr_hov_obs = MyEmpty(pr_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_obs) > 0:
                en_pr_ts_obs = Composite(pr_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_pr_hov_obs = Composite(
                    pr_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_pr_ts_obs = MyEmpty(pr_obs[:12 * nbr_years_window], time=True, time_id="months")
                en_pr_hov_obs = MyEmpty(pr_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            # Supplementary metrics
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "mm/day"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = ["temporal curve of " + prbox + " averaged PRA during " + str(nbr_years_window) + " years " +
                     "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " + region_ev +
                     " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "temporal curve of " + prbox + " averaged PRA during " + str(nbr_years_window) + " years " +
                     "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev +
                     " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                     "hovmoeller (time - longitude) of PRA across longitudes during " + str(nbr_years_window) +
                     " years (centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " + season_ev +
                     enso_method3,
                     "hovmoeller (time - longitude) of PRA across longitudes during " + str(nbr_years_window) +
                     " years (centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " +
                     region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "hovmoeller (time - longitude) of PRA during " + str(nbr_years_window) + " years (centered on " +
                     "ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                     my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_pr_ts_mod, en_pr_ts_mod, pr_hov_slope_mod, ln_pr_hov_mod, en_pr_hov_mod]
            my_ob = [ln_pr_ts_obs, en_pr_ts_obs, pr_hov_slope_obs, ln_pr_hov_obs, en_pr_hov_obs]
            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
            my_ax = ["0", "0", "01", "01", "01"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "arraySTD_" + dataset1: sm_std_mod, "description": method.split(", ")[0]}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "arraySTD_" + dataset1: sm_std_obs, "description": method.split(", ")[0]}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                     "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=pr_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=pr_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSshTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs,
                  sstbox, sshbox, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="",
                  dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSshTsRmse() function computes dynamic sea surface height anomalies life cycle associated with ENSO in a
    'sshbox' (usually the nino3.4) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and
    lagging ENSO).
    It is the regression of 'sshbox' averaged dynamic SSHA (dynamic sea surface height anomalies) time series onto
    'region_ev' averaged SSTA (sea surface temperature anomalies) (usually the regression of nino3 SSHA onto nino3.4
    SSTA).

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon May  3 2021

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SSH areacell
    :param sshareanamemod: string, optional
        name of areacell for the SSH variable (areacella, areacello,...) in 'sshareafilemod'
    :param sshlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SSH landmask
    :param sshlandmasknamemod: string, optional
        name of landmask for the SSH variable (sftlf,...) in 'sshlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SSH areacell
    :param sshareanameobs: string, optional
        name of areacell for the SSH variable (areacella, areacello,...) in 'sshareafileobs'
    :param sshlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SSH landmask
    :param sshlandmasknameobs: string, optional
        name of landmask for the SSH variable (sftlf,...) in 'sshlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'nino3.4') for SST
    :param sshbox: string
        name of box (e.g. 'nino3.4') for SSH
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'ORAS5',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSshTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "ENSO life cyle of dynamic SSHA"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "cm/C"
    method = "temporal curve of " + sshbox + " averaged dynamic sea surface height anomalies (SSHA) during " + \
             str(nbr_years_window) + " years (centered on ENSO) regressed onto " + region_ev + " averaged sea " + \
             "surface temperature anomalies (SSTA) during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoSshTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        ssh_mod, ssh_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, sshbox, file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_mod_glo, ssh_mod_glo_areacell, keyerror_mod3 = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, "global2", file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_obs, ssh_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, sshbox, file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        ssh_obs_glo, ssh_obs_glo_areacell, keyerror_obs3 = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, "global2", file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors(
            [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst_mod, ssh_mod, keyerror_mod1 = CheckTime(sst_mod, ssh_mod, metric_name=metric, debug=debug, **kwargs)
        sst_mod, ssh_mod_glo, keyerror_mod2 = CheckTime(sst_mod, ssh_mod_glo, metric_name=metric, debug=debug, **kwargs)
        sst_obs, ssh_obs, keyerror_obs1 = CheckTime(sst_obs, ssh_obs, metric_name=metric, debug=debug, **kwargs)
        sst_obs, ssh_obs_glo, keyerror_obs2 = CheckTime(sst_obs, ssh_obs_glo, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12))
        # 1.4 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        my_smo = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = False
        ssh_mod_glo, _, keyerror_mod = PreProcessTS(
            ssh_mod_glo, "", areacell=ssh_mod_glo_areacell, average="horizontal", region="global2", **kwargs)
        ssh_obs_glo, _, keyerror_obs = PreProcessTS(
            ssh_obs_glo, "", areacell=ssh_obs_glo_areacell, average="horizontal", region="global2", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_mod])
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        ssh_mod, method, keyerror_mod = remove_global_mean(ssh_mod, ssh_mod_glo, "SSH", method)
        ssh_obs, _, keyerror_obs = remove_global_mean(ssh_obs, ssh_obs_glo, "", "")
        keyerror = add_up_errors([keyerror_mod, keyerror_mod])
        if keyerror is not None:
            break
        # 2.3 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        kwargs["smoothing"] = deepcopy(my_smo)
        # 2.4 SSH averaged in 'sshbox' are normalized / detrended / smoothed (running average) if applicable
        ssh_ts_mod, method, keyerror_mod2 = PreProcessTS(
            ssh_mod, method, areacell=ssh_mod_areacell, average="horizontal", compute_anom=True, region=sshbox,
            **kwargs)
        ssh_ts_obs, _, keyerror_obs2 = PreProcessTS(
            ssh_obs, "", areacell=ssh_obs_areacell, average="horizontal", compute_anom=True, region=sshbox, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        del my_det, my_smo, ssh_mod_areacell, ssh_mod_glo_areacell, ssh_obs_areacell, ssh_obs_glo_areacell, \
            sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod ssh) " + str([ax.id for ax in ssh_ts_mod.getAxisList()]),
                "axes4": "(obs ssh) " + str([ax.id for ax in ssh_ts_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                "shape3": "(mod ssh) " + str(ssh_ts_mod.shape), "shape4": "(obs ssh) " + str(ssh_ts_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod ssh) " + str(TimeBounds(ssh_ts_mod)),
                "time4": "(obs ssh) " + str(TimeBounds(ssh_ts_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
        if debug is True:
            dict_debug = {"axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                          "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                          "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                          "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                          "time2": "(obs sst) " + str(TimeBounds(enso_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Regression
        # ------------------------------------------------
        ssh_slope_mod = LinearRegressionTsAgainstTs(
            ssh_ts_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        ssh_slope_obs = LinearRegressionTsAgainstTs(
            ssh_ts_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        if debug is True:
            dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_slope_mod.getAxisList()]),
                          "axes2": "(obs ssh) " + str([ax.id for ax in ssh_slope_obs.getAxisList()]),
                          "shape1": "(mod ssh) " + str(ssh_slope_mod.shape),
                          "shape2": "(obs ssh) " + str(ssh_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsAxis(ssh_slope_mod, ssh_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(ssh_slope_mod, ssh_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(ssh_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(ssh_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, map_mod_areacell, keyerror_mod = Read_data_mask_area(
                sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific", file_area=sshareafilemod,
                name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, map_obs_areacell, keyerror_obs = Read_data_mask_area(
                sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific", file_area=sshareafileobs,
                name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_mod, map_mod, keyerror_mod = CheckTime(sst_mod, map_mod, metric_name=metric, debug=debug, **kwargs)
            sst_obs, map_obs, keyerror_obs = CheckTime(sst_obs, map_obs, metric_name=metric, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Remove global mean SSH to local SSH at every time step
            map_mod, _, keyerror_mod = remove_global_mean(map_mod, ssh_mod_glo, "", "")
            map_obs, _, keyerror_obs = remove_global_mean(map_obs, ssh_obs_glo, "", "")
            keyerror = add_up_errors([keyerror_mod, keyerror_mod])
            if keyerror is not None:
                break
            # Preprocess SSH (computes anomalies, normalizes, detrends, smoothes, averages,...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=map_mod_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=map_obs_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del map_mod_areacell, map_obs_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs ssh) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod ssh) " + str(map_mod.shape), "shape2": "(obs ssh) " + str(map_obs.shape),
                              "time1": "(mod ssh) " + str(TimeBounds(map_mod)),
                              "time2": "(obs ssh) " + str(TimeBounds(map_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs ssh) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod ssh) " + str(map_mod.shape),
                                  "shape2": "(obs ssh) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
            # Meridional average
            ssh_hov_mod, keyerror_mod = AverageMeridional(map_mod)
            ssh_hov_obs, keyerror_obs = AverageMeridional(map_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_hov_mod.getAxisList()]),
                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_hov_obs.getAxisList()]),
                              "shape1": "(mod ssh) " + str(ssh_hov_mod.shape),
                              "shape2": "(obs ssh) " + str(ssh_hov_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
            # Linear regression
            ssh_hov_slope_mod = LinearRegressionTsAgainstTs(
                ssh_hov_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            ssh_hov_slope_obs = LinearRegressionTsAgainstTs(
                ssh_hov_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_hov_slope_mod.getAxisList()]),
                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_hov_slope_obs.getAxisList()]),
                              "shape1": "(mod ssh) " + str(ssh_hov_slope_mod.shape),
                              "shape2": "(obs ssh) " + str(ssh_hov_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_ssh_ts_mod = Composite(
                    ssh_ts_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_ssh_hov_mod = Composite(
                    ssh_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_ssh_ts_mod = MyEmpty(ssh_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                ln_ssh_hov_mod = MyEmpty(ssh_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_mod) > 0:
                en_ssh_ts_mod = Composite(
                    ssh_ts_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_ssh_hov_mod = Composite(
                    ssh_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_ssh_ts_mod = MyEmpty(ssh_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                en_ssh_hov_mod = MyEmpty(ssh_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(ln_years_obs) > 0:
                ln_ssh_ts_obs = Composite(
                    ssh_ts_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_ssh_hov_obs = Composite(
                    ssh_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_ssh_ts_obs = MyEmpty(ssh_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                ln_ssh_hov_obs = MyEmpty(ssh_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_obs) > 0:
                en_ssh_ts_obs = Composite(
                    ssh_ts_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_ssh_hov_obs = Composite(
                    ssh_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_ssh_ts_obs = MyEmpty(ssh_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                en_ssh_hov_obs = MyEmpty(ssh_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            # Supplementary metrics
            my_units = "" if kwargs["normalization"] is True else "cm"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = [
                "temporal curve of " + sshbox + " averaged dynamic SSHA during " + str(nbr_years_window) + " years " +
                "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " + region_ev +
                " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                "temporal curve of " + sshbox + " averaged dynamic SSHA during " + str(nbr_years_window) + " years " +
                "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev +
                " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                "hovmoeller (time - longitude) of dynamic SSHA during " + str(nbr_years_window) + " years (centered " +
                "on ENSO) regressed onto " + region_ev + " averaged SSTA during " + season_ev + enso_method3,
                "hovmoeller (time - longitude) of dynamic SSHA during " + str(nbr_years_window) + " years (centered " +
                "on ENSO) of La Nina events composite during " + season_ev + "; Nina = " + region_ev + " SSTA < -" +
                my_thresh + " during " + season_ev + enso_method,
                "hovmoeller (time - longitude) of dynamic SSHA during " + str(nbr_years_window) + " years (centered " +
                "on ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_ssh_ts_mod, en_ssh_ts_mod, ssh_hov_slope_mod, ln_ssh_hov_mod, en_ssh_hov_mod]
            my_ob = [ln_ssh_ts_obs, en_ssh_ts_obs, ssh_hov_slope_obs, ln_ssh_hov_obs, en_ssh_hov_obs]
            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
            my_ax = ["0", "0", "01", "01", "01"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "arraySTD_" + dataset1: sm_std_mod, "description": method.split(", ")[0]}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "arraySTD_" + dataset1: sm_std_obs, "description": method.split(", ")[0]}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                     "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=ssh_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=ssh_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSstTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  box, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                  debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstTsRmse() function computes SSTA (sea surface temperature anomalies) life cycle associated with ENSO in a
    'region_ev' (usually the nino3.4) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading
    and lagging ENSO).
    It is the regression of 'region_ev' averaged SSTA time series onto 'region_ev' averaged SSTA (usually the regression
    of nino3.4 SSTA onto nino3.4 SSTA).

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param box: string
        name of box (e.g. 'nino3.4') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3.4', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "ENSO life cyle SSTA"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "C/C"
    method = "temporal curve of " + region_ev + " averaged sea surface temperature anomalies (SSTA) during " + \
             str(nbr_years_window) + " years (centered on ENSO) regressed onto " + region_ev + " averaged SSTA " + \
             "during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoSstTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, keyerror, nbr_year_mod, nbr_year_obs = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        # 2.2 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_ts_mod, method, keyerror_mod2 = PreProcessTS(
            sst_mod, method, areacell=sst_mod_areacell, average="horizontal", compute_anom=True, region=region_ev,
            **kwargs)
        sst_ts_obs, _, keyerror_obs2 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", compute_anom=True, region=region_ev, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        del smooth, sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod enso) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs enso) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod sst) " + str([ax.id for ax in sst_ts_mod.getAxisList()]),
                "axes4": "(obs sst) " + str([ax.id for ax in sst_ts_obs.getAxisList()]),
                "shape1": "(mod enso) " + str(sst_box_mod.shape), "shape2": "(obs enso) " + str(sst_box_obs.shape),
                "shape3": "(mod sst) " + str(sst_ts_mod.shape), "shape4": "(obs sst) " + str(sst_ts_obs.shape),
                "time1": "(mod enso) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs enso) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod sst) " + str(TimeBounds(sst_ts_mod)),
                "time4": "(obs sst) " + str(TimeBounds(sst_ts_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)

        # ------------------------------------------------
        # 4. Regression time series
        # ------------------------------------------------
        # 4.1 Regression time series
        sst_slope_mod = LinearRegressionTsAgainstTs(
            sst_ts_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        sst_slope_obs = LinearRegressionTsAgainstTs(
            sst_ts_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        if debug is True:
            dict_debug = {"axes1": "(mod sst) " + str([ax.id for ax in sst_slope_mod.getAxisList()]),
                          "axes2": "(obs sst) " + str([ax.id for ax in sst_slope_obs.getAxisList()]),
                          "shape1": "(mod sst) " + str(sst_slope_mod.shape),
                          "shape2": "(obs sst) " + str(sst_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsAxis(sst_slope_mod, sst_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(sst_slope_mod, sst_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(sst_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(sst_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, map_mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific", file_area=sstareafilemod,
                name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, map_obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific", file_area=sstareafileobs,
                name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess SST (computes anomalies, normalizes, detrends, smoothes, averages,...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=map_mod_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=map_obs_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del map_mod_areacell, map_obs_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod sst) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs sst) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod sst) " + str(map_mod.shape), "shape2": "(obs sst) " + str(map_obs.shape),
                              "time1": "(mod sst) " + str(TimeBounds(map_mod)),
                              "time2": "(obs sst) " + str(TimeBounds(map_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs thf) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod thf) " + str(map_mod.shape),
                                  "shape2": "(obs thf) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Meridional average
            sst_hov_mod, keyerror_mod = AverageMeridional(map_mod)
            sst_hov_obs, keyerror_obs = AverageMeridional(map_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                              "axes2": "(obs thf) " + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                              "shape1": "(mod thf) " + str(sst_hov_mod.shape),
                              "shape2": "(obs thf) " + str(sst_hov_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Linear regression
            sst_hov_slope_mod = LinearRegressionTsAgainstTs(
                sst_hov_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            sst_hov_slope_obs = LinearRegressionTsAgainstTs(
                sst_hov_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(mod sst) " + str([ax.id for ax in sst_hov_slope_mod.getAxisList()]),
                              "axes2": "(obs sst) " + str([ax.id for ax in sst_hov_slope_obs.getAxisList()]),
                              "shape1": "(mod sst) " + str(sst_hov_slope_mod.shape),
                              "shape2": "(obs sst) " + str(sst_hov_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs: netcdf", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_sst_ts_mod = Composite(
                    sst_ts_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_sst_hov_mod = Composite(
                    sst_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_sst_ts_mod = MyEmpty(sst_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                ln_sst_hov_mod = MyEmpty(sst_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_mod) > 0:
                en_sst_ts_mod = Composite(
                    sst_ts_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_sst_hov_mod = Composite(
                    sst_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_sst_ts_mod = MyEmpty(sst_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                en_sst_hov_mod = MyEmpty(sst_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(ln_years_obs) > 0:
                ln_sst_ts_obs = Composite(
                    sst_ts_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_sst_hov_obs = Composite(
                    sst_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_sst_ts_obs = MyEmpty(sst_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                ln_sst_hov_obs = MyEmpty(sst_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_obs) > 0:
                en_sst_ts_obs = Composite(
                    sst_ts_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_sst_hov_obs = Composite(
                    sst_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_sst_ts_obs = MyEmpty(sst_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                en_sst_hov_obs = MyEmpty(sst_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            # Supplementary metrics
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "C"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = ["temporal curve of " + box + " averaged SSTA during " + str(nbr_years_window) + " years " +
                     "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " + region_ev +
                     " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "temporal curve of " + box + " averaged SSTA during " + str(nbr_years_window) + " years " +
                     "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev +
                     " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                     "hovmoeller (time - longitude) of SSTA across longitudes during " + str(nbr_years_window) +
                     " years (centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " + season_ev +
                     enso_method3,
                     "hovmoeller (time - longitude) of SSTA across longitudes during " + str(nbr_years_window) +
                     " years (centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " +
                     region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "hovmoeller (time - longitude) of SSTA during " + str(nbr_years_window) + " years (centered on " +
                     "ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                     my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_sst_ts_mod, en_sst_ts_mod, sst_hov_slope_mod, ln_sst_hov_mod, en_sst_hov_mod]
            my_ob = [ln_sst_ts_obs, en_sst_ts_obs, sst_hov_slope_obs, ln_sst_hov_obs, en_sst_hov_obs]
            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
            my_ax = ["0", "0", "01", "01", "01"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "arraySTD_" + dataset1: sm_std_mod, "description": method.split(", ")[0]}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "arraySTD_" + dataset1: sm_std_obs, "description": method.split(", ")[0]}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                     "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sst_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=sst_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoTauxTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                   taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs,
                   sstbox, taubox, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="",
                   dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoTauxTsRmse() function computes TauxA (zonal wind stress anomalies) life cycle associated with ENSO in a
    'taubox' (usually the nino4) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and
    lagging ENSO).
    It is the regression of 'taubox' averaged TauxA time series onto 'region_ev' averaged SSTA (sea surface temperature
    anomalies) (usually the regression of nino3 TauxA onto nino3.4 SSTA).

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon Sep 17 2018

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Taux
    :param taunamemod: string
        name of Taux variable (tauu, tauuo, taux) in 'taufilemod'
    :param tauareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled Taux areacell
    :param tauareanamemod: string, optional
        name of areacell for the Taux variable (areacella, areacello,...) in 'tauareafilemod'
    :param taulandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled Taux landmask
    :param taulandmasknamemod: string, optional
        name of landmask for the Taux variable (sftlf,...) in 'taulandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Taux
    :param taunameobs: string
        name of Taux variable (tauu, tauuo, taux) in 'taufileobs'
    :param tauareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed Taux areacell
    :param tauareanameobs: string, optional
        name of areacell for the Taux variable (areacella, areacello,...) in 'tauareafileobs'
    :param taulandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed Taux landmask
    :param taulandmasknameobs: string, optional
        name of landmask for the Taux variable (sftlf,...) in 'taulandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'nino3.4') for SST
    :param taubox: string
        name of box (e.g. 'nino4') for Taux
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'ERA-Interim',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoTauxTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "ENSO life cyle TauxA"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "1e-3 N/m2/C"
    method = "temporal curve of " + taubox + " averaged zonal wind stress anomalies (TauxA) during " + \
             str(nbr_years_window) + " years (centered on ENSO) regressed onto " + region_ev + " averaged sea " + \
             "surface temperature anomalies (SSTA) during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoTauxTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, keyerror, nbr_year_mod, nbr_year_obs = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        tau_mod, tau_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, taubox, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, tau_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, taubox, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst_mod, tau_mod, keyerror_mod = CheckTime(sst_mod, tau_mod, metric_name=metric, debug=debug, **kwargs)
        sst_obs, tau_obs, keyerror_obs = CheckTime(sst_obs, tau_obs, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.4 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        # 2.2 Taux in 'taubox' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod2 = PreProcessTS(
            tau_mod, method, areacell=tau_mod_areacell, average="horizontal", compute_anom=True, region=taubox,
            **kwargs)
        tau_obs, _, keyerror_obs2 = PreProcessTS(
            tau_obs, "", areacell=tau_obs_areacell, average="horizontal", compute_anom=True, region=taubox, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        del tau_mod_areacell, tau_obs_areacell, smooth, sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                "axes4": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                "shape3": "(mod Tau) " + str(tau_mod.shape), "shape4": "(obs Tau) " + str(tau_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod Tau) " + str(TimeBounds(tau_mod)), "time4": "(obs Tau) " + str(TimeBounds(tau_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)

        # ------------------------------------------------
        # 4. Regression time series
        # ------------------------------------------------
        # 4.1 Regression time series
        tau_slope_mod = LinearRegressionTsAgainstTs(
            tau_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        tau_slope_obs = LinearRegressionTsAgainstTs(
            tau_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        if debug is True:
            dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_slope_mod.getAxisList()]),
                          "axes2": "(obs Tau) " + str([ax.id for ax in tau_slope_obs.getAxisList()]),
                          "shape1": "(mod Tau) " + str(tau_slope_mod.shape),
                          "shape2": "(obs Tau) " + str(tau_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsAxis(tau_slope_mod, tau_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(tau_slope_mod, tau_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tau_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tau_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, map_mod_areacell, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, map_obs_areacell, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_mod, map_mod, keyerror_mod = CheckTime(sst_mod, map_mod, metric_name=metric, debug=debug, **kwargs)
            sst_obs, map_obs, keyerror_obs = CheckTime(sst_obs, map_obs, metric_name=metric, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess Taux (computes anomalies, normalizes, detrends, smoothes, averages,...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=map_mod_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=map_obs_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del map_mod_areacell, map_obs_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(map_mod.shape), "shape2": "(obs Tau) " + str(map_obs.shape),
                              "time1": "(mod Tau) " + str(TimeBounds(map_mod)),
                              "time2": "(obs Tau) " + str(TimeBounds(map_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs Tau) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod Tau) " + str(map_mod.shape),
                                  "shape2": "(obs Tau) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Meridional average
            tau_hov_mod, keyerror_mod = AverageMeridional(map_mod)
            tau_hov_obs, keyerror_obs = AverageMeridional(map_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_hov_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_hov_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_hov_mod.shape),
                              "shape2": "(obs Tau) " + str(tau_hov_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Linear regression
            tau_hov_slope_mod = LinearRegressionTsAgainstTs(
                tau_hov_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            tau_hov_slope_obs = LinearRegressionTsAgainstTs(
                tau_hov_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_hov_slope_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_hov_slope_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_hov_slope_mod.shape),
                              "shape2": "(obs Tau) " + str(tau_hov_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs: netcdf", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_tau_ts_mod = Composite(tau_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_tau_hov_mod = Composite(
                    tau_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_tau_ts_mod = MyEmpty(tau_mod[:12 * nbr_years_window], time=True, time_id="months")
                ln_tau_hov_mod = MyEmpty(tau_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_mod) > 0:
                en_tau_ts_mod = Composite(tau_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_tau_hov_mod = Composite(
                    tau_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_tau_ts_mod = MyEmpty(tau_mod[:12 * nbr_years_window], time=True, time_id="months")
                en_tau_hov_mod = MyEmpty(tau_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(ln_years_obs) > 0:
                ln_tau_ts_obs = Composite(tau_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_tau_hov_obs = Composite(
                    tau_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_tau_ts_obs = MyEmpty(tau_obs[:12 * nbr_years_window], time=True, time_id="months")
                ln_tau_hov_obs = MyEmpty(tau_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_obs) > 0:
                en_tau_ts_obs = Composite(tau_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_tau_hov_obs = Composite(
                    tau_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_tau_ts_obs = MyEmpty(tau_obs[:12 * nbr_years_window], time=True, time_id="months")
                en_tau_hov_obs = MyEmpty(tau_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            # Supplementary metrics
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "1e-3 N/m2"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = ["temporal curve of " + taubox + " averaged TauxA during " + str(nbr_years_window) + " years " +
                     "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " + region_ev +
                     " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "temporal curve of " + taubox + " averaged TauxA during " + str(nbr_years_window) + " years " +
                     "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev +
                     " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                     "hovmoeller (time - longitude) of TauxA across longitudes during " + str(nbr_years_window) +
                     " years (centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " + season_ev +
                     enso_method3,
                     "hovmoeller (time - longitude) of TauxA across longitudes during " + str(nbr_years_window) +
                     " years (centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " +
                     region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "hovmoeller (time - longitude) of TauxA during " + str(nbr_years_window) + " years (centered on " +
                     "ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                     my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_tau_ts_mod, en_tau_ts_mod, tau_hov_slope_mod, ln_tau_hov_mod, en_tau_hov_mod]
            my_ob = [ln_tau_ts_obs, en_tau_ts_obs, tau_hov_slope_obs, ln_tau_hov_obs, en_tau_hov_obs]
            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
            my_ax = ["0", "0", "01", "01", "01"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "arraySTD_" + dataset1: sm_std_mod, "description": method.split(", ")[0]}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "arraySTD_" + dataset1: sm_std_obs, "description": method.split(", ")[0]}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                     "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tau_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tau_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoTauyTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                   taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs,
                   sstbox, taubox, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="",
                   dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoTauyTsRmse() function computes TauyA (meridional wind stress anomalies) life cycle associated with ENSO in a
    'taubox' (usually the nino3.4) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and
    lagging ENSO).
    It is the regression of 'taubox' averaged TauyA time series onto 'region_ev' averaged SSTA (sea surface temperature
    anomalies) (usually the regression of nino3.4 TauyA onto nino3.4 SSTA).

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Wed Dec  9 2020

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Tauy
    :param taunamemod: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufilemod'
    :param tauareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled Tauy areacell
    :param tauareanamemod: string, optional
        name of areacell for the Tauy variable (areacella, areacello,...) in 'tauareafilemod'
    :param taulandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled Tauy landmask
    :param taulandmasknamemod: string, optional
        name of landmask for the Tauy variable (sftlf,...) in 'taulandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Tauy
    :param taunameobs: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufileobs'
    :param tauareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed Tauy areacell
    :param tauareanameobs: string, optional
        name of areacell for the Tauy variable (areacella, areacello,...) in 'tauareafileobs'
    :param taulandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed Tauy landmask
    :param taulandmasknameobs: string, optional
        name of landmask for the Tauy variable (sftlf,...) in 'taulandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'nino3.4') for SST
    :param taubox: string
        name of box (e.g. 'nino3.4') for Tauy
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'ERA-Interim',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoTauyTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "ENSO life cyle TauyA"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "1e-3 N/m2/C"
    method = "temporal curve of " + taubox + " averaged meridional wind stress anomalies (TauyA) during " + \
             str(nbr_years_window) + " years (centered on ENSO) regressed onto " + region_ev + " averaged sea " + \
             "surface temperature anomalies (SSTA) during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoTauyTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, keyerror, nbr_year_mod, nbr_year_obs = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        tau_mod, tau_mod_areacell, keyerror_mod2 = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, taubox, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, tau_obs_areacell, keyerror_obs2 = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, taubox, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst_mod, tau_mod, keyerror_mod = CheckTime(sst_mod, tau_mod, metric_name=metric, debug=debug, **kwargs)
        sst_obs, tau_obs, keyerror_obs = CheckTime(sst_obs, tau_obs, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.4 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        # 2.2 Tauy in 'taubox' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod2 = PreProcessTS(
            tau_mod, method, areacell=tau_mod_areacell, average="horizontal", compute_anom=True, region=taubox,
            **kwargs)
        tau_obs, _, keyerror_obs2 = PreProcessTS(
            tau_obs, "", areacell=tau_obs_areacell, average="horizontal", compute_anom=True, region=taubox, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        del tau_mod_areacell, tau_obs_areacell, smooth, sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod Tau) " + str([ax.id for ax in tau_mod.getAxisList()]),
                "axes4": "(obs Tau) " + str([ax.id for ax in tau_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                "shape3": "(mod Tau) " + str(tau_mod.shape), "shape4": "(obs Tau) " + str(tau_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod Tau) " + str(TimeBounds(tau_mod)), "time4": "(obs Tau) " + str(TimeBounds(tau_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)

        # ------------------------------------------------
        # 4. Regression time series
        # ------------------------------------------------
        # 4.1 Regression time series
        tau_slope_mod = LinearRegressionTsAgainstTs(
            tau_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        tau_slope_obs = LinearRegressionTsAgainstTs(
            tau_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        if debug is True:
            dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_slope_mod.getAxisList()]),
                          "axes2": "(obs Tau) " + str([ax.id for ax in tau_slope_obs.getAxisList()]),
                          "shape1": "(mod Tau) " + str(tau_slope_mod.shape),
                          "shape2": "(obs Tau) " + str(tau_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsAxis(tau_slope_mod, tau_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(tau_slope_mod, tau_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tau_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tau_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, map_mod_areacell, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, map_obs_areacell, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_mod, map_mod, keyerror_mod = CheckTime(sst_mod, map_mod, metric_name=metric, debug=debug, **kwargs)
            sst_obs, map_obs, keyerror_obs = CheckTime(sst_obs, map_obs, metric_name=metric, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess Tauy (computes anomalies, normalizes, detrends, smoothes, averages,...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=map_mod_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=map_obs_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del map_mod_areacell, map_obs_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(map_mod.shape), "shape2": "(obs Tau) " + str(map_obs.shape),
                              "time1": "(mod Tau) " + str(TimeBounds(map_mod)),
                              "time2": "(obs Tau) " + str(TimeBounds(map_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs Tau) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod Tau) " + str(map_mod.shape),
                                  "shape2": "(obs Tau) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Meridional average
            tau_hov_mod, keyerror_mod = AverageMeridional(map_mod)
            tau_hov_obs, keyerror_obs = AverageMeridional(map_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_hov_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_hov_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_hov_mod.shape),
                              "shape2": "(obs Tau) " + str(tau_hov_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Linear regression
            tau_hov_slope_mod = LinearRegressionTsAgainstTs(
                tau_hov_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            tau_hov_slope_obs = LinearRegressionTsAgainstTs(
                tau_hov_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(mod Tau) " + str([ax.id for ax in tau_hov_slope_mod.getAxisList()]),
                              "axes2": "(obs Tau) " + str([ax.id for ax in tau_hov_slope_obs.getAxisList()]),
                              "shape1": "(mod Tau) " + str(tau_hov_slope_mod.shape),
                              "shape2": "(obs Tau) " + str(tau_hov_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs: netcdf", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_tau_ts_mod = Composite(tau_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_tau_hov_mod = Composite(
                    tau_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_tau_ts_mod = MyEmpty(tau_mod[:12 * nbr_years_window], time=True, time_id="months")
                ln_tau_hov_mod = MyEmpty(tau_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_mod) > 0:
                en_tau_ts_mod = Composite(tau_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_tau_hov_mod = Composite(
                    tau_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_tau_ts_mod = MyEmpty(tau_mod[:12 * nbr_years_window], time=True, time_id="months")
                en_tau_hov_mod = MyEmpty(tau_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(ln_years_obs) > 0:
                ln_tau_ts_obs = Composite(tau_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_tau_hov_obs = Composite(
                    tau_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_tau_ts_obs = MyEmpty(tau_obs[:12 * nbr_years_window], time=True, time_id="months")
                ln_tau_hov_obs = MyEmpty(tau_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_obs) > 0:
                en_tau_ts_obs = Composite(tau_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_tau_hov_obs = Composite(
                    tau_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_tau_ts_obs = MyEmpty(tau_obs[:12 * nbr_years_window], time=True, time_id="months")
                en_tau_hov_obs = MyEmpty(tau_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            # Supplementary metrics
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "1e-3 N/m2"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = ["temporal curve of " + taubox + " averaged TauyA during " + str(nbr_years_window) + " years " +
                     "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " + region_ev +
                     " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "temporal curve of " + taubox + " averaged TauyA during " + str(nbr_years_window) + " years " +
                     "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev +
                     " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                     "hovmoeller (time - longitude) of TauyA across longitudes during " + str(nbr_years_window) +
                     " years (centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " + season_ev +
                     enso_method3,
                     "hovmoeller (time - longitude) of TauyA across longitudes during " + str(nbr_years_window) +
                     " years (centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " +
                     region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "hovmoeller (time - longitude) of TauyA during " + str(nbr_years_window) + " years (centered on " +
                     "ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                     my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_tau_ts_mod, en_tau_ts_mod, tau_hov_slope_mod, ln_tau_hov_mod, en_tau_hov_mod]
            my_ob = [ln_tau_ts_obs, en_tau_ts_obs, tau_hov_slope_obs, ln_tau_hov_obs, en_tau_hov_obs]
            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
            my_ax = ["0", "0", "01", "01", "01"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "arraySTD_" + dataset1: sm_std_mod, "description": method.split(", ")[0]}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "arraySTD_" + dataset1: sm_std_obs, "description": method.split(", ")[0]}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                     "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tau_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tau_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoThfTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  thffilemod, thfnamemod, thfareafilemod, thfareanamemod, thflandmaskfilemod, thflandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  thffileobs, thfnameobs, thfareafileobs, thfareanameobs, thflandmaskfileobs, thflandmasknameobs,
                  sstbox, thfbox, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="",
                  dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoThfTsRmse() function computes THFA (net heat flux anomalies) life cycle associated with ENSO in a 'thfbox'
    (usually the nino3.4) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and lagging
    ENSO).
    It is the regression of 'thfbox' averaged THFA time series onto 'region_ev' averaged SSTA (sea surface temperature
    anomalies) (usually the regression of nino3.4 THFA onto nino3.4 SSTA).
    The net heat flux is the sum of four term:
         - net surface shortwave radiation,
         - net surface longwave radiation,
         - latent heat flux,
         - sensible heat flux

    The net heat flux is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into thffile and thfname of this
    function a list() of the four needed files and variable names (CMIP: rsds-rsus, rlds-rlus, hfls, hfss)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Wed Dec  9 2020

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param thffilemod: string
        path_to/filename of the file (NetCDF) of the modeled THF
    :param thfnamemod: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffilemod'
    :param thfareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled THF areacell
    :param thfareanamemod: string, optional
        name of areacell for the THF variable (areacella, areacello,...) in 'thfareafilemod'
    :param thflandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled THF landmask
    :param thflandmasknamemod: string, optional
        name of landmask for the THF variable (sftlf,...) in 'thflandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param thffileobs: string
        path_to/filename of the file (NetCDF) of the observed THF
    :param thfnameobs: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffileobs'
    :param thfareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed THF areacell
    :param thfareanameobs: string, optional
        name of areacell for the THF variable (areacella, areacello,...) in 'thfareafileobs'
    :param thflandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed THF landmask
    :param thflandmasknameobs: string, optional
        name of landmask for the THF variable (sftlf,...) in 'thflandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'nino3.4') for SST
    :param thfbox: string
        name of box (e.g. 'nino3.4') for THF
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3.4', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'ERA-Interim',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoThfTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # Setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "ENSO life cyle THFA"
    units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "W/m2/C"
    method = "temporal curve of " + thfbox + " averaged net heat flux anomalies (THFA) during " + \
             str(nbr_years_window) + " years (centered on ENSO) regressed onto " + region_ev + " averaged sea " + \
             "surface temperature anomalies (SSTA) during " + season_ev + enso_method3
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoThfTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    actualtimebounds_mod, actualtimebounds_obs, keyerror, nbr_year_mod, nbr_year_obs = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod,
            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs,
            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        thf_mod, thf_mod_areacell, keyerror_mod2 = Read_data_mask_area_multifile(
            thffilemod, thfnamemod, "heat flux", "thf", metric, thfbox, file_area=thfareafilemod,
            name_area=thfareanamemod, file_mask=thflandmaskfilemod, name_mask=thflandmasknamemod, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug,
            interpreter="project_interpreter_mod_var2", **kwargs)
        thf_obs, thf_obs_areacell, keyerror_obs2 = Read_data_mask_area_multifile(
            thffileobs, thfnameobs, "heat flux", "thf", metric, thfbox, file_area=thfareafileobs,
            name_area=thfareanameobs, file_mask=thflandmaskfileobs, name_mask=thflandmasknameobs, maskland=True,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug,
            interpreter="project_interpreter_obs_var2", **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst_mod, thf_mod, keyerror_mod = CheckTime(sst_mod, thf_mod, metric_name=metric, debug=debug, **kwargs)
        sst_obs, thf_obs, keyerror_obs = CheckTime(sst_obs, thf_obs, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.4 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = deepcopy(smooth_ev)
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=sst_mod_areacell, average="horizontal", region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=sst_obs_areacell, average="horizontal", region=region_ev, **kwargs)
        kwargs["smoothing"] = deepcopy(smooth)
        # 2.2 THF in 'thfbox' are normalized / detrended / smoothed (running average) if applicable
        thf_mod, method, keyerror_mod2 = PreProcessTS(
            thf_mod, method, areacell=thf_mod_areacell, average="horizontal", compute_anom=True, region=thfbox,
            **kwargs)
        thf_obs, _, keyerror_obs2 = PreProcessTS(
            thf_obs, "", areacell=thf_obs_areacell, average="horizontal", compute_anom=True, region=thfbox, **kwargs)
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        del thf_mod_areacell, thf_obs_areacell, smooth, sst_mod_areacell, sst_obs_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {
                "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                "axes3": "(mod thf) " + str([ax.id for ax in thf_mod.getAxisList()]),
                "axes4": "(obs thf) " + str([ax.id for ax in thf_obs.getAxisList()]),
                "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                "shape3": "(mod thf) " + str(thf_mod.shape), "shape4": "(obs thf) " + str(thf_obs.shape),
                "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                "time3": "(mod thf) " + str(TimeBounds(thf_mod)), "time4": "(obs thf) " + str(TimeBounds(thf_obs))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Seasonal mean and anomalies
        # ------------------------------------------------
        # 3.1 horizontally averaged SST
        enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
        enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)

        # ------------------------------------------------
        # 4. Regression time series
        # ------------------------------------------------
        # 4.1 Regression time series
        thf_slope_mod = LinearRegressionTsAgainstTs(
            thf_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        thf_slope_obs = LinearRegressionTsAgainstTs(
            thf_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
        if debug is True:
            dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in thf_slope_mod.getAxisList()]),
                          "axes2": "(obs thf) " + str([ax.id for ax in thf_slope_obs.getAxisList()]),
                          "shape1": "(mod thf) " + str(thf_slope_mod.shape),
                          "shape2": "(obs thf) " + str(thf_slope_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

        # ------------------------------------------------
        # 5. Metric value and supplementary metric values
        # ------------------------------------------------
        # 5.1 Computes the root mean square difference
        mv, keyerror = RmsAxis(thf_slope_mod, thf_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 5.2 Supplementary metrics
        sm_corr = float(Correlation(thf_slope_mod, thf_slope_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(thf_slope_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(thf_slope_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 6. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, map_mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                thffilemod, thfnamemod, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                file_area=thfareafilemod, name_area=thfareanamemod, file_mask=thflandmaskfilemod,
                name_mask=thflandmasknamemod, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, interpreter="project_interpreter_mod_var2", **kwargs)
            map_obs, map_obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                thffileobs, thfnameobs, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                file_area=thfareafileobs, name_area=thfareanameobs, file_mask=thflandmaskfileobs,
                name_mask=thflandmasknameobs, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, interpreter="project_interpreter_obs_var2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_mod, map_mod, keyerror_mod = CheckTime(sst_mod, map_mod, metric_name=metric, debug=debug, **kwargs)
            sst_obs, map_obs, keyerror_obs = CheckTime(sst_obs, map_obs, metric_name=metric, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess THF (computes anomalies, normalizes, detrends, smoothes, averages,...)
            map_mod, _, keyerror_mod = PreProcessTS(
                map_mod, "", areacell=map_mod_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(
                map_obs, "", areacell=map_obs_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del map_mod_areacell, map_obs_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs thf) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod thf) " + str(map_mod.shape), "shape2": "(obs thf) " + str(map_obs.shape),
                              "time1": "(mod thf) " + str(TimeBounds(map_mod)),
                              "time2": "(obs thf) " + str(TimeBounds(map_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs thf) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod thf) " + str(map_mod.shape),
                                  "shape2": "(obs thf) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
            # Meridional average
            thf_hov_mod, keyerror_mod = AverageMeridional(map_mod)
            thf_hov_obs, keyerror_obs = AverageMeridional(map_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in thf_hov_mod.getAxisList()]),
                              "axes2": "(obs thf) " + str([ax.id for ax in thf_hov_obs.getAxisList()]),
                              "shape1": "(mod thf) " + str(thf_hov_mod.shape),
                              "shape2": "(obs thf) " + str(thf_hov_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Linear regression
            thf_hov_slope_mod = LinearRegressionTsAgainstTs(
                thf_hov_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            thf_hov_slope_obs = LinearRegressionTsAgainstTs(
                thf_hov_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(mod thf) " + str([ax.id for ax in thf_hov_slope_mod.getAxisList()]),
                              "axes2": "(obs thf) " + str([ax.id for ax in thf_hov_slope_obs.getAxisList()]),
                              "shape1": "(mod thf) " + str(thf_hov_slope_mod.shape),
                              "shape2": "(obs thf) " + str(thf_hov_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs: netcdf", 15, **dict_debug)
            # Lists event years
            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                        compute_season=True, duration=length_ev)
            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize, nino=True,
                                        compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                              "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                              "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                              "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents: netcdf", 15, **dict_debug)
            # Composites
            if len(ln_years_mod) > 0:
                ln_thf_ts_mod = Composite(thf_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_thf_hov_mod = Composite(
                    thf_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_thf_ts_mod = MyEmpty(thf_mod[:12 * nbr_years_window], time=True, time_id="months")
                ln_thf_hov_mod = MyEmpty(thf_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_mod) > 0:
                en_thf_ts_mod = Composite(thf_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_thf_hov_mod = Composite(
                    thf_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_thf_ts_mod = MyEmpty(thf_mod[:12 * nbr_years_window], time=True, time_id="months")
                en_thf_hov_mod = MyEmpty(thf_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
            if len(ln_years_obs) > 0:
                ln_thf_ts_obs = Composite(thf_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                ln_thf_hov_obs = Composite(
                    thf_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                ln_thf_ts_obs = MyEmpty(thf_obs[:12 * nbr_years_window], time=True, time_id="months")
                ln_thf_hov_obs = MyEmpty(thf_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            if len(en_years_obs) > 0:
                en_thf_ts_obs = Composite(thf_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                en_thf_hov_obs = Composite(
                    thf_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            else:
                en_thf_ts_obs = MyEmpty(thf_obs[:12 * nbr_years_window], time=True, time_id="months")
                en_thf_hov_obs = MyEmpty(thf_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
            # Supplementary metrics
            my_units = "" if "normalization" in list(kwargs.keys()) and kwargs["normalization"] is True else "W/m2"
            dict_metric, dict_nc = dict(), dict()
            nbr = 3
            my_ev = ["nina", "nino", None, "nina", "nino"]
            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
            my_de = ["temporal curve of " + thfbox + " averaged THFA during " + str(nbr_years_window) + " years " +
                     "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " + region_ev +
                     " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "temporal curve of " + thfbox + " averaged THFA during " + str(nbr_years_window) + " years " +
                     "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev +
                     " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                     "hovmoeller (time - longitude) of THFA across longitudes during " + str(nbr_years_window) +
                     " years (centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " + season_ev +
                     enso_method3,
                     "hovmoeller (time - longitude) of THFA across longitudes during " + str(nbr_years_window) +
                     " years (centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " +
                     region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                     "hovmoeller (time - longitude) of THFA during " + str(nbr_years_window) + " years (centered on " +
                     "ENSO) of El Nino events composite during " + season_ev + "; Nino = " + region_ev + " SSTA > " +
                     my_thresh + " during " + season_ev + enso_method]
            my_un = [my_units, my_units, units, my_units, my_units]
            my_mo = [ln_thf_ts_mod, en_thf_ts_mod, thf_hov_slope_mod, ln_thf_hov_mod, en_thf_hov_mod]
            my_ob = [ln_thf_ts_obs, en_thf_ts_obs, thf_hov_slope_obs, ln_thf_hov_obs, en_thf_hov_obs]
            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
            my_ax = ["0", "0", "01", "01", "01"]
            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                dict_metric, dict_nc = fill_dict_axis(
                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                    nbr_year_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                    dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=des)
                nbr += 2
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "time_period": str(actualtimebounds_mod),
                     "arraySTD_" + dataset1: sm_std_mod, "description": method.split(", ")[0]}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "time_period": str(actualtimebounds_obs),
                     "arraySTD_" + dataset1: sm_std_obs, "description": method.split(", ")[0]}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"],
                     "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                     "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                     "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=thf_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=thf_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, my_eo, my_mo, my_ob, \
                my_ty, my_un, my_units, nbr
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_period_model": actualtimebounds_mod,
        "time_period_observations": actualtimebounds_obs, "time_frequency": kwargs["frequency"], "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSshSst(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox,
                 sshfile, sshname, sshareafile, sshareaname, sshlandmaskfile, sshlandmaskname, sshbox,
                 dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSshSst() function computes the regression of 'sstbox' SSTA (sea surface temperature anomalies) onto
    'sshbox' dynamic SSHA (sea shurface height anomalies) (usually the regression of nino3 SSTA onto nino3 dynamic SSHA)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box ('nino3') for SST
    :param sshfile: string
        path_to/filename of the file (NetCDF) of SSH
    :param sshname: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfile'
    :param sshareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SSH
    :param sshareaname: string
        name of areacell variable (areacella, areacello) in 'sshareafile'
    :param sshlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SSH
    :param sshlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfile'
    :param sshbox: string
        name of box ('nino3') for SSH
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSshSst_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "SST-SSH feedback"
    units = "C/cm"
    method = "Regression of " + sstbox + " sea surface temperature anomalies (SSTA) onto " + sshbox + " dynamic " + \
             "dynamic sea surface height anomalies (SSHA)"
    method_NL = "The nonlinearity is the regression computed when " + sshbox + " dynamic SSHA<0 minus the " + \
                "regression computed when " + sshbox + " dynamic SSHA>0"
    ref = "Using CDAT regression calculation"
    metric = "EnsoFbSshSst"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag, actualtimebounds, nbr_year, keyerror = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        ssh, ssh_areacell, keyerror2 = Read_data_mask_area(
            sshfile, sshname, "sea surface height", metric, sshbox, file_area=sshareafile, name_area=sshareaname,
            file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False, maskocean=False, debug=debug,
            **kwargs)
        glo, glo_areacell, keyerror3 = Read_data_mask_area(
            sshfile, sshname, "sea surface height", metric, "global2", file_area=sshareafile, name_area=sshareaname,
            file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False, maskocean=False, debug=debug,
            **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst, ssh, keyerror1 = CheckTime(sst, ssh, metric_name=metric, debug=debug, **kwargs)
        sst, glo, keyerror2 = CheckTime(sst, glo, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.3 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.4 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        glo, _, glo_keyerror = PreProcessTS(
            glo, "", areacell=glo_areacell, average="horizontal", region="global2", **kwargs)
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        ssh, method, keyerror = remove_global_mean(ssh, glo, "SSH", method)
        if keyerror is not None:
            break
        # 2.3 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror1 = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        # 2.3 SSH averaged in 'sshbox' are normalized / detrended / smoothed (running average) if applicable
        ssh, _, keyerror2 = PreProcessTS(
            ssh, "", areacell=ssh_areacell, average="horizontal", compute_anom=True, region=sshbox, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del my_det, ssh_areacell, sst_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                          "axes2": "(ssh) " + str([ax.id for ax in ssh.getAxisList()]),
                          "shape1": "(sst) " + str(sst.shape), "shape2": "(ssh) " + str(ssh.shape),
                          "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(ssh) " + str(TimeBounds(ssh))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
            
        # ------------------------------------------------
        # 3. Regression (diagnostic value and supplementary diagnostic values)
        # ------------------------------------------------
        # 3.1 Computes the linear regression for all points, for SSHA >=0 and for SSHA<=0
        val, v_pos, v_neg = LinearRegressionAndNonlinearity(sst, ssh, return_stderr=True, return_intercept=True)
        # 3.2 Non linearities
        nl1 = v_neg[0] - v_pos[0]
        nl2 = v_neg[1] + v_pos[1]

        # ------------------------------------------------
        # 4. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            ssh_map, ssh_map_areacell, keyerror2 = Read_data_mask_area(
                sshfile, sshname, "sea surface height", metric, "equatorial_pacific", file_area=sshareafile,
                name_area=sshareaname, file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False,
                maskocean=False, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_map, ssh_map, keyerror = CheckTime(sst_map, ssh_map, metric_name=metric, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Remove global mean SSH to local SSH at every time step
            ssh_map, _, keyerror = remove_global_mean(ssh_map, glo, "", "")
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
            sst_map, _, keyerror1 = PreProcessTS(sst_map, "", areacell=sst_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            ssh_map, _, keyerror2 = PreProcessTS(ssh_map, "", areacell=ssh_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            del ssh_map_areacell, sst_map_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(ssh) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
            ssh_map = Regrid(ssh_map, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(ssh) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            sst_map, keyerror1 = AverageMeridional(sst_map)
            ssh_map, keyerror2 = AverageMeridional(ssh_map)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(ssh) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Zonal smoothing
            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
            ssh_map, _ = Smoothing(ssh_map, "", axis=1, window=31, method="square")
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(ssh) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing: netcdf", 15, **dict_debug)
            # Array year by year
            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
            ssh_yby = get_year_by_year(ssh_map, frequency=kwargs["frequency"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_yby.getAxisList()]),
                              "axes2": "(ssh) " + str([ax.id for ax in ssh_yby.getAxisList()]),
                              "shape1": "(sst) " + str(sst_yby.shape), "shape2": "(ssh) " + str(ssh_yby.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year: netcdf", 15, **dict_debug)
            # Computes the linear regression for all points, for SSHA >=0 and for SSHA<=0
            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                sst_map, ssh_map, return_stderr=False, return_intercept=False)
            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                sst_yby, ssh_yby, return_stderr=False, return_intercept=False)
            if debug is True:
                dict_debug = {"axes1": "(zonal Th. Fb) " + str([ax.id for ax in cur_val.getAxisList()]),
                              "axes2": "(hovtx Th. Fb) " + str([ax.id for ax in hov_val.getAxisList()]),
                              "shape1": "(zonal Th. Fb) " + str(cur_val.shape),
                              "shape2": "(hovtx Th. Fb) " + str(hov_val.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionAndNonlinearity: netcdf", 15, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": "cm", "number_of_years_used": nbr_year, "description": sshbox + " dynamic SSHA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict2 = {"units": "C", "number_of_years_used": nbr_year, "description": sstbox + " SSTA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of SSTA onto dynamic dynamic SSHA across longitudes"}
            dict4 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of SSTA onto dynamic SSHA>0 across longitudes"}
            dict5 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of SSTA onto dynamic SSHA<0 across longitudes"}
            dict6 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of SSTA onto dynamic SSHA " +
                                    "across longitudes"}
            dict7 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of SSTA onto dynamic SSHA>0 " +
                                    "across longitudes"}
            dict8 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of SSTA onto dynamic SSHA<0 " +
                                    "across longitudes"}
            dict9 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict9,
                       var1=ssh, var1_attributes=dict1, var1_name=ovar[0] + dataset, var1_time_name="months_" + dataset,
                       var2=sst, var2_attributes=dict2, var2_name=ovar[1] + dataset, var2_time_name="months_" + dataset,
                       var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                       var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                       var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(val[0]), "line2": "diagnostic value_error: " + str(val[1])}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": val[0], "value_error": val[1], "units": units, "method": method, "ref": ref,
        "method_nonlinearity": method_NL, "nyears": nbr_year, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "nonlinearity": nl1, "nonlinearity_error": nl2, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstLhf(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox,
                 lhffile, lhfname, lhfareafile, lhfareaname, lhflandmaskfile, lhflandmaskname, lhfbox,
                 dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstLhf() function computes the regression of 'lhfbox' LHFA (latent heat flux anomalies) onto 'sstbox' SSTA
    (sea surface temperature anomalies) (usually the regression of nino3 LHFA onto nino3 SSTA)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param lhffile: string
        path_to/filename of the file (NetCDF) of LHF
    :param lhfname: string
        name of LHF variable (hfls, lhf, lhtfl, solatent) in 'lhffile'
    :param lhfareafile: string
        path_to/filename of the file (NetCDF) of the areacell for LHF
    :param lhfareaname: string
        name of areacell variable (areacella, areacello) in 'lhfareafile'
    :param lhflandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for LHF
    :param lhflandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfile'
    :param lhfbox: string
        name of box (nino3') for LHF
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstLhf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "LHF-SST feedback (alpha_lh)"
    units = "W/m2/C"
    method = "Regression of " + lhfbox + " latent heat flux anomalies (LHFA) onto " + sstbox + " sea surface " + \
             "temperature anomalies (SSTA)"
    method_NL = "The nonlinearity is the regression computed when " + sstbox + " SSTA<0 minus the regression " + \
                "computed when " + sstbox + " SSTA>0"
    ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstLhf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag, actualtimebounds, nbr_year, keyerror = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        lhf, lhf_areacell, keyerror2 = Read_data_mask_area(
            lhffile, lhfname, "heat flux", metric, lhfbox, file_area=lhfareafile, name_area=lhfareaname,
            file_mask=lhflandmaskfile, name_mask=lhflandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst, lhf, keyerror = CheckTime(sst, lhf, metric_name=metric, debug=debug, **kwargs)
        if keyerror is not None:
            break
        # 1.3 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.4 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror1 = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        # 2.2 LHF averaged in 'lhfbox' are normalized / detrended / smoothed (running average) if applicable
        lhf, _, keyerror2 = PreProcessTS(
            lhf, "", areacell=lhf_areacell, average="horizontal", compute_anom=True, region=lhfbox, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del lhf_areacell, sst_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                          "axes2": "(lhf) " + str([ax.id for ax in lhf.getAxisList()]),
                          "shape1": "(sst) " + str(sst.shape), "shape2": "(lhf) " + str(lhf.shape),
                          "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(lhf) " + str(TimeBounds(lhf))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regression (diagnostic value and supplementary diagnostic values)
        # ------------------------------------------------
        # 3.1 Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
        val, v_pos, v_neg = LinearRegressionAndNonlinearity(lhf, sst, return_stderr=True, return_intercept=True)
        # 3.2 Non linearities
        nl1 = v_neg[0] - v_pos[0]
        nl2 = v_neg[1] + v_pos[1]

        # ------------------------------------------------
        # 4. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            lhf_map, lhf_map_areacell, keyerror2 = Read_data_mask_area(
                lhffile, lhfname, "heat flux", metric, "equatorial_pacific", file_area=lhfareafile,
                name_area=lhfareaname, file_mask=lhflandmaskfile, name_mask=lhflandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_map, lhf_map, keyerror = CheckTime(sst_map, lhf_map, metric_name=metric, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
            sst_map, _, keyerror1 = PreProcessTS(sst_map, "", areacell=sst_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            lhf_map, _, keyerror2 = PreProcessTS(lhf_map, "", areacell=lhf_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            del lhf_map_areacell, sst_map_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(lhf) " + str([ax.id for ax in lhf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(lhf) " + str(lhf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
            lhf_map = Regrid(lhf_map, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(lhf) " + str([ax.id for ax in lhf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(lhf) " + str(lhf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            sst_map, keyerror1 = AverageMeridional(sst_map)
            lhf_map, keyerror2 = AverageMeridional(lhf_map)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(lhf) " + str([ax.id for ax in lhf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(lhf) " + str(lhf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Zonal smoothing
            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
            lhf_map, _ = Smoothing(lhf_map, "", axis=1, window=31, method="square")
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(lhf) " + str([ax.id for ax in lhf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(lhf) " + str(lhf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing: netcdf", 15, **dict_debug)
            # Array year by year
            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
            lhf_yby = get_year_by_year(lhf_map, frequency=kwargs["frequency"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_yby.getAxisList()]),
                              "axes2": "(lhf) " + str([ax.id for ax in lhf_yby.getAxisList()]),
                              "shape1": "(sst) " + str(sst_yby.shape), "shape2": "(lhf) " + str(lhf_yby.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year: netcdf", 15, **dict_debug)
            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                lhf_map, sst_map, return_stderr=False, return_intercept=False)
            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                lhf_yby, sst_yby, return_stderr=False, return_intercept=False)
            if debug is True:
                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionAndNonlinearity: netcdf", 15, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": "C", "number_of_years_used": nbr_year, "description": sstbox + " SSTA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict2 = {"units": "W/m2", "number_of_years_used": nbr_year, "description": lhfbox + " LHFA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of LHFA onto dynamic SSTA across longitudes"}
            dict4 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of LHFA onto dynamic SSTA>0 across longitudes"}
            dict5 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of LHFA onto dynamic SSTA<0 across longitudes"}
            dict6 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of LHFA onto SSTA " +
                                    "across longitudes"}
            dict7 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of LHFA onto SSTA>0 " +
                                    "across longitudes"}
            dict8 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of LHFA onto SSTA<0 " +
                                    "across longitudes"}
            dict9 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict9,
                       var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, var1_time_name="months_" + dataset,
                       var2=lhf, var2_attributes=dict2, var2_name=ovar[1] + dataset, var2_time_name="months_" + dataset,
                       var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                       var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                       var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(val[0]), "line2": "diagnostic value_error: " + str(val[1])}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": val[0], "value_error": val[1], "units": units, "method": method, "ref": ref,
        "method_nonlinearity": method_NL, "nyears": nbr_year, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "nonlinearity": nl1, "nonlinearity_error": nl2, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstLwr(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox,
                 lwrfile, lwrname, lwrareafile, lwrareaname, lwrlandmaskfile, lwrlandmaskname, lwrbox,
                 dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstLwr() function computes the regression of 'lwrbox' LWRA (net surface longwave radiation anomalies) onto
    'sstbox' SSTA (sea surface temperature anomalies) (usually the regression of nino3 LWRA onto nino3 SSTA)

    The net surface longwave radiation is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into lwrfile and lwrname of this
    function a list() of the two needed files and variable names (CMIP: rlds-rlus)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param lwrfile: string
        path_to/filename of the file (NetCDF) of LWR
    :param lwrname: string
        name of LWR variable (lwr, rls, solongwa, dlwrf - ulwrf, rlds - rlus) (may be a list of variables) in 'lwrfile'
    :param lwrareafile: string
        path_to/filename of the file (NetCDF) of the areacell for LWR
    :param lwrareaname: string
        name of areacell variable (areacella, areacello) in 'lwrareafile'
    :param lwrlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for LWR
    :param lwrlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfile'
    :param lwrbox: string
        name of box (nino3') for LWR
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstLwr_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "LWR-SST feedback (alpha_lwr)"
    units = "W/m2/C"
    method = "Regression of " + lwrbox + " net surface longwave radiation anomalies (LWRA) onto " + sstbox + " sea " + \
             "surface temperature anomalies (SSTA)"
    method_NL = "The nonlinearity is the regression computed when " + sstbox + " SSTA<0 minus the regression " + \
                "computed when " + sstbox + " SSTA>0"
    ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstLwr"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag, actualtimebounds, nbr_year, keyerror = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        lwr, lwr_areacell, keyerror2 = Read_data_mask_area_multifile(
            lwrfile, lwrname, "heat flux", "lwr", metric, lwrbox, file_area=lwrareafile, name_area=lwrareaname,
            file_mask=lwrlandmaskfile, name_mask=lwrlandmaskname, maskland=True, maskocean=False, debug=debug,
            interpreter="project_interpreter_var2", **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst, lwr, keyerror = CheckTime(sst, lwr, metric_name=metric, debug=debug, **kwargs)
        if keyerror is not None:
            break
        # 1.3 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.4 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror1 = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        # 2.2 LWR averaged in 'lwrbox' are normalized / detrended / smoothed (running average) if applicable
        lwr, _, keyerror2 = PreProcessTS(
            lwr, "", areacell=lwr_areacell, average="horizontal", compute_anom=True, region=lwrbox, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del lwr_areacell, sst_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                          "axes2": "(lwr) " + str([ax.id for ax in lwr.getAxisList()]),
                          "shape1": "(sst) " + str(sst.shape), "shape2": "(lwr) " + str(lwr.shape),
                          "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(lwr) " + str(TimeBounds(lwr))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regression (diagnostic value and supplementary diagnostic values)
        # ------------------------------------------------
        # 3.1 Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
        val, v_pos, v_neg = LinearRegressionAndNonlinearity(lwr, sst, return_stderr=True, return_intercept=True)
        # 3.2 Non linearities
        nl1 = v_neg[0] - v_pos[0]
        nl2 = v_neg[1] + v_pos[1]

        # ------------------------------------------------
        # 4. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            lwr_map, lwr_map_areacell, keyerror2 = Read_data_mask_area_multifile(
                lwrfile, lwrname, "heat flux", "lwr", metric,  "equatorial_pacific", file_area=lwrareafile,
                name_area=lwrareaname, file_mask=lwrlandmaskfile, name_mask=lwrlandmaskname, maskland=True,
                maskocean=False, debug=debug, interpreter="project_interpreter_var2", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_map, lwr_map, keyerror = CheckTime(sst_map, lwr_map, metric_name=metric, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
            sst_map, _, keyerror1 = PreProcessTS(sst_map, "", areacell=sst_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            lwr_map, _, keyerror2 = PreProcessTS(lwr_map, "", areacell=lwr_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            del lwr_map_areacell, sst_map_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(lwr) " + str([ax.id for ax in lwr_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(lwr) " + str(lwr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
            lwr_map = Regrid(lwr_map, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(lwr) " + str([ax.id for ax in lwr_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(lwr) " + str(lwr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            sst_map, keyerror1 = AverageMeridional(sst_map)
            lwr_map, keyerror2 = AverageMeridional(lwr_map)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(lwr) " + str([ax.id for ax in lwr_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(lwr) " + str(lwr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Zonal smoothing
            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
            lwr_map, _ = Smoothing(lwr_map, "", axis=1, window=31, method="square")
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(lwr) " + str([ax.id for ax in lwr_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(lwr) " + str(lwr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing: netcdf", 15, **dict_debug)
            # Array year by year
            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
            lwr_yby = get_year_by_year(lwr_map, frequency=kwargs["frequency"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_yby.getAxisList()]),
                              "axes2": "(lwr) " + str([ax.id for ax in lwr_yby.getAxisList()]),
                              "shape1": "(sst) " + str(sst_yby.shape), "shape2": "(lwr) " + str(lwr_yby.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year: netcdf", 15, **dict_debug)
            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                lwr_map, sst_map, return_stderr=False, return_intercept=False)
            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                lwr_yby, sst_yby, return_stderr=False, return_intercept=False)
            if debug is True:
                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionAndNonlinearity: netcdf", 15, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": "C", "number_of_years_used": nbr_year, "description": sstbox + " SSTA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict2 = {"units": "W/m2", "number_of_years_used": nbr_year, "description": lwrbox + " LWRA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of LWRA onto dynamic SSTA across longitudes"}
            dict4 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of LWRA onto dynamic SSTA>0 across longitudes"}
            dict5 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of LWRA onto dynamic SSTA<0 across longitudes"}
            dict6 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of LWRA onto SSTA " +
                                    "across longitudes"}
            dict7 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of LWRA onto SSTA>0 " +
                                    "across longitudes"}
            dict8 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of LWRA onto SSTA<0 " +
                                    "across longitudes"}
            dict9 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict9,
                       var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, var1_time_name="months_" + dataset,
                       var2=lwr, var2_attributes=dict2, var2_name=ovar[1] + dataset, var2_time_name="months_" + dataset,
                       var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                       var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                       var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(val[0]), "line2": "diagnostic value_error: " + str(val[1])}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": val[0], "value_error": val[1], "units": units, "method": method, "ref": ref,
        "method_nonlinearity": method_NL, "nyears": nbr_year, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "nonlinearity": nl1, "nonlinearity_error": nl2, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstShf(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox,
                 shffile, shfname, shfareafile, shfareaname, shflandmaskfile, shflandmaskname, shfbox,
                 dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstShf() function computes the regression of 'shfbox' SHFA (sensible heat flux anomalies) onto 'sstbox'
    SSTA (sea surface temperature anomalies) (usually the regression of nino3 SHFA onto nino3 SSTA)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param shffile: string
        path_to/filename of the file (NetCDF) of SHF
    :param shfname: string
        name of SHF variable (hfss, shf, shtfl, sosensib) in 'shffile'
    :param shfareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SHF
    :param shfareaname: string
        name of areacell variable (areacella, areacello) in 'shfareafile'
    :param shflandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SHF
    :param shflandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfile'
    :param shfbox: string
        name of box (nino3') for SHF
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstShf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "SHF-SST feedback (alpha_sh)"
    units = "W/m2/C"
    method = "Regression of " + shfbox + " sensible heat flux anomalies (SHFA) onto " + sstbox + " sea surface " + \
             "temperature anomalies (SSTA)"
    method_NL = "The nonlinearity is the regression computed when " + sstbox + " SSTA<0 minus the regression " + \
                "computed when " + sstbox + " SSTA>0"
    ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstShf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag, actualtimebounds, nbr_year, keyerror = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        shf, shf_areacell, keyerror2 = Read_data_mask_area(
            shffile, shfname, "heat flux", metric, shfbox, file_area=shfareafile, name_area=shfareaname,
            file_mask=shflandmaskfile, name_mask=shflandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst, shf, keyerror = CheckTime(sst, shf, metric_name=metric, debug=debug, **kwargs)
        if keyerror is not None:
            break
        # 1.3 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.4 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror1 = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        # 2.2 SHF averaged in 'shfbox' are normalized / detrended / smoothed (running average) if applicable
        shf, _, keyerror2 = PreProcessTS(
            shf, "", areacell=shf_areacell, average="horizontal", compute_anom=True, region=shfbox, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del shf_areacell, sst_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                          "axes2": "(shf) " + str([ax.id for ax in shf.getAxisList()]),
                          "shape1": "(sst) " + str(sst.shape), "shape2": "(shf) " + str(shf.shape),
                          "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(shf) " + str(TimeBounds(shf))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regression (diagnostic value and supplementary diagnostic values)
        # ------------------------------------------------
        # 3.1 Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
        val, v_pos, v_neg = LinearRegressionAndNonlinearity(shf, sst, return_stderr=True, return_intercept=True)
        # 3.2 Non linearities
        nl1 = v_neg[0] - v_pos[0]
        nl2 = v_neg[1] + v_pos[1]

        # ------------------------------------------------
        # 4. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            shf_map, shf_map_areacell, keyerror2 = Read_data_mask_area(
                shffile, shfname, "heat flux", metric, "equatorial_pacific", file_area=shfareafile,
                name_area=shfareaname, file_mask=shflandmaskfile, name_mask=shflandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_map, shf_map, keyerror = CheckTime(sst_map, shf_map, metric_name=metric, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
            sst_map, _, keyerror1 = PreProcessTS(sst_map, "", areacell=sst_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            shf_map, _, keyerror2 = PreProcessTS(shf_map, "", areacell=shf_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            del shf_map_areacell, sst_map_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(shf) " + str([ax.id for ax in shf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(shf) " + str(shf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
            shf_map = Regrid(shf_map, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(shf) " + str([ax.id for ax in shf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(shf) " + str(shf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            sst_map, keyerror1 = AverageMeridional(sst_map)
            shf_map, keyerror2 = AverageMeridional(shf_map)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(shf) " + str([ax.id for ax in shf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(shf) " + str(shf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Zonal smoothing
            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
            shf_map, _ = Smoothing(shf_map, "", axis=1, window=31, method="square")
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(shf) " + str([ax.id for ax in shf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(shf) " + str(shf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing: netcdf", 15, **dict_debug)
            # Array year by year
            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
            shf_yby = get_year_by_year(shf_map, frequency=kwargs["frequency"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_yby.getAxisList()]),
                              "axes2": "(shf) " + str([ax.id for ax in shf_yby.getAxisList()]),
                              "shape1": "(sst) " + str(sst_yby.shape), "shape2": "(shf) " + str(shf_yby.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year: netcdf", 15, **dict_debug)
            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                shf_map, sst_map, return_stderr=False, return_intercept=False)
            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                shf_yby, sst_yby, return_stderr=False, return_intercept=False)
            if debug is True:
                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionAndNonlinearity: netcdf", 15, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": "C", "number_of_years_used": nbr_year, "description": sstbox + " SSTA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict2 = {"units": "W/m2", "number_of_years_used": nbr_year, "description": shfbox + " SHFA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of SHFA onto dynamic SSTA across longitudes"}
            dict4 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of SHFA onto dynamic SSTA>0 across longitudes"}
            dict5 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of SHFA onto dynamic SSTA<0 across longitudes"}
            dict6 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of SHFA onto SSTA " +
                                    "across longitudes"}
            dict7 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of SHFA onto SSTA>0 " +
                                    "across longitudes"}
            dict8 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of SHFA onto SSTA<0 " +
                                    "across longitudes"}
            dict9 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict9,
                       var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, var1_time_name="months_" + dataset,
                       var2=shf, var2_attributes=dict2, var2_name=ovar[1] + dataset, var2_time_name="months_" + dataset,
                       var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                       var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                       var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(val[0]), "line2": "diagnostic value_error: " + str(val[1])}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": val[0], "value_error": val[1], "units": units, "method": method, "ref": ref,
        "method_nonlinearity": method_NL, "nyears": nbr_year, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "nonlinearity": nl1, "nonlinearity_error": nl2, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstSwr(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox,
                 swrfile, swrname, swrareafile, swrareaname, swrlandmaskfile, swrlandmaskname, swrbox,
                 dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstSwr() function computes the regression of 'swrbox' SWRA (net surface shortwave radiation anomalies)
    onto 'sstbox' SSTA (sea surface temperature anomalies) (usually the regression of nino3 SWRA onto nino3 SSTA)

    The net surface shortwave radiation is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into swrfile and swrname of this
    function a list() of the two needed files and variable names (CMIP: rsds-rsus)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param swrfile: string
        path_to/filename of the file (NetCDF) of SWR
    :param swrname: string
        name of SWR variable (rss, soshfldo, swr, dswrf - uswrf, rsds - rsus) (may be a list of variables) in 'swrfile'
    :param swrareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SWR
    :param swrareaname: string
        name of areacell variable (areacella, areacello) in 'swrareafile'
    :param swrlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SWR
    :param swrlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfile'
    :param swrbox: string
        name of box (nino3') for SWR
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstSwr_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "SWR-SST feedback (alpha_swr)"
    units = "W/m2/C"
    method = "Regression of " + swrbox + " net surface shortwave radiation anomalies (SWRA) onto " + sstbox + \
             " sea surface temperature anomalies (SSTA)"
    method_NL = "The nonlinearity is the regression computed when " + sstbox + " SSTA<0 minus the regression " + \
                "computed when " + sstbox + " SSTA>0"
    ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstSwr"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag, actualtimebounds, nbr_year, keyerror = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        swr, swr_areacell, keyerror2 = Read_data_mask_area_multifile(
            swrfile, swrname, "heat flux", "swr", metric, swrbox, file_area=swrareafile, name_area=swrareaname,
            file_mask=swrlandmaskfile, name_mask=swrlandmaskname, maskland=True, maskocean=False, debug=debug,
            interpreter="project_interpreter_var2", **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst, swr, keyerror = CheckTime(sst, swr, metric_name=metric, debug=debug, **kwargs)
        if keyerror is not None:
            break
        # 1.3 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.4 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror1 = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        # 2.2 SWR averaged in 'swrbox' are normalized / detrended / smoothed (running average) if applicable
        swr, _, keyerror2 = PreProcessTS(
            swr, "", areacell=swr_areacell, average="horizontal", compute_anom=True, region=swrbox, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del swr_areacell, sst_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                          "axes2": "(swr) " + str([ax.id for ax in swr.getAxisList()]),
                          "shape1": "(sst) " + str(sst.shape), "shape2": "(swr) " + str(swr.shape),
                          "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(swr) " + str(TimeBounds(swr))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regression (diagnostic value and supplementary diagnostic values)
        # ------------------------------------------------
        # 3.1 Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
        val, v_pos, v_neg = LinearRegressionAndNonlinearity(swr, sst, return_stderr=True, return_intercept=True)
        # 3.2 Non linearities
        nl1 = v_neg[0] - v_pos[0]
        nl2 = v_neg[1] + v_pos[1]

        # ------------------------------------------------
        # 4. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            swr_map, swr_map_areacell, keyerror2 = Read_data_mask_area_multifile(
                swrfile, swrname, "heat flux", "swr", metric,  "equatorial_pacific", file_area=swrareafile,
                name_area=swrareaname, file_mask=swrlandmaskfile, name_mask=swrlandmaskname, maskland=True,
                maskocean=False, debug=debug, interpreter="project_interpreter_var2", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_map, swr_map, keyerror = CheckTime(sst_map, swr_map, metric_name=metric, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
            sst_map, _, keyerror1 = PreProcessTS(sst_map, "", areacell=sst_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            swr_map, _, keyerror2 = PreProcessTS(swr_map, "", areacell=swr_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            del swr_map_areacell, sst_map_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(swr) " + str([ax.id for ax in swr_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(swr) " + str(swr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
            swr_map = Regrid(swr_map, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(swr) " + str([ax.id for ax in swr_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(swr) " + str(swr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            sst_map, keyerror1 = AverageMeridional(sst_map)
            swr_map, keyerror2 = AverageMeridional(swr_map)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(swr) " + str([ax.id for ax in swr_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(swr) " + str(swr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Zonal smoothing
            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
            swr_map, _ = Smoothing(swr_map, "", axis=1, window=31, method="square")
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(swr) " + str([ax.id for ax in swr_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(swr) " + str(swr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing: netcdf", 15, **dict_debug)
            # Array year by year
            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
            swr_yby = get_year_by_year(swr_map, frequency=kwargs["frequency"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_yby.getAxisList()]),
                              "axes2": "(swr) " + str([ax.id for ax in swr_yby.getAxisList()]),
                              "shape1": "(sst) " + str(sst_yby.shape), "shape2": "(swr) " + str(swr_yby.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year: netcdf", 15, **dict_debug)
            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                swr_map, sst_map, return_stderr=False, return_intercept=False)
            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                swr_yby, sst_yby, return_stderr=False, return_intercept=False)
            if debug is True:
                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionAndNonlinearity: netcdf", 15, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": "C", "number_of_years_used": nbr_year, "description": sstbox + " SSTA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict2 = {"units": "W/m2", "number_of_years_used": nbr_year, "description": swrbox + " SWRA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of SWRA onto dynamic SSTA across longitudes"}
            dict4 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of SWRA onto dynamic SSTA>0 across longitudes"}
            dict5 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of SWRA onto dynamic SSTA<0 across longitudes"}
            dict6 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of SWRA onto SSTA " +
                                    "across longitudes"}
            dict7 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of SWRA onto SSTA>0 " +
                                    "across longitudes"}
            dict8 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of SWRA onto SSTA<0 " +
                                    "across longitudes"}
            dict9 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict9,
                       var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, var1_time_name="months_" + dataset,
                       var2=swr, var2_attributes=dict2, var2_name=ovar[1] + dataset, var2_time_name="months_" + dataset,
                       var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                       var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                       var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(val[0]), "line2": "diagnostic value_error: " + str(val[1])}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": val[0], "value_error": val[1], "units": units, "method": method, "ref": ref,
        "method_nonlinearity": method_NL, "nyears": nbr_year, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "nonlinearity": nl1, "nonlinearity_error": nl2, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstTaux(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox,
                  taufile, tauname, tauareafile, tauareaname, taulandmaskfile, taulandmaskname, taubox,
                  dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstTaux() function computes the regression of 'taubox' TauxA (surface downward zonal stress anomalies)
    onto 'sstbox' SSTA (sea surface temperature anomalies) (usually the regression of nino4 TauxA onto nino3 SSTA)

    Author:	Eric Guilyardi : Eric.Guilyardi@locean-ipsl.upmc.fr
    Co-author: Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon Jan  9 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box ('nino3') for SST
    :param taufile: string
        path_to/filename of the file (NetCDF) of Taux
    :param tauname: string
        name of Taux variable (tauu, tauuo, taux) in 'taufile'
    :param tauareafile: string
        path_to/filename of the file (NetCDF) of the areacell for Taux
    :param tauareaname: string
        name of areacell variable (areacella, areacello) in 'tauareafile'
    :param taulandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for Taux
    :param taulandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfile'
    :param taubox: string
        name of box ('nino4') for Taux
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstTaux_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "Taux-SST feedback (mu)"
    units = "1e-3 N/m2/C"
    method = "Regression of " + taubox + " zonal wind stress anomalies (TauxA) onto " + sstbox + " sea surface " + \
             "temperature anomalies (SSTA)"
    method_NL = "The nonlinearity is the regression computed when " + sstbox + " SSTA<0 minus the regression " + \
                "computed when " + sstbox + " SSTA>0"
    ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstTaux"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag, actualtimebounds, keyerror, nbr_year = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        tau, tau_areacell, keyerror2 = Read_data_mask_area(
            taufile, tauname, "wind stress", metric, taubox, file_area=tauareafile, name_area=tauareaname,
            file_mask=taulandmaskfile, name_mask=taulandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst, tau, keyerror = CheckTime(sst, tau, metric_name=metric, debug=debug, **kwargs)
        if keyerror is not None:
            break
        # 1.3 Compute number of years for model and observation
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.4 Read time period used for model and observation
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror1 = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        # 2.2 Taux averaged in 'taubox' are normalized / detrended / smoothed (running average) if applicable
        tau, _, keyerror2 = PreProcessTS(
            tau, "", areacell=tau_areacell, average="horizontal", compute_anom=True, region=taubox, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del sst_areacell, tau_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                          "axes2": "(tau) " + str([ax.id for ax in tau.getAxisList()]),
                          "shape1": "(sst) " + str(sst.shape), "shape2": "(tau) " + str(tau.shape),
                          "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(tau) " + str(TimeBounds(tau))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regression (diagnostic value and supplementary diagnostic values)
        # ------------------------------------------------
        # 3.1 Computes the linear regression for all points, for SSTA>=0 and for SSTA<=0
        val, v_pos, v_neg = LinearRegressionAndNonlinearity(tau, sst, return_stderr=True, return_intercept=True)
        # 3.2 Non linearities
        nl1 = v_neg[0] - v_pos[0]
        nl2 = v_neg[1] + v_pos[1]

        # ------------------------------------------------
        # 4. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            tau_map, tau_map_areacell, keyerror = Read_data_mask_area(
                taufile, tauname, "wind stress", metric, "equatorial_pacific", file_area=tauareafile,
                name_area=tauareaname, file_mask=taulandmaskfile, name_mask=taulandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst, tau_map, keyerror = CheckTime(sst, tau_map, metric_name=metric, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
            tau_map, _, keyerror = PreProcessTS(
                tau_map, "", areacell=tau_map_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            del tau_map_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(tau) " + str([ax.id for ax in tau_map.getAxisList()]),
                              "shape1": "(tau) " + str(tau_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            tau_map = Regrid(tau_map, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(tau) " + str([ax.id for ax in tau_map.getAxisList()]),
                              "shape1": "(tau) " + str(tau_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            tau_map, keyerror = AverageMeridional(tau_map)
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(tau) " + str([ax.id for ax in tau_map.getAxisList()]),
                              "shape2": "(tau) " + str(tau_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Zonal smoothing
            tau_map, _ = Smoothing(tau_map, "", axis=1, window=31, method="square")
            if debug is True:
                dict_debug = {"axes1": "(tau) " + str([ax.id for ax in tau_map.getAxisList()]),
                              "shape1": "(tau) " + str(tau_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing: netcdf", 15, **dict_debug)
            # Sst to map
            sst_map = TsToMap(sst, tau_map)
            # Array year by year
            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
            tau_yby = get_year_by_year(tau_map, frequency=kwargs["frequency"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(tau) " + str([ax.id for ax in tau_yby.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(tau) " + str(tau_yby.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year: netcdf", 15, **dict_debug)
            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                tau_map, sst_map, return_stderr=False, return_intercept=False)
            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                tau_yby, sst_yby, return_stderr=False, return_intercept=False)
            if debug is True:
                dict_debug = {"axes1": "(zonal mu) " + str([ax.id for ax in cur_val.getAxisList()]),
                              "axes2": "(hovtx mu) " + str([ax.id for ax in hov_val.getAxisList()]),
                              "shape1": "(zonal mu) " + str(cur_val.shape),
                              "shape2": "(hovtx mu) " + str(hov_val.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionAndNonlinearity: netcdf", 15, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": "C", "number_of_years_used": nbr_year, "description": sstbox + " SSTA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict2 = {"units": "1e-3 N/m2", "number_of_years_used": nbr_year, "description": taubox + " TauxA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds), }
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of TauxA across longitudes onto " + sstbox + " SSTA"}
            dict4 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of TauxA across longitudes onto " + sstbox + " SSTA>0"}
            dict5 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of TauxA across longitudes onto " + sstbox + " SSTA<0"}
            dict6 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of TauxA across longitudes " +
                                    "onto " + sstbox + " SSTA"}
            dict7 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of TauxA across longitudes " +
                                    "onto " + sstbox + " SSTA>0"}
            dict8 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of TauxA across longitudes " +
                                    "onto " + sstbox + " SSTA<0"}
            dict9 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict9,
                       var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, var1_time_name="months_" + dataset,
                       var2=tau, var2_attributes=dict2, var2_name=ovar[1] + dataset, var2_time_name="months_" + dataset,
                       var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                       var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                       var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(val[0]), "line2": "diagnostic value_error: " + str(val[1])}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": val[0], "value_error": val[1], "units": units, "method": method, "ref": ref,
        "method_nonlinearity": method_NL, "nyears": nbr_year, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "nonlinearity": nl1, "nonlinearity_error": nl2, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstThf(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox,
                 thffile, thfname, thfareafile, thfareaname, thflandmaskfile, thflandmaskname, thfbox,
                 dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstThf() function computes the regression of 'thfbox' THFA (net heat flux anomalies) onto
    'sstbox' SSTA (sea surface temperature anomalies) (usually the regression of nino3 THFA onto nino3 SSTA)
    The net heat flux is the sum of four term:
         - net surface shortwave radiation,
         - net surface longwave radiation,
         - latent heat flux,
         - sensible heat flux

    The net heat flux is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into thffile and thfname of this
    function a list() of the four needed files and variable names (CMIP: rsds-rsus, rlds-rlus, hfls, hfss)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param thffile: string
        path_to/filename of the file (NetCDF) of THF
    :param thfname: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffile'
    :param thfareafile: string
        path_to/filename of the file (NetCDF) of the areacell for THF
    :param thfareaname: string
        name of areacell variable (areacella, areacello) in 'thfareafile'
    :param thflandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for THF
    :param thflandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfile'
    :param thfbox: string
        name of box (nino3') for THF
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstThf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "THF-SST feedback (alpha)"
    units = "W/m2/C"
    method = "Regression of " + thfbox + " net heat flux anomalies (THFA) onto " + sstbox + " sea " + \
             "surface temperature anomalies (SSTA)"
    method_NL = "The nonlinearity is the regression computed when " + sstbox + " SSTA<0 minus the regression " + \
                "computed when " + sstbox + " SSTA>0"
    ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstThf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag, actualtimebounds, nbr_year, keyerror = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst, sst_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        thf, thf_areacell, keyerror2 = Read_data_mask_area_multifile(
            thffile, thfname, "heat flux", "thf", metric, thfbox, file_area=thfareafile, name_area=thfareaname,
            file_mask=thflandmaskfile, name_mask=thflandmaskname, maskland=True, maskocean=False, debug=debug,
            interpreter="project_interpreter_var2", **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        sst, thf, keyerror = CheckTime(sst, thf, metric_name=metric, debug=debug, **kwargs)
        if keyerror is not None:
            break
        # 1.3 Compute number of years
        nbr_year = int(round(sst.shape[0] / 12.))
        # 1.4 Read time period used
        actualtimebounds = TimeBounds(sst)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        sst, method, keyerror1 = PreProcessTS(
            sst, method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        # 2.2 THF averaged in 'thfbox' are normalized / detrended / smoothed (running average) if applicable
        thf, _, keyerror2 = PreProcessTS(
            thf, "", areacell=thf_areacell, average="horizontal", compute_anom=True, region=thfbox, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del thf_areacell, sst_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                          "axes2": "(thf) " + str([ax.id for ax in thf.getAxisList()]),
                          "shape1": "(sst) " + str(sst.shape), "shape2": "(thf) " + str(thf.shape),
                          "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(thf) " + str(TimeBounds(thf))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regression (diagnostic value and supplementary diagnostic values)
        # ------------------------------------------------
        # 3.1 Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
        val, v_pos, v_neg = LinearRegressionAndNonlinearity(thf, sst, return_stderr=True, return_intercept=True)
        # 3.2 Non linearities
        nl1 = v_neg[0] - v_pos[0]
        nl2 = v_neg[1] + v_pos[1]

        # ------------------------------------------------
        # 4. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            thf_map, thf_map_areacell, keyerror2 = Read_data_mask_area_multifile(
                thffile, thfname, "heat flux", "thf", metric,  "equatorial_pacific", file_area=thfareafile,
                name_area=thfareaname, file_mask=thflandmaskfile, name_mask=thflandmaskname, maskland=True,
                maskocean=False, debug=debug, interpreter="project_interpreter_var2", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            sst_map, thf_map, keyerror = CheckTime(sst_map, thf_map, metric_name=metric, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
            sst_map, _, keyerror1 = PreProcessTS(sst_map, "", areacell=sst_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            thf_map, _, keyerror2 = PreProcessTS(thf_map, "", areacell=thf_map_areacell, average=False,
                                                 compute_anom=True, region="equatorial_pacific", **kwargs)
            keyerror = add_up_errors([keyerror1, keyerror2])
            del thf_map_areacell, sst_map_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(thf) " + str([ax.id for ax in thf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(thf) " + str(thf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
            thf_map = Regrid(thf_map, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(thf) " + str([ax.id for ax in thf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(thf) " + str(thf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            sst_map, keyerror1 = AverageMeridional(sst_map)
            thf_map, keyerror2 = AverageMeridional(thf_map)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(thf) " + str([ax.id for ax in thf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(thf) " + str(thf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Zonal smoothing
            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
            thf_map, _ = Smoothing(thf_map, "", axis=1, window=31, method="square")
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "axes2": "(thf) " + str([ax.id for ax in thf_map.getAxisList()]),
                              "shape1": "(sst) " + str(sst_map.shape), "shape2": "(thf) " + str(thf_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing: netcdf", 15, **dict_debug)
            # Array year by year
            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
            thf_yby = get_year_by_year(thf_map, frequency=kwargs["frequency"])
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_yby.getAxisList()]),
                              "axes2": "(thf) " + str([ax.id for ax in thf_yby.getAxisList()]),
                              "shape1": "(sst) " + str(sst_yby.shape), "shape2": "(thf) " + str(thf_yby.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year: netcdf", 15, **dict_debug)
            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                thf_map, sst_map, return_stderr=False, return_intercept=False)
            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                thf_yby, sst_yby, return_stderr=False, return_intercept=False)
            if debug is True:
                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionAndNonlinearity: netcdf", 15, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": "C", "number_of_years_used": nbr_year, "description": sstbox + " SSTA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict2 = {"units": "W/m2", "number_of_years_used": nbr_year, "description": thfbox + " THFA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of THFA onto dynamic SSTA across longitudes"}
            dict4 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of THFA onto dynamic SSTA>0 across longitudes"}
            dict5 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of THFA onto dynamic SSTA<0 across longitudes"}
            dict6 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of THFA onto SSTA " +
                                    "across longitudes"}
            dict7 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of THFA onto SSTA>0 " +
                                    "across longitudes"}
            dict8 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of THFA onto SSTA<0 " +
                                    "across longitudes"}
            dict9 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict9,
                       var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, var1_time_name="months_" + dataset,
                       var2=thf, var2_attributes=dict2, var2_name=ovar[1] + dataset, var2_time_name="months_" + dataset,
                       var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                       var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                       var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(val[0]), "line2": "diagnostic value_error: " + str(val[1])}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": val[0], "value_error": val[1], "units": units, "method": method, "ref": ref,
        "method_nonlinearity": method_NL, "nyears": nbr_year, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "nonlinearity": nl1, "nonlinearity_error": nl2, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbTauxSsh(taufile, tauname, tauareafile, tauareaname, taulandmaskfile, taulandmaskname, taubox,
                  sshfile, sshname, sshareafile, sshareaname, sshlandmaskfile, sshlandmaskname, sshbox,
                  dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbTauxSsh() function computes the regression of 'sshbox' dynamic SSHA (sea surface height anomalies) onto
    'taubox' TauxA (surface downward zonal stress anomalies) (usually the regression of nino3 SSHA onto nino4 TauxA)

    Author:	Eric Guilyardi : Eric.Guilyardi@locean-ipsl.upmc.fr
    Co-author: Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon Jan  9 2017

    Inputs:
    ------
    :param taufile: string
        path_to/filename of the file (NetCDF) of Taux
    :param tauname: string
        name of Taux variable (tauu, tauuo, taux) in 'taufile'
    :param tauareafile: string
        path_to/filename of the file (NetCDF) of the areacell for Taux
    :param tauareaname: string
        name of areacell variable (areacella, areacello) in 'tauareafile'
    :param taulandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for Taux
    :param taulandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfile'
    :param taubox: string
        name of box ('nino4') for Taux
    :param sshfile: string
        path_to/filename of the file (NetCDF) of SSH
    :param sshname: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfile'
    :param sshareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SSH
    :param sshareaname: string
        name of areacell variable (areacella, areacello) in 'sshareafile'
    :param sshlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SSH
    :param sshlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfile'
    :param sshbox: string
        name of box ('nino3') for SSH
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbTauxSsh_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # Test given kwarg
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "SSH-Taux feedback"
    units = "1e3 cm/N/m2"
    method = "Regression of " + sshbox + " dynamic sea surface height anomalies (SSHA) onto " + taubox + " zonal " + \
             "wind stress anomalies (TauxA)"
    method_NL = "The nonlinearity is the regression computed when " + taubox + " TauxA<0 minus the regression " + \
                "computed when " + taubox + " TauxA>0"
    ref = "Using CDAT regression calculation"
    metric = "EnsoFbTauxSsh"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag, actualtimebounds, keyerror, nbr_year = {"value": None, "axis": None}, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        ssh, ssh_areacell, keyerror1 = Read_data_mask_area(
            sshfile, sshname, "sea surface height", metric, sshbox, file_area=sshareafile, name_area=sshareaname,
            file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False, maskocean=False, debug=debug,
            **kwargs)
        glo, glo_areacell, keyerror2 = Read_data_mask_area(
            sshfile, sshname, "sea surface height", metric, "global2", file_area=sshareafile, name_area=sshareaname,
            file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False, maskocean=False, debug=debug,
            **kwargs)
        tau, tau_areacell, keyerror3 = Read_data_mask_area(
            taufile, tauname, "wind stress", metric, taubox, file_area=tauareafile, name_area=tauareaname,
            file_mask=taulandmaskfile, name_mask=taulandmaskname, maskland=True, maskocean=False, debug=debug,
            **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
        if keyerror is not None:
            break
        # 1.2 Checks if both variables have the same time period and if the minimum number of time steps is respected
        ssh, tau, keyerror1 = CheckTime(ssh, tau, metric_name=metric, debug=debug, **kwargs)
        ssh, glo, keyerror2 = CheckTime(ssh, glo, metric_name=metric, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.3 Compute number of years
        nbr_year = int(round(ssh.shape[0] / 12.))
        # 1.4 Read time period used
        actualtimebounds = TimeBounds(ssh)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        glo, _, glo_keyerror = PreProcessTS(
            glo, "", areacell=glo_areacell, average="horizontal", region="global2", **kwargs)
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        ssh, method, keyerror = remove_global_mean(ssh, glo, "SSH", method)
        if keyerror is not None:
            break
        # 2.3 SSH averaged in 'sshbox' are normalized / detrended / smoothed (running average) if applicable
        ssh, method, keyerror1 = PreProcessTS(
            ssh, method, areacell=ssh_areacell, average="horizontal", compute_anom=True, region=sshbox, **kwargs)
        # 2.4 Taux averaged in 'taubox' are normalized / detrended / smoothed (running average) if applicable
        tau, _, keyerror2 = PreProcessTS(
            tau, "", areacell=tau_areacell, average="horizontal", compute_anom=True, region=taubox, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del glo_areacell, my_det, ssh_areacell, tau_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh.getAxisList()]),
                          "axes2": "(tau) " + str([ax.id for ax in tau.getAxisList()]),
                          "shape1": "(ssh) " + str(ssh.shape), "shape2": "(tau) " + str(tau.shape),
                          "time1": "(ssh) " + str(TimeBounds(ssh)), "time2": "(tau) " + str(TimeBounds(tau))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Regression (diagnostic value and supplementary diagnostic values)
        # ------------------------------------------------
        # 3.1 Computes the linear regression for all points, for TauxA>=0 and for TauxA<=0
        val, v_pos, v_neg = LinearRegressionAndNonlinearity(ssh, tau, return_stderr=True, return_intercept=True)
        # 3.2 Non linearities
        nl1 = v_neg[0] - v_pos[0]
        nl2 = v_neg[1] + v_pos[1]

        # ------------------------------------------------
        # 4. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            ssh_map, ssh_map_areacell, keyerror = Read_data_mask_area(
                sshfile, sshname, "sea surface height", metric, "equatorial_pacific", file_area=sshareafile,
                name_area=sshareaname, file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False,
                maskocean=False, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Checks if the same time period is used for both variables
            ssh_map, tau, keyerror = CheckTime(ssh_map, tau, metric_name=metric, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Remove global mean SSH to local SSH at every time step
            ssh_map, _, keyerror = remove_global_mean(ssh_map, glo, "", "")
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
            ssh_map, _, keyerror = PreProcessTS(
                ssh_map, "", areacell=ssh_map_areacell, compute_anom=True, region="equatorial_pacific", **kwargs)
            del ssh_map_areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(ssh) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            ssh_map = Regrid(ssh_map, None, region="equatorial_pacific", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(ssh) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            ssh_map, keyerror = AverageMeridional(ssh_map)
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape2": "(ssh) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Zonal smoothing
            ssh_map, _ = Smoothing(ssh_map, "", axis=1, window=31, method="square")
            if debug is True:
                dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(ssh) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing: netcdf", 15, **dict_debug)
            # Taux to map
            tau_map = TsToMap(tau, ssh_map)
            # Array year by year
            ssh_yby = get_year_by_year(ssh_map, frequency=kwargs["frequency"])
            tau_yby = get_year_by_year(tau_map, frequency=kwargs["frequency"])
            if debug is True:
                dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh_yby.getAxisList()]),
                              "axes2": "(tau) " + str([ax.id for ax in tau_yby.getAxisList()]),
                              "shape1": "(ssh) " + str(ssh_yby.shape), "shape2": "(tau) " + str(tau_yby.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year: netcdf", 15, **dict_debug)
            # Computes the linear regression for all points, for TauxA >=0 and for TauxA<=0
            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                ssh_map, tau_map, return_stderr=False, return_intercept=False)
            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                ssh_yby, tau_yby, return_stderr=False, return_intercept=False)
            if debug is True:
                dict_debug = {"axes1": "(zonal SSH-Taux fb.) " + str([ax.id for ax in cur_val.getAxisList()]),
                              "axes2": "(hovtx SSH-Taux fb.) " + str([ax.id for ax in hov_val.getAxisList()]),
                              "shape1": "(zonal SSH-Taux fb.) " + str(cur_val.shape),
                              "shape2": "(hovtx SSH-Taux fb.) " + str(hov_val.shape)}
                EnsoErrorsWarnings.debug_mode(
                    "\033[92m", "after LinearRegressionAndNonlinearity: netcdf", 15, **dict_debug)
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": "1e-3 N/m2", "number_of_years_used": nbr_year, "description": taubox + " TauxA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict2 = {"units": "cm", "number_of_years_used": nbr_year, "description": sshbox + " dynamic SSHA",
                     "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                     "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0], "intercept_pos": v_pos[2],
                     "time_period": str(actualtimebounds)}
            dict3 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of dynamic SSHA across longitudes onto " + taubox + " TauxA"}
            dict4 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of dynamic SSHA across longitudes onto " + taubox + " TauxA>0"}
            dict5 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "regression of dynamic SSHA across longitudes onto " + taubox + " TauxA<0"}
            dict6 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of dynamic SSHA across " +
                                    "longitudes onto " + taubox + " TauxA"}
            dict7 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of dynamic SSHA across " +
                                    "longitudes onto " + taubox + " TauxA>0"}
            dict8 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "description": "Hovmoeller diagram (time-longitude) of regression of dynamic SSHA across " +
                                    "longitudes onto " + taubox + " TauxA<0"}
            dict9 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict9,
                       var1=tau, var1_attributes=dict1, var1_name=ovar[0] + dataset, var1_time_name="months_" + dataset,
                       var2=ssh, var2_attributes=dict2, var2_name=ovar[1] + dataset, var2_time_name="months_" + dataset,
                       var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                       var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                       var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                       var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                       var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                       var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(val[0]), "line2": "diagnostic value_error: " + str(val[1])}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": val[0], "value_error": val[1], "units": units, "method": method, "ref": ref,
        "method_nonlinearity": method_NL, "nyears": nbr_year, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "nonlinearity": nl1, "nonlinearity_error": nl2,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def grad_lat_pr(prfile, prname, prareafile, prareaname, prlandmaskfile, prlandmaskname, prbox, dataset="", debug=False,
                netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The grad_lat_pr() function computes the difference between the PR (precipitation) in the eastern off-equatorial and
    equatorial Pacific.
    The definition of Cai et al. (2014; https://doi.org/10.1038/nclimate2100) is used.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon March  9 2021

    Inputs:
    ------
    :param prfile: string
        path_to/filename of the file (NetCDF) of PR
    :param prname: string
        name of PR variable (pr, prec, precip, rain) in 'prfile'
    :param prareafile: string
        path_to/filename of the file (NetCDF) of the areacell for PR
    :param prareaname: string
        name of areacell variable (areacella, areacello) in 'prareafile'
    :param prlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for PR
    :param prlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfile'
    :param prbox: list
        list of region names (['grad_off', 'grad_equ']) for PR
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'grad_lat_pr_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "meridional PR gradient in the eastern Pacific"
    lat = ReferenceRegions(prbox[0])['latitude']
    lat0 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat1 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(prbox[0])['longitude']
    lon0 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon1 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    lat = ReferenceRegions(prbox[0])['latitude']
    lat2 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat3 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(prbox[0])['longitude']
    lon2 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon3 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    units = "mm/day"
    method = str(prbox[0]) + " averaged (" + str(lat0[0]) + "-" + str(lat1[1]) + " ; " + str(lon0[0]) + \
             "-" + str(lon1[1]) + ") minus " + str(prbox[1]) + " averaged (" + str(lat2[0]) + "-" + str(lat3[1]) + \
             " ; " + str(lon2[0]) + "-" + str(lon3[1]) + ") precipitation (PR)"
    ref = "Using CDAT averager and std"
    metric = "grad_lat_pr"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    actualtimebounds, nbr_year, keyerror = None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        off, off_areacell, keyerror1 = Read_data_mask_area(
            prfile, prname, "precipitation", metric, prbox[0], file_area=prareafile, name_area=prareaname,
            file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        equ, equ_areacell, keyerror2 = Read_data_mask_area(
            prfile, prname, "precipitation", metric, prbox[1], file_area=prareafile, name_area=prareaname,
            file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(equ.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(equ)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 PR averaged in 'prbox' are normalized / detrended / smoothed (running average) if applicable
        off, method, keyerror1 = PreProcessTS(
            off, method, areacell=off_areacell, average="horizontal", region=prbox[0], **kwargs)
        equ, _, keyerror2 = PreProcessTS(
            equ, "", areacell=equ_areacell, average="horizontal", region=prbox[1], **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del equ_areacell, off_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(equ) " + str([ax.id for ax in equ.getAxisList()]),
                          "axes2": "(off) " + str([ax.id for ax in off.getAxisList()]),
                          "shape1": "(equ) " + str(equ.shape), "shape2": "(off) " + str(off.shape),
                          "time1": "(equ) " + str(TimeBounds(equ)), "time2": "(off) " + str(TimeBounds(off))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Compute monthly gradient
        # ------------------------------------------------
        # 3.1 Compute monthly gradient
        grad = off - equ

        # ------------------------------------------------
        # 4. Mean gradient (diagnostic value)
        # ------------------------------------------------
        # 4.1 Computes mean gradient
        mv, keyerror = AverageTemporal(grad)
        if keyerror is not None:
            break
        mv = float(mv)
        # 4.2 Standard Error of the mean (function of nyears)
        mv_error = float(Std(grad)) / NUMPYsqrt(len(grad))

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            pr_map, areacell, keyerror = Read_data_mask_area(
                prfile, prname, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafile,
                name_area=prareaname, file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            pr_map, _, keyerror = PreProcessTS(
                pr_map, "", areacell=areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in pr_map.getAxisList()]),
                              "shape1": "(map) " + str(pr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            pr_map = Regrid(pr_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in pr_map.getAxisList()]),
                              "shape1": "(map) " + str(pr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Zonal average
            pr_lat = pr_map(longitude=ReferenceRegions(prbox[0])["longitude"])
            pr_lat, keyerror = AverageZonal(pr_lat)
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(lat) " + str([ax.id for ax in pr_lat.getAxisList()]),
                              "shape1": "(lat) " + str(pr_lat.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal: netcdf", 15, **dict_debug)
            # Supplementary metrics
            std_lat = float(Std(pr_lat, weights=None, axis=0, centered=1, biased=1))
            std_map = float(Std(pr_map, weights=None, axis="xy", centered=1, biased=1))
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_lat,
                     "description": "mean PR across latitudes (zonal averaged [" + str(lon0[0]) + "-" + str(lon1[1]) +
                                    "])"}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_map,
                     "description": "mean PR map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=pr_lat, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                       var2=pr_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": nbr_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def grad_lat_ssh(sshfile, sshname, sshareafile, sshareaname, sshlandmaskfile, sshlandmaskname, sshbox, dataset="",
                 debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The

    () function computes the difference between the dynamic SSH (sea shurface height) in the eastern
    off-equatorial and equatorial Pacific.
    The definition of Cai et al. (2014; https://doi.org/10.1038/nclimate2100) is used.
    The mean spatio-temporal value in the region is subtracted because the geoid is not defined the same way in models
    and observations. The additional mass linked to the melting of the polar caps and the thermal expansion of the ocean
    is not usually taken into account in climate models.
    This metric cannot measure the mean bias (SSH too high or too low) but can illustrate is gradients are reproduced.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Wed April 28 2021

    Inputs:
    ------
    :param sshfile: string
        path_to/filename of the file (NetCDF) of SSH
    :param sshname: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfile'
    :param sshareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SSH
    :param sshareaname: string
        name of areacell variable (areacella, areacello) in 'sshareafile'
    :param sshlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SSH
    :param sshlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfile'
    :param sshbox: list
        list of region names (['grad_off', 'grad_equ']) for SSH
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'grad_lat_ssh_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "meridional dynamic SSH gradient in the eastern Pacific"
    lat = ReferenceRegions(sshbox[0])['latitude']
    lat0 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat1 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sshbox[0])['longitude']
    lon0 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon1 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    lat = ReferenceRegions(sshbox[0])['latitude']
    lat2 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat3 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sshbox[0])['longitude']
    lon2 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon3 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    units = "C"
    method = str(sshbox[0]) + " averaged (" + str(lat0[0]) + "-" + str(lat1[1]) + " ; " + str(lon0[0]) + \
             "-" + str(lon1[1]) + ") minus " + str(sshbox[1]) + " averaged (" + str(lat2[0]) + "-" + str(lat3[1]) + \
             " ; " + str(lon2[0]) + "-" + str(lon3[1]) + ") dynamic sea surface height (SSH)"
    ref = "Using CDAT averager and std"
    metric = "grad_lat_ssh"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    actualtimebounds, nbr_year, keyerror = None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        off, off_areacell, keyerror1 = Read_data_mask_area(
            sshfile, sshname, "sea surface height", metric, sshbox[0], file_area=sshareafile, name_area=sshareaname,
            file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False, maskocean=False, debug=debug,
            **kwargs)
        equ, equ_areacell, keyerror2 = Read_data_mask_area(
            sshfile, sshname, "sea surface height", metric, sshbox[1], file_area=sshareafile, name_area=sshareaname,
            file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False, maskocean=False, debug=debug,
            **kwargs)
        glo, glo_areacell, keyerror3 = Read_data_mask_area(
            sshfile, sshname, "sea surface height", metric, "global2", file_area=sshareafile, name_area=sshareaname,
            file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False, maskocean=False, debug=debug,
            **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(equ.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(equ)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        glo, _, keyerror = PreProcessTS(
            glo, "", areacell=glo_areacell, average="horizontal", region="global2", **kwargs)
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        off, method, keyerror1 = remove_global_mean(off, glo, "SSH", method)
        equ, _, keyerror2 = remove_global_mean(equ, glo, "", "")
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 2.3 SSH in 'box' are normalized / detrended / smoothed (running average) if applicable
        off, method, keyerror1 = PreProcessTS(
            off, method, areacell=off_areacell, average="horizontal", region=sshbox[0], **kwargs)
        equ, _, keyerror2 = PreProcessTS(
            equ, "", areacell=equ_areacell, average="horizontal", region=sshbox[1], **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del equ_areacell, glo_areacell, my_det, off_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(equ) " + str([ax.id for ax in equ.getAxisList()]),
                          "axes2": "(off) " + str([ax.id for ax in off.getAxisList()]),
                          "shape1": "(equ) " + str(equ.shape), "shape2": "(off) " + str(off.shape),
                          "time1": "(equ) " + str(TimeBounds(equ)), "time2": "(off) " + str(TimeBounds(off))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Compute monthly gradient
        # ------------------------------------------------
        # 3.1 Compute monthly gradient
        grad = off - equ

        # ------------------------------------------------
        # 4. Mean gradient (diagnostic value)
        # ------------------------------------------------
        # 4.1 Computes mean gradient
        mv, keyerror = AverageTemporal(grad)
        if keyerror is not None:
            break
        mv = float(mv)
        # 4.2 Standard Error of the mean (function of nyears)
        mv_error = float(Std(grad)) / NUMPYsqrt(len(grad))

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            ssh_map, areacell, keyerror = Read_data_mask_area(
                sshfile, sshname, "sea surface height", metric, "equatorial_pacific_LatExt2", file_area=sshareafile,
                name_area=sshareaname, file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False,
                maskocean=False, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Remove global mean SSH to local SSH at every time step
            ssh_map, _, keyerror = remove_global_mean(ssh_map, glo, "", "")
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(map) " + str(ssh_map.shape), "time1": "(map) " + str(TimeBounds(ssh_map))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean: netcdf", 15, **dict_debug)
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            ssh_map, _, keyerror = PreProcessTS(
                ssh_map, "", areacell=areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(map) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            ssh_map = Regrid(ssh_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(map) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Zonal average
            ssh_lat = ssh_map(longitude=ReferenceRegions(sshbox[0])["longitude"])
            ssh_lat, keyerror = AverageZonal(ssh_lat)
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(lat) " + str([ax.id for ax in ssh_lat.getAxisList()]),
                              "shape1": "(lat) " + str(ssh_lat.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal: netcdf", 15, **dict_debug)
            # Spatial mean value
            mean_map, keyerror1 = AverageHorizontal(ssh_map, region="equatorial_pacific_LatExt2")
            mean_lat, keyerror2 = AverageAxis(ssh_lat)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Remove spatial mean
            ssh_map = ssh_map - mean_map
            ssh_lat = ssh_lat - mean_lat
            # Supplementary metrics
            std_lat = float(Std(ssh_lat, weights=None, axis=0, centered=1, biased=1))
            std_map = float(Std(ssh_map, weights=None, axis="xy", centered=1, biased=1))
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_lat,
                     "description": "mean dynamic SSH across latitudes (zonal averaged [" + str(lon0[0]) + "-" +
                                    str(lon1[1]) + "]), mean spatial value removed"}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_map,
                     "description": "mean dynamic SSH map, mean spatial value removed"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=ssh_lat, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                       var2=ssh_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": nbr_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def grad_lat_sst(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, dataset="",
                 debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The grad_lat_sst() function computes the difference between the SST (sea surface temperature) in the eastern
    off-equatorial and equatorial Pacific.
    The definition of Cai et al. (2014; https://doi.org/10.1038/nclimate2100) is used.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon March  9 2021

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: list
        list of region names (['grad_off', 'grad_equ']) for SST
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'grad_lat_sst_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "meridional SST gradient in the eastern Pacific"
    lat = ReferenceRegions(sstbox[0])['latitude']
    lat0 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat1 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sstbox[0])['longitude']
    lon0 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon1 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    lat = ReferenceRegions(sstbox[0])['latitude']
    lat2 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat3 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sstbox[0])['longitude']
    lon2 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon3 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    units = "C"
    method = str(sstbox[0]) + " averaged (" + str(lat0[0]) + "-" + str(lat1[1]) + " ; " + str(lon0[0]) + \
             "-" + str(lon1[1]) + ") minus " + str(sstbox[1]) + " averaged (" + str(lat2[0]) + "-" + str(lat3[1]) + \
             " ; " + str(lon2[0]) + "-" + str(lon3[1]) + ") sea surface temperature (SST)"
    ref = "Using CDAT averager and std"
    metric = "grad_lat_sst"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    actualtimebounds, nbr_year, keyerror = None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        off, off_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox[0], file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        equ, equ_areacell, keyerror2 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox[1], file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(equ.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(equ)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        off, method, keyerror1 = PreProcessTS(
            off, method, areacell=off_areacell, average="horizontal", region=sstbox[0], **kwargs)
        equ, _, keyerror2 = PreProcessTS(
            equ, "", areacell=equ_areacell, average="horizontal", region=sstbox[1], **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del equ_areacell, off_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(equ) " + str([ax.id for ax in equ.getAxisList()]),
                          "axes2": "(off) " + str([ax.id for ax in off.getAxisList()]),
                          "shape1": "(equ) " + str(equ.shape), "shape2": "(off) " + str(off.shape),
                          "time1": "(equ) " + str(TimeBounds(equ)), "time2": "(off) " + str(TimeBounds(off))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Compute monthly gradient
        # ------------------------------------------------
        # 3.1 Compute monthly gradient
        grad = off - equ

        # ------------------------------------------------
        # 4. Mean gradient (diagnostic value)
        # ------------------------------------------------
        # 4.1 Computes mean gradient
        mv, keyerror = AverageTemporal(grad)
        if keyerror is not None:
            break
        mv = float(mv)
        # 4.2 Standard Error of the mean (function of nyears)
        mv_error = float(Std(grad)) / NUMPYsqrt(len(grad))

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, areacell, keyerror = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            sst_map, _, keyerror = PreProcessTS(
                sst_map, "", areacell=areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "shape1": "(map) " + str(sst_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "shape1": "(map) " + str(sst_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Zonal average
            sst_lat = sst_map(longitude=ReferenceRegions(sstbox[0])["longitude"])
            sst_lat, keyerror = AverageZonal(sst_lat)
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(lat) " + str([ax.id for ax in sst_lat.getAxisList()]),
                              "shape1": "(lat) " + str(sst_lat.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal: netcdf", 15, **dict_debug)
            # Supplementary metrics
            std_lat = float(Std(sst_lat, weights=None, axis=0, centered=1, biased=1))
            std_map = float(Std(sst_map, weights=None, axis="xy", centered=1, biased=1))
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_lat,
                     "description": "mean SST across latitudes (zonal averaged [" + str(lon0[0]) + "-" + str(lon1[1]) +
                                    "])"}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_map,
                     "description": "mean SST map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sst_lat, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                       var2=sst_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": nbr_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def grad_lon_pr(prfile, prname, prareafile, prareaname, prlandmaskfile, prlandmaskname, prbox, dataset="", debug=False,
                netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The grad_lon_pr() function computes the difference between the PR (precipitation) in the western and eastern
    equatorial Pacific.
    The definition of Burls and Fedorov (2014; https://doi.org/10.1175/JCLI-D-13-00255.1) is used.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon March  9 2021

    Inputs:
    ------
    :param prfile: string
        path_to/filename of the file (NetCDF) of PR
    :param prname: string
        name of PR variable (pr, prec, precip, rain) in 'prfile'
    :param prareafile: string
        path_to/filename of the file (NetCDF) of the areacell for PR
    :param prareaname: string
        name of areacell variable (areacella, areacello) in 'prareafile'
    :param prlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for PR
    :param prlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfile'
    :param prbox: list
        list of region names (['grad_west', 'grad_east']) for PR
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'grad_lon_pr_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "meridional PR gradient in the eastern Pacific"
    lat = ReferenceRegions(prbox[0])['latitude']
    lat0 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat1 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(prbox[0])['longitude']
    lon0 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon1 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    lat = ReferenceRegions(prbox[0])['latitude']
    lat2 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat3 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(prbox[0])['longitude']
    lon2 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon3 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    units = "mm/day"
    method = str(prbox[0]) + " averaged (" + str(lat0[0]) + "-" + str(lat1[1]) + " ; " + str(lon0[0]) + \
             "-" + str(lon1[1]) + ") minus " + str(prbox[1]) + " averaged (" + str(lat2[0]) + "-" + str(lat3[1]) + \
             " ; " + str(lon2[0]) + "-" + str(lon3[1]) + ") precipitation (PR)"
    ref = "Using CDAT averager and std"
    metric = "grad_lon_pr"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    actualtimebounds, nbr_year, keyerror = None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        west, west_areacell, keyerror1 = Read_data_mask_area(
            prfile, prname, "precipitation", metric, prbox[0], file_area=prareafile, name_area=prareaname,
            file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        east, east_areacell, keyerror2 = Read_data_mask_area(
            prfile, prname, "precipitation", metric, prbox[1], file_area=prareafile, name_area=prareaname,
            file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(east.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(east)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 PR averaged in 'prbox' are normalized / detrended / smoothed (running average) if applicable
        west, method, keyerror1 = PreProcessTS(
            west, method, areacell=west_areacell, average="horizontal", region=prbox[0], **kwargs)
        east, _, keyerror2 = PreProcessTS(
            east, "", areacell=east_areacell, average="horizontal", region=prbox[1], **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del east_areacell, west_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(east) " + str([ax.id for ax in east.getAxisList()]),
                          "axes2": "(west) " + str([ax.id for ax in west.getAxisList()]),
                          "shape1": "(east) " + str(east.shape), "shape2": "(west) " + str(west.shape),
                          "time1": "(east) " + str(TimeBounds(east)), "time2": "(west) " + str(TimeBounds(west))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Compute monthly gradient
        # ------------------------------------------------
        # 3.1 Compute monthly gradient
        grad = west - east

        # ------------------------------------------------
        # 4. Mean gradient (diagnostic value)
        # ------------------------------------------------
        # 4.1 Computes mean gradient
        mv, keyerror = AverageTemporal(grad)
        if keyerror is not None:
            break
        mv = float(mv)
        # 4.2 Standard Error of the mean (function of nyears)
        mv_error = float(Std(grad)) / NUMPYsqrt(len(grad))

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            pr_map, areacell, keyerror = Read_data_mask_area(
                prfile, prname, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafile,
                name_area=prareaname, file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            pr_map, _, keyerror = PreProcessTS(
                pr_map, "", areacell=areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in pr_map.getAxisList()]),
                              "shape1": "(map) " + str(pr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            pr_map = Regrid(pr_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in pr_map.getAxisList()]),
                              "shape1": "(map) " + str(pr_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            pr_lon = pr_map(latitude=ReferenceRegions(prbox[0])["latitude"])
            pr_lon, keyerror = AverageMeridional(pr_lon)
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(lat) " + str([ax.id for ax in pr_lon.getAxisList()]),
                              "shape1": "(lat) " + str(pr_lon.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Supplementary metrics
            std_lon = float(Std(pr_lon, weights=None, axis=0, centered=1, biased=1))
            std_map = float(Std(pr_map, weights=None, axis="xy", centered=1, biased=1))
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_lon,
                     "description": "mean PR across longitudes (meridional averaged [" + str(lat0[0]) + "-" +
                                    str(lat1[1]) + "])"}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_map,
                     "description": "mean PR map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=pr_lon, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                       var2=pr_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": nbr_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def grad_lon_ssh(sshfile, sshname, sshareafile, sshareaname, sshlandmaskfile, sshlandmaskname, sshbox, dataset="",
                 debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The grad_lon_ssh() function computes the difference between the dynamic SSH (sea shurface height) in the western
    and eastern equatorial Pacific.
    The definition of Burls and Fedorov (2014; https://doi.org/10.1175/JCLI-D-13-00255.1) is used.
    The mean spatio-temporal value in the region is subtracted because the geoid is not defined the same way in models
    and observations. The additional mass linked to the melting of the polar caps and the thermal expansion of the ocean
    is not usually taken into account in climate models.
    This metric cannot measure the mean bias (SSH too high or too low) but can illustrate is gradients are reproduced.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Wed April 28 2021

    Inputs:
    ------
    :param sshfile: string
        path_to/filename of the file (NetCDF) of SSH
    :param sshname: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfile'
    :param sshareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SSH
    :param sshareaname: string
        name of areacell variable (areacella, areacello) in 'sshareafile'
    :param sshlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SSH
    :param sshlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfile'
    :param sshbox: list
        list of region names (['grad_west', 'grad_east']) for SSH
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'grad_lon_ssh_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "meridional dynamic SSH gradient in the eastern Pacific"
    lat = ReferenceRegions(sshbox[0])['latitude']
    lat0 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat1 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sshbox[0])['longitude']
    lon0 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon1 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    lat = ReferenceRegions(sshbox[0])['latitude']
    lat2 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat3 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sshbox[0])['longitude']
    lon2 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon3 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    units = "C"
    method = str(sshbox[0]) + " averaged (" + str(lat0[0]) + "-" + str(lat1[1]) + " ; " + str(lon0[0]) + \
             "-" + str(lon1[1]) + ") minus " + str(sshbox[1]) + " averaged (" + str(lat2[0]) + "-" + str(lat3[1]) + \
             " ; " + str(lon2[0]) + "-" + str(lon3[1]) + ") dynamic sea surface height (SSH)"
    ref = "Using CDAT averager and std"
    metric = "grad_lon_ssh"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    actualtimebounds, nbr_year, keyerror = None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        west, west_areacell, keyerror1 = Read_data_mask_area(
            sshfile, sshname, "sea surface height", metric, sshbox[0], file_area=sshareafile, name_area=sshareaname,
            file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False, maskocean=False, debug=debug,
            **kwargs)
        east, east_areacell, keyerror2 = Read_data_mask_area(
            sshfile, sshname, "sea surface height", metric, sshbox[1], file_area=sshareafile, name_area=sshareaname,
            file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False, maskocean=False, debug=debug,
            **kwargs)
        glo, glo_areacell, keyerror3 = Read_data_mask_area(
            sshfile, sshname, "sea surface height", metric, "global2", file_area=sshareafile, name_area=sshareaname,
            file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False, maskocean=False, debug=debug,
            **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(east.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(east)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        glo, _, keyerror = PreProcessTS(
            glo, "", areacell=glo_areacell, average="horizontal", region="global2", **kwargs)
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        west, method, keyerror1 = remove_global_mean(west, glo, "SSH", method)
        east, _, keyerror2 = remove_global_mean(east, glo, "", "")
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 2.3 SSH in 'box' are normalized / detrended / smoothed (running average) if applicable
        west, method, keyerror1 = PreProcessTS(
            west, method, areacell=west_areacell, average="horizontal", region=sshbox[0], **kwargs)
        east, _, keyerror2 = PreProcessTS(
            east, "", areacell=east_areacell, average="horizontal", region=sshbox[1], **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del east_areacell, glo_areacell, my_det, west_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(east) " + str([ax.id for ax in east.getAxisList()]),
                          "axes2": "(west) " + str([ax.id for ax in west.getAxisList()]),
                          "shape1": "(east) " + str(east.shape), "shape2": "(west) " + str(west.shape),
                          "time1": "(east) " + str(TimeBounds(east)), "time2": "(west) " + str(TimeBounds(west))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Compute monthly gradient
        # ------------------------------------------------
        # 3.1 Compute monthly gradient
        grad = west - east

        # ------------------------------------------------
        # 4. Mean gradient (diagnostic value)
        # ------------------------------------------------
        # 4.1 Computes mean gradient
        mv, keyerror = AverageTemporal(grad)
        if keyerror is not None:
            break
        mv = float(mv)
        # 4.2 Standard Error of the mean (function of nyears)
        mv_error = float(Std(grad)) / NUMPYsqrt(len(grad))

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            ssh_map, areacell, keyerror = Read_data_mask_area(
                sshfile, sshname, "sea surface height", metric, "equatorial_pacific_LatExt2", file_area=sshareafile,
                name_area=sshareaname, file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=False,
                maskocean=False, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Remove global mean SSH to local SSH at every time step
            ssh_map, _, keyerror = remove_global_mean(ssh_map, glo, "", "")
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(map) " + str(ssh_map.shape), "time1": "(map) " + str(TimeBounds(ssh_map))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean: netcdf", 15, **dict_debug)
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            ssh_map, _, keyerror = PreProcessTS(
                ssh_map, "", areacell=areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(map) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            ssh_map = Regrid(ssh_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in ssh_map.getAxisList()]),
                              "shape1": "(map) " + str(ssh_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            ssh_lon = ssh_map(latitude=ReferenceRegions(sshbox[0])["latitude"])
            ssh_lon, keyerror = AverageMeridional(ssh_lon)
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(lat) " + str([ax.id for ax in ssh_lon.getAxisList()]),
                              "shape1": "(lat) " + str(ssh_lon.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Spatial mean value
            mean_map, keyerror1 = AverageHorizontal(ssh_map, region="equatorial_pacific_LatExt2")
            mean_lon, keyerror2 = AverageAxis(ssh_lon)
            keyerror = add_up_errors([keyerror1, keyerror2])
            if keyerror is not None:
                break
            # Remove spatial mean
            ssh_map = ssh_map - float(mean_map)
            ssh_lon = ssh_lon - float(mean_lon)
            # Supplementary metrics
            std_lon = float(Std(ssh_lon, weights=None, axis=0, centered=1, biased=1))
            std_map = float(Std(ssh_map, weights=None, axis="xy", centered=1, biased=1))
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_lon,
                     "description": "mean dynamic SSH across longitudes (meridional averaged [" + str(lat0[0]) + "-" +
                                    str(lat1[1]) + "]), mean spatial value removed"}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_map,
                     "description": "mean dynamic SSH map, mean spatial value removed"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=ssh_lon, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                       var2=ssh_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": nbr_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def grad_lon_sst(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, dataset="",
                 debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The grad_lon_sst() function computes the difference between the SST (sea surface temperature) in the western and
    eastern equatorial Pacific.
    The definition of Burls and Fedorov (2014; https://doi.org/10.1175/JCLI-D-13-00255.1) is used.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Mon March  9 2021

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (sst, tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: list
        list of region names (['grad_west', 'grad_east']) for SST
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'grad_lon_sst_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "meridional SST gradient in the eastern Pacific"
    lat = ReferenceRegions(sstbox[0])['latitude']
    lat0 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat1 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sstbox[0])['longitude']
    lon0 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon1 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    lat = ReferenceRegions(sstbox[0])['latitude']
    lat2 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat3 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sstbox[0])['longitude']
    lon2 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon3 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    units = "C"
    method = str(sstbox[0]) + " averaged (" + str(lat0[0]) + "-" + str(lat1[1]) + " ; " + str(lon0[0]) + \
             "-" + str(lon1[1]) + ") minus " + str(sstbox[1]) + " averaged (" + str(lat2[0]) + "-" + str(lat3[1]) + \
             " ; " + str(lon2[0]) + "-" + str(lon3[1]) + ") sea surface temperature (SST)"
    ref = "Using CDAT averager and std"
    metric = "grad_lon_sst"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    actualtimebounds, nbr_year, keyerror = None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        west, west_areacell, keyerror1 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox[0], file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        east, east_areacell, keyerror2 = Read_data_mask_area(
            sstfile, sstname, "temperature", metric, sstbox[1], file_area=sstareafile, name_area=sstareaname,
            file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            break
        # 1.2 Compute number of years
        nbr_year = int(round(east.shape[0] / 12.))
        # 1.3 Read time period used
        actualtimebounds = TimeBounds(east)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'sstbox' are normalized / detrended / smoothed (running average) if applicable
        west, method, keyerror1 = PreProcessTS(
            west, method, areacell=west_areacell, average="horizontal", region=sstbox[0], **kwargs)
        east, _, keyerror2 = PreProcessTS(
            east, "", areacell=east_areacell, average="horizontal", region=sstbox[1], **kwargs)
        keyerror = add_up_errors([keyerror1, keyerror2])
        del east_areacell, west_areacell
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(east) " + str([ax.id for ax in east.getAxisList()]),
                          "axes2": "(west) " + str([ax.id for ax in west.getAxisList()]),
                          "shape1": "(east) " + str(east.shape), "shape2": "(west) " + str(west.shape),
                          "time1": "(east) " + str(TimeBounds(east)), "time2": "(west) " + str(TimeBounds(west))}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Compute monthly gradient
        # ------------------------------------------------
        # 3.1 Compute monthly gradient
        grad = west - east

        # ------------------------------------------------
        # 4. Mean gradient (diagnostic value)
        # ------------------------------------------------
        # 4.1 Computes mean gradient
        mv, keyerror = AverageTemporal(grad)
        if keyerror is not None:
            break
        mv = float(mv)
        # 4.2 Standard Error of the mean (function of nyears)
        mv_error = float(Std(grad)) / NUMPYsqrt(len(grad))

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            sst_map, areacell, keyerror = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafile,
                name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                maskocean=False, debug=debug, **kwargs)
            if keyerror is not None:
                break
            # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            sst_map, _, keyerror = PreProcessTS(
                sst_map, "", areacell=areacell, average="time", region="equatorial_pacific_LatExt2", **kwargs)
            del areacell
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "shape1": "(map) " + str(sst_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Regridding
            if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                        "newgrid_name": "generic_1x1deg"}
            sst_map = Regrid(sst_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(map) " + str([ax.id for ax in sst_map.getAxisList()]),
                              "shape1": "(map) " + str(sst_map.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid: netcdf", 15, **dict_debug)
            # Meridional average
            sst_lon = sst_map(latitude=ReferenceRegions(sstbox[0])["latitude"])
            sst_lon, keyerror = AverageMeridional(sst_lon)
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(lat) " + str([ax.id for ax in sst_lon.getAxisList()]),
                              "shape1": "(lat) " + str(sst_lon.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Supplementary metrics
            std_lon = float(Std(sst_lon, weights=None, axis=0, centered=1, biased=1))
            std_map = float(Std(sst_map, weights=None, axis="xy", centered=1, biased=1))
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_lon,
                     "description": "mean SST across longitudes (meridional averaged [" + str(lat0[0]) + "-" +
                                    str(lat1[1]) + "])"}
            dict2 = {"units": units, "number_of_years_used": nbr_year, "time_period": str(actualtimebounds),
                     "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_map,
                     "description": "mean SST map"}
            dict3 = {"metric_name": name, "metric_method": method, "metric_reference": ref,
                     "frequency": kwargs["frequency"]}
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sst_lon, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                       var2=sst_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
            del dict1, dict2, dict3, file_name
    # Metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": nbr_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalPrLatRmse(prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                      prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs,
                      box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                      netcdf_name="", metname="", **kwargs):
    """
    The SeasonalPrLatRmse() function computes the climatological (12 months) PR (precipitation) meridional (latitude)
    standard deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the nino3_LatExt)

    Inputs:
    ------
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr, prec, precip, rain) in 'prfilemod'
    :param prareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for PR
    :param prareanamemod: string
        name of areacell variable (areacella, areacello) in 'prareafilemod'
    :param prlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for PR
    :param prlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfilemod'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, prec, precip, rain) in 'prfileobs'
    :param prareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for PR
    :param prareanameobs: string
        name of areacell variable (areacella, areacello) in 'prareafileobs'
    :param prlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for PR
    :param prlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for PR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'GPCPv2.3',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalPrLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "PR meridional seasonality RMSE"
    units = "mm/day"
    method = "Meridional root mean square error of " + box + " climatological precipitation (PR) STD"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalPrLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        pr_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            prfilemod, prnamemod, "precipitation", metric, box, file_area=prareafilemod, name_area=prareanamemod,
            file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        pr_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            prfileobs, prnameobs, "precipitation", metric, box, file_area=prareafileobs, name_area=prareanameobs,
            file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(pr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(pr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(pr_mod)
        actualtimebounds_obs = TimeBounds(pr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 PR averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, method, keyerror_mod = PreProcessTS(
            pr_mod, method, areacell=areacell_mod, compute_sea_cycle=True, region=box, **kwargs)
        pr_obs, _, keyerror_obs = PreProcessTS(
            pr_obs, "", areacell=areacell_obs, compute_sea_cycle=True, region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation and zonal average
        # ------------------------------------------------
        # 3.1 Standard deviation computation
        prStd_mod = Std(pr_mod)
        prStd_obs = Std(pr_obs)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStd_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in prStd_obs.getAxisList()]),
                          "shape1": "(mod) " + str(prStd_mod.shape), "shape2": "(obs) " + str(prStd_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            prStd_mod, prStd_obs, method = TwoVarRegrid(
                prStd_mod, prStd_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in prStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(prStd_mod.shape), "shape2": "(obs) " + str(prStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Zonal average
        prStdLat_mod, keyerror_mod = AverageZonal(prStd_mod)
        prStdLat_obs, keyerror_obs = AverageZonal(prStd_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStdLat_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in prStdLat_obs.getAxisList()]),
                          "shape1": "(mod) " + str(prStdLat_mod.shape), "shape2": "(obs) " + str(prStdLat_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(prStdLat_mod, prStdLat_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(prStdLat_mod, prStdLat_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(prStdLat_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(prStdLat_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
                prfilemod, prnamemod, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafilemod,
                name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmaskfilemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
                prfileobs, prnameobs, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafileobs,
                name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmaskfileobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess PR (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(map_mod, "", areacell=areacell_mod, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(map_obs, "", areacell=areacell_obs, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del areacell_mod, areacell_obs
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netCDF", 15, **dict_debug)
            # Standard deviation computation
            map_mod = Std(map_mod)
            map_obs = Std(map_obs)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                pr_mod, pr_obs, _ = TwoVarRegrid(pr_mod, pr_obs, "", region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
            # Zonal average
            pr_mod, keyerror_mod = AverageZonal(pr_mod)
            pr_obs, keyerror_obs = AverageZonal(pr_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="map of the standard deviation of the mean annual cycle of PR")
            dict_metric, dict_nc = fill_dict_axis(
                pr_mod, pr_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 5, ovar[2], "hov", units, "01", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean annual cycle of PR across latitudes")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_mod),
                     "description": "standard deviation of the mean annual cycle of PR across latitudes"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_obs),
                     "description": "standard deviation of the mean annual cycle of PR across latitudes"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=prStdLat_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=prStdLat_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalPrLonRmse(prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                      prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs,
                      box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                      netcdf_name="", metname="", **kwargs):
    """
    The SeasonalPrLonRmse() function computes the climatological (12 months) PR (precipitation) zonal (longitude)
    standard deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the Equatorial Pacific)

    Inputs:
    ------
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr, prec, precip, rain) in 'prfilemod'
    :param prareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for PR
    :param prareanamemod: string
        name of areacell variable (areacella, areacello) in 'prareafilemod'
    :param prlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for PR
    :param prlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfilemod'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, prec, precip, rain) in 'prfileobs'
    :param prareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for PR
    :param prareanameobs: string
        name of areacell variable (areacella, areacello) in 'prareafileobs'
    :param prlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for PR
    :param prlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for PR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'GPCPv2.3',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalPrLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "PR zonal seasonality RMSE"
    units = "mm/day"
    method = "Zonal root mean square error of " + box + " climatological precipitation (PR) STD"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalPrLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        pr_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            prfilemod, prnamemod, "precipitation", metric, box, file_area=prareafilemod, name_area=prareanamemod,
            file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        pr_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            prfileobs, prnameobs, "precipitation", metric, box, file_area=prareafileobs, name_area=prareanameobs,
            file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(pr_mod.shape[0] / 12.))
        nbr_year_obs = int(round(pr_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(pr_mod)
        actualtimebounds_obs = TimeBounds(pr_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 PR averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, method, keyerror_mod = PreProcessTS(
            pr_mod, method, areacell=areacell_mod, compute_sea_cycle=True, region=box, **kwargs)
        pr_obs, _, keyerror_obs = PreProcessTS(
            pr_obs, "", areacell=areacell_obs, compute_sea_cycle=True, region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                          "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation and meridional average
        # ------------------------------------------------
        # 3.1 Standard deviation computation
        prStd_mod = Std(pr_mod)
        prStd_obs = Std(pr_obs)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStd_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in prStd_obs.getAxisList()]),
                          "shape1": "(mod) " + str(prStd_mod.shape), "shape2": "(obs) " + str(prStd_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            prStd_mod, prStd_obs, method = TwoVarRegrid(
                prStd_mod, prStd_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in prStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(prStd_mod.shape), "shape2": "(obs) " + str(prStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Meridional average
        prStdLon_mod, keyerror_mod = AverageMeridional(prStd_mod)
        prStdLon_obs, keyerror_obs = AverageMeridional(prStd_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStdLon_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in prStdLon_obs.getAxisList()]),
                          "shape1": "(mod) " + str(prStdLon_mod.shape), "shape2": "(obs) " + str(prStdLon_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(prStdLon_mod, prStdLon_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(prStdLon_mod, prStdLon_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(prStdLon_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(prStdLon_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
                prfilemod, prnamemod, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafilemod,
                name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmaskfilemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
                prfileobs, prnameobs, "precipitation", metric, "equatorial_pacific_LatExt2", file_area=prareafileobs,
                name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmaskfileobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess PR (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(map_mod, "", areacell=areacell_mod, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(map_obs, "", areacell=areacell_obs, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del areacell_mod, areacell_obs
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netCDF", 15, **dict_debug)
            # Standard deviation computation
            map_mod = Std(map_mod)
            map_obs = Std(map_obs)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                pr_mod, pr_obs, _ = TwoVarRegrid(pr_mod, pr_obs, "", region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
            # Meridional average
            pr_mod, keyerror_mod = AverageMeridional(pr_mod)
            pr_obs, keyerror_obs = AverageMeridional(pr_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="map of the standard deviation of the mean annual cycle of PR")
            dict_metric, dict_nc = fill_dict_axis(
                pr_mod, pr_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 5, ovar[2], "hov", units, "01", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean annual cycle of PR across longitudes")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_mod),
                     "description": "standard deviation of the mean annual cycle of PR across longitudes"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_obs),
                     "description": "standard deviation of the mean annual cycle of PR across longitudes"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=prStdLon_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=prStdLon_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalSshLatRmse(sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                       sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs,
                       box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                       netcdf_name="", metname="", **kwargs):
    """
    The SeasonalSshLatRmse() function computes the climatological (12 months) dynamic SSH (sea surface height)
    meridional (latitude) standard deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the
    nino3_LatExt).
    The mean spatio-temporal value in the region is subtracted because the geoid is not defined the same way in models
    and observations. The additional mass linked to the melting of the polar caps and the thermal expansion of the ocean
    is not usually taken into account in climate models.
    This metric cannot measure the mean bias (SSH too high or too low) but can illustrate is gradients are reproduced.

    Inputs:
    ------
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SSH
    :param sshareanamemod: string
        name of areacell variable (areacella, areacello) in 'sshareafilemod'
    :param sshlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SSH
    :param sshlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfilemod'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SSH
    :param sshareanameobs: string
        name of areacell variable (areacella, areacello) in 'sshareafileobs'
    :param sshlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SSH
    :param sshlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for SSH
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'AVISO',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalSshLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "dynamic SSH meridional seasonality RMSE"
    units = "cm"
    method = "Meridional root mean square error of " + box + " climatological dynamic sea surface height (SSH) STD"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalSshLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        ssh_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, box, file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_mod_glo, areacell_mod_glo, keyerror_mod_glo = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, "global2", file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, box, file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        ssh_obs_glo, areacell_obs_glo, keyerror_obs_glo = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, "global2", file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_mod_glo, keyerror_obs, keyerror_obs_glo])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(ssh_mod.shape[0] / 12.))
        nbr_year_obs = int(round(ssh_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(ssh_mod)
        actualtimebounds_obs = TimeBounds(ssh_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        ssh_mod_glo, _, keyerror_mod = PreProcessTS(
            ssh_mod_glo, "", areacell=areacell_mod_glo, average="horizontal", region="global2", **kwargs)
        ssh_obs_glo, _, keyerror_obs = PreProcessTS(
            ssh_obs_glo, "", areacell=areacell_obs_glo, average="horizontal", region="global2", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        ssh_mod, method, keyerror_mod = remove_global_mean(ssh_mod, ssh_mod_glo, "SSH", method)
        ssh_obs, _, keyerror_obs = remove_global_mean(ssh_obs, ssh_obs_glo, "", "")
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean", 15, **dict_debug)
        # 2.3 SSH averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        ssh_mod, method, keyerror_mod = PreProcessTS(
            ssh_mod, method, areacell=areacell_mod, compute_sea_cycle=True, region=box, **kwargs)
        ssh_obs, _, keyerror_obs = PreProcessTS(
            ssh_obs, "", areacell=areacell_obs, compute_sea_cycle=True, region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_mod_glo, areacell_obs, areacell_obs_glo, my_det
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation and zonal average
        # ------------------------------------------------
        # 3.1 Standard deviation computation
        sshStd_mod = Std(ssh_mod)
        sshStd_obs = Std(ssh_obs)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStd_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sshStd_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sshStd_mod.shape), "shape2": "(obs) " + str(sshStd_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            sshStd_mod, sshStd_obs, method = TwoVarRegrid(
                sshStd_mod, sshStd_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sshStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sshStd_mod.shape), "shape2": "(obs) " + str(sshStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Zonal average
        sshStdLat_mod, keyerror_mod = AverageZonal(sshStd_mod)
        sshStdLat_obs, keyerror_obs = AverageZonal(sshStd_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStdLat_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sshStdLat_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sshStdLat_mod.shape), "shape2": "(obs) " + str(sshStdLat_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(sshStdLat_mod, sshStdLat_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(sshStdLat_mod, sshStdLat_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(sshStdLat_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(sshStdLat_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
                sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific_LatExt2",
                file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                name_mask=sshlandmaskfilemod, maskland=False, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, **kwargs)
            map_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
                sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific_LatExt2",
                file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                name_mask=sshlandmaskfileobs, maskland=False, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Remove global mean SSH to local SSH at every time step
            map_mod, _, keyerror_mod = remove_global_mean(map_mod, ssh_mod_glo, "", "")
            map_obs, _, keyerror_obs = remove_global_mean(map_obs, ssh_obs_glo, "", "")
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean: netcdf", 15, **dict_debug)
            # Preprocess SSH (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(map_mod, "", areacell=areacell_mod, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(map_obs, "", areacell=areacell_obs, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del areacell_mod, areacell_obs
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Standard deviation computation
            map_mod = Std(map_mod)
            map_obs = Std(map_obs)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                ssh_mod, ssh_obs, _ = TwoVarRegrid(ssh_mod, ssh_obs, "", region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
            # Zonal average
            ssh_mod, keyerror_mod = AverageZonal(ssh_mod)
            ssh_obs, keyerror_obs = AverageZonal(ssh_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal: netcdf", 15, **dict_debug)
            # Spatial mean value
            mean_mod, keyerror_mod = AverageAxis(ssh_mod, axis="01")
            mean_obs, keyerror_obs = AverageAxis(ssh_obs, axis="01")
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Remove spatial mean
            ssh_mod = ssh_mod - mean_mod
            ssh_obs = ssh_obs - mean_obs
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric,
                description="map of the standard deviation of the mean annual cycle of dynamic SSH")
            dict_metric, dict_nc = fill_dict_axis(
                ssh_mod, ssh_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 5, ovar[2], "hov", units, "01", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean annual cycle of dynamic SSH across latitudes")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_mod),
                     "description": "standard deviation of the mean annual cycle of dynamic SSH across latitudes"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_obs),
                     "description": "standard deviation of the mean annual cycle of dynamic SSH across latitudes, " +
                                    "mean spatial value removed"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sshStdLat_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=sshStdLat_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalSshLonRmse(sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                       sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs,
                       box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                       netcdf_name="", metname="", **kwargs):
    """
    The SeasonalSshLonRmse() function computes the climatological (12 months) dynamic SSH (sea surface height)
    zonal (longitude) standard deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the
    Equatorial Pacific).
    The mean spatio-temporal value in the region is subtracted because the geoid is not defined the same way in models
    and observations. The additional mass linked to the melting of the polar caps and the thermal expansion of the ocean
    is not usually taken into account in climate models.
    This metric cannot measure the mean bias (SSH too high or too low) but can illustrate is gradients are reproduced.

    Inputs:
    ------
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SSH
    :param sshareanamemod: string
        name of areacell variable (areacella, areacello) in 'sshareafilemod'
    :param sshlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SSH
    :param sshlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfilemod'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (sla, ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SSH
    :param sshareanameobs: string
        name of areacell variable (areacella, areacello) in 'sshareafileobs'
    :param sshlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SSH
    :param sshlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SSH
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'AVISO',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalSshLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "dynamic SSH zonal seasonality RMSE"
    units = "cm"
    method = "Zonal root mean square error of " + box + " climatological dynamic sea surface height (SSH) STD"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalSshLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        ssh_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, box, file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_mod_glo, areacell_mod_glo, keyerror_mod_glo = Read_data_mask_area(
            sshfilemod, sshnamemod, "sea surface height", metric, "global2", file_area=sshareafilemod,
            name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        ssh_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, box, file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        ssh_obs_glo, areacell_obs_glo, keyerror_obs_glo = Read_data_mask_area(
            sshfileobs, sshnameobs, "sea surface height", metric, "global2", file_area=sshareafileobs,
            name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
            maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_mod_glo, keyerror_obs, keyerror_obs_glo])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(ssh_mod.shape[0] / 12.))
        nbr_year_obs = int(round(ssh_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(ssh_mod)
        actualtimebounds_obs = TimeBounds(ssh_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SSH in 'global2' are normalized / detrended / smoothed (running average) if applicable
        my_det = deepcopy(kwargs["detrending"]) if "detrending" in list(kwargs.keys()) else False
        kwargs["detrending"] = False
        ssh_mod_glo, _, keyerror_mod = PreProcessTS(
            ssh_mod_glo, "", areacell=areacell_mod_glo, average="horizontal", region="global2", **kwargs)
        ssh_obs_glo, _, keyerror_obs = PreProcessTS(
            ssh_obs_glo, "", areacell=areacell_obs_glo, average="horizontal", region="global2", **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        kwargs["detrending"] = deepcopy(my_det)
        if keyerror is not None:
            break
        # 2.2 Remove global mean SSH to local SSH at every time step
        ssh_mod, method, keyerror_mod = remove_global_mean(ssh_mod, ssh_mod_glo, "SSH", method)
        ssh_obs, _, keyerror_obs = remove_global_mean(ssh_obs, ssh_obs_glo, "", "")
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean", 15, **dict_debug)
        # 2.3 SSH averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        ssh_mod, method, keyerror_mod = PreProcessTS(
            ssh_mod, method, areacell=areacell_mod, compute_sea_cycle=True, region=box, **kwargs)
        ssh_obs, _, keyerror_obs = PreProcessTS(
            ssh_obs, "", areacell=areacell_obs, compute_sea_cycle=True, region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_mod_glo, areacell_obs, areacell_obs_glo, my_det
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                          "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation and meridional average
        # ------------------------------------------------
        # 3.1 Standard deviation computation
        sshStd_mod = Std(ssh_mod)
        sshStd_obs = Std(ssh_obs)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStd_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sshStd_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sshStd_mod.shape), "shape2": "(obs) " + str(sshStd_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            sshStd_mod, sshStd_obs, method = TwoVarRegrid(
                sshStd_mod, sshStd_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sshStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sshStd_mod.shape), "shape2": "(obs) " + str(sshStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Meridional average
        sshStdLon_mod, keyerror_mod = AverageMeridional(sshStd_mod)
        sshStdLon_obs, keyerror_obs = AverageMeridional(sshStd_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStdLon_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sshStdLon_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sshStdLon_mod.shape), "shape2": "(obs) " + str(sshStdLon_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(sshStdLon_mod, sshStdLon_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(sshStdLon_mod, sshStdLon_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(sshStdLon_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(sshStdLon_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
                sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific_LatExt2",
                file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                name_mask=sshlandmaskfilemod, maskland=False, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                debug=debug, **kwargs)
            map_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
                sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific_LatExt2",
                file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                name_mask=sshlandmaskfileobs, maskland=False, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Remove global mean SSH to local SSH at every time step
            map_mod, _, keyerror_mod = remove_global_mean(map_mod, ssh_mod_glo, "", "")
            map_obs, _, keyerror_obs = remove_global_mean(map_obs, ssh_obs_glo, "", "")
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after remove_global_mean: netcdf", 15, **dict_debug)
            # Preprocess SSH (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(map_mod, "", areacell=areacell_mod, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(map_obs, "", areacell=areacell_obs, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del areacell_mod, areacell_obs
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Standard deviation computation
            map_mod = Std(map_mod)
            map_obs = Std(map_obs)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                ssh_mod, ssh_obs, _ = TwoVarRegrid(ssh_mod, ssh_obs, "", region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
            # Meridional average
            ssh_mod, keyerror_mod = AverageMeridional(ssh_mod)
            ssh_obs, keyerror_obs = AverageMeridional(ssh_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Spatial mean value
            mean_mod, keyerror_mod = AverageAxis(ssh_mod, axis="01")
            mean_obs, keyerror_obs = AverageAxis(ssh_obs, axis="01")
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Remove spatial mean
            ssh_mod = ssh_mod - mean_mod
            ssh_obs = ssh_obs - mean_obs
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric,
                description="map of the standard deviation of the mean annual cycle of dynamic SSH")
            dict_metric, dict_nc = fill_dict_axis(
                ssh_mod, ssh_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 5, ovar[2], "hov", units, "01", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean annual cycle of dynamic SSH across longitudes")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_mod),
                     "description": "standard deviation of the mean annual cycle of dynamic SSH across longitudes"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_obs),
                     "description": "standard deviation of the mean annual cycle of dynamic SSH across longitudes, " +
                                    "mean spatial value removed"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sshStdLon_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=sshStdLon_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalSstLatRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                       sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                       box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                       netcdf_name="", metname="", **kwargs):
    """
    The SeasonalSstLatRmse() function computes the climatological (12 months) SST (sea surface temperature) meridional
    (latitude) standard deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the
    nino3_LatExt)

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalSstLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "SST meridional seasonality RMSE"
    units = "C"
    method = "Meridional root mean square error of " + box + " climatological sea surface temperature (SST) STD"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalSstLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
            file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
            file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, method, keyerror_mod = PreProcessTS(
            sst_mod, method, areacell=areacell_mod, compute_sea_cycle=True, region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=areacell_obs, compute_sea_cycle=True, region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation and zonal average
        # ------------------------------------------------
        # 3.1 Standard deviation computation
        sstStd_mod = Std(sst_mod)
        sstStd_obs = Std(sst_obs)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStd_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sstStd_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sstStd_mod.shape), "shape2": "(obs) " + str(sstStd_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            sstStd_mod, sstStd_obs, method = TwoVarRegrid(
                sstStd_mod, sstStd_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sstStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sstStd_mod.shape), "shape2": "(obs) " + str(sstStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Zonal average
        sstStdLat_mod, keyerror_mod = AverageZonal(sstStd_mod)
        sstStdLat_obs, keyerror_obs = AverageZonal(sstStd_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStdLat_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sstStdLat_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sstStdLat_mod.shape), "shape2": "(obs) " + str(sstStdLat_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(sstStdLat_mod, sstStdLat_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(sstStdLat_mod, sstStdLat_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(sstStdLat_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(sstStdLat_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafilemod,
                name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafileobs,
                name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess SST (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(map_mod, "", areacell=areacell_mod, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(map_obs, "", areacell=areacell_obs, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del areacell_mod, areacell_obs
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Standard deviation computation
            map_mod = Std(map_mod)
            map_obs = Std(map_obs)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                sst_mod, sst_obs, _ = TwoVarRegrid(sst_mod, sst_obs, "", region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
            # Zonal average
            sst_mod, keyerror_mod = AverageZonal(sst_mod)
            sst_obs, keyerror_obs = AverageZonal(sst_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="map of the standard deviation of the mean annual cycle of SST")
            dict_metric, dict_nc = fill_dict_axis(
                sst_mod, sst_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 5, ovar[2], "hov", units, "01", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean annual cycle of SST across latitudes")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_mod),
                     "description": "standard deviation of the mean annual cycle of SST across latitudes"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_obs),
                     "description": "standard deviation of the mean annual cycle of SST across latitudes"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sstStdLat_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=sstStdLat_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalSstLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                       sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                       box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                       netcdf_name="", metname="", **kwargs):
    """
    The SeasonalSstLonRmse() function computes the climatological (12 months) SST (sea surface temperature) zonal
    (longitude) standard deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the Equatorial
    Pacific)

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (sst, tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (sst, tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalSstLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "SST zonal seasonality RMSE"
    units = "C"
    method = "Zonal root mean square error of " + box + " climatological sea surface temperature (SST) STD"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalSstLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        sst_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
            file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        sst_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
            file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(sst_mod.shape[0] / 12.))
        nbr_year_obs = int(round(sst_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(sst_mod)
        actualtimebounds_obs = TimeBounds(sst_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 SST averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, method, keyerror_mod = PreProcessTS(
            sst_mod, method, areacell=areacell_mod, compute_sea_cycle=True, region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=areacell_obs, compute_sea_cycle=True, region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation and meridional average
        # ------------------------------------------------
        # 3.1 Standard deviation computation
        sstStd_mod = Std(sst_mod)
        sstStd_obs = Std(sst_obs)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStd_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sstStd_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sstStd_mod.shape), "shape2": "(obs) " + str(sstStd_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            sstStd_mod, sstStd_obs, method = TwoVarRegrid(
                sstStd_mod, sstStd_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sstStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sstStd_mod.shape), "shape2": "(obs) " + str(sstStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Meridional average
        sstStdLon_mod, keyerror_mod = AverageMeridional(sstStd_mod)
        sstStdLon_obs, keyerror_obs = AverageMeridional(sstStd_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStdLon_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in sstStdLon_obs.getAxisList()]),
                          "shape1": "(mod) " + str(sstStdLon_mod.shape), "shape2": "(obs) " + str(sstStdLon_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(sstStdLon_mod, sstStdLon_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(sstStdLon_mod, sstStdLon_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(sstStdLon_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(sstStdLon_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafilemod,
                name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafileobs,
                name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess SST (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(map_mod, "", areacell=areacell_mod, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(map_obs, "", areacell=areacell_obs, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del areacell_mod, areacell_obs
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Standard deviation computation
            map_mod = Std(map_mod)
            map_obs = Std(map_obs)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                sst_mod, sst_obs, _ = TwoVarRegrid(sst_mod, sst_obs, "", region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
            # Meridional average
            sst_mod, keyerror_mod = AverageMeridional(sst_mod)
            sst_obs, keyerror_obs = AverageMeridional(sst_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="map of the standard deviation of the mean annual cycle of SST")
            dict_metric, dict_nc = fill_dict_axis(
                sst_mod, sst_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 5, ovar[2], "hov", units, "01", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean annual cycle of SST across longitudes")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_mod),
                     "description": "standard deviation of the mean annual cycle of SST across longitudes"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_obs),
                     "description": "standard deviation of the mean annual cycle of SST across longitudes"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=sstStdLon_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=sstStdLon_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalTauxLatRmse(taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                        taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs,
                        box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                        netcdf_name="", metname="", **kwargs):
    """
    The SeasonalTauxLatRmse() function computes the climatological (12 months) Taux (zonal wind stress) meridional
    (latitude) standard deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the
    equatorial_pacific_LatExt)

    Inputs:
    ------
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Taux
    :param taunamemod: string
        name of Taux variable (tauu, tauuo, taux) in 'taufilemod'
    :param tauareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for Taux
    :param tauareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauareafilemod'
    :param taulandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for Taux
    :param taulandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfilemod'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Taux
    :param taunameobs: string
        name of Taux variable (tauu, tauuo, taux) in 'taufileobs'
    :param tauareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for Taux
    :param tauareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauareafileobs'
    :param taulandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for Taux
    :param taulandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for Taux
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalTauxLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled Taux file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed Taux file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "Taux meridional seasonality RMSE"
    units = "1e-3 N/m2"
    method = "Meridional root mean square error of " + box + " climatological zonal wind stress (Taux) STD"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalTauxLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        tau_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, box, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, box, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(tau_mod.shape[0] / 12.))
        nbr_year_obs = int(round(tau_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(tau_mod)
        actualtimebounds_obs = TimeBounds(tau_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 Taux averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod = PreProcessTS(
            tau_mod, method, areacell=areacell_mod, compute_sea_cycle=True, region=box, **kwargs)
        tau_obs, _, keyerror_obs = PreProcessTS(
            tau_obs, "", areacell=areacell_obs, compute_sea_cycle=True, region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation and zonal average
        # ------------------------------------------------
        # 3.1 Standard deviation computation
        tauStd_mod = Std(tau_mod)
        tauStd_obs = Std(tau_obs)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStd_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tauStd_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tauStd_mod.shape), "shape2": "(obs) " + str(tauStd_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tauStd_mod, tauStd_obs, method = TwoVarRegrid(
                tauStd_mod, tauStd_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauStd_mod.shape), "shape2": "(obs) " + str(tauStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Zonal average
        tauStdLat_mod, keyerror_mod = AverageZonal(tauStd_mod)
        tauStdLat_obs, keyerror_obs = AverageZonal(tauStd_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStdLat_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tauStdLat_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tauStdLat_mod.shape), "shape2": "(obs) " + str(tauStdLat_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(tauStdLat_mod, tauStdLat_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(tauStdLat_mod, tauStdLat_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tauStdLat_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tauStdLat_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmaskfilemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmaskfileobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess Taux (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(map_mod, "", areacell=areacell_mod, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(map_obs, "", areacell=areacell_obs, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del areacell_mod, areacell_obs
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Standard deviation computation
            map_mod = Std(map_mod)
            map_obs = Std(map_obs)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                tau_mod, tau_obs, _ = TwoVarRegrid(tau_mod, tau_obs, "", region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
            # Zonal average
            tau_mod, keyerror_mod = AverageZonal(tau_mod)
            tau_obs, keyerror_obs = AverageZonal(tau_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="map of the standard deviation of the mean annual cycle of Taux")
            dict_metric, dict_nc = fill_dict_axis(
                tau_mod, tau_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 5, ovar[2], "hov", units, "01", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean annual cycle of Taux across latitudes")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_mod),
                     "description": "standard deviation of the mean annual cycle of Taux across latitudes"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_obs),
                     "description": "standard deviation of the mean annual cycle of Taux across latitudes"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tauStdLat_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tauStdLat_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalTauxLonRmse(taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                        taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs,
                        box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                        netcdf_name="", metname="", **kwargs):
    """
    The SeasonalTauxLonRmse() function computes the climatological (12 months) Taux (zonal wind stress) zonal
    (longitude) standard deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the Equatorial
    Pacific)

    Inputs:
    ------
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Taux
    :param taunamemod: string
        name of Taux variable (tauu, tauuo, taux) in 'taufilemod'
    :param tauareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for Taux
    :param tauareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauareafilemod'
    :param taulandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for Taux
    :param taulandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfilemod'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Taux
    :param taunameobs: string
        name of Taux variable (tauu, tauuo, taux) in 'taufileobs'
    :param tauareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for Taux
    :param tauareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauareafileobs'
    :param taulandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for Taux
    :param taulandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for Taux
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalTauxLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled Taux file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed Taux file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "Taux zonal seasonality RMSE"
    units = "1e-3 N/m2"
    method = "Zonal root mean square error of " + box + " climatological zonal wind stress (Taux) STD"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalTauxLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        tau_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, box, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, box, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(tau_mod.shape[0] / 12.))
        nbr_year_obs = int(round(tau_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(tau_mod)
        actualtimebounds_obs = TimeBounds(tau_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 Taux averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod = PreProcessTS(
            tau_mod, method, areacell=areacell_mod, compute_sea_cycle=True, region=box, **kwargs)
        tau_obs, _, keyerror_obs = PreProcessTS(
            tau_obs, "", areacell=areacell_obs, compute_sea_cycle=True, region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation and meridional average
        # ------------------------------------------------
        # 3.1 Standard deviation computation
        tauStd_mod = Std(tau_mod)
        tauStd_obs = Std(tau_obs)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStd_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tauStd_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tauStd_mod.shape), "shape2": "(obs) " + str(tauStd_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tauStd_mod, tauStd_obs, method = TwoVarRegrid(
                tauStd_mod, tauStd_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauStd_mod.shape), "shape2": "(obs) " + str(tauStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Meridional average
        tauStdLon_mod, keyerror_mod = AverageMeridional(tauStd_mod)
        tauStdLon_obs, keyerror_obs = AverageMeridional(tauStd_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStdLon_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tauStdLon_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tauStdLon_mod.shape), "shape2": "(obs) " + str(tauStdLon_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(tauStdLon_mod, tauStdLon_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(tauStdLon_mod, tauStdLon_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tauStdLon_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tauStdLon_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmaskfilemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmaskfileobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess Taux (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(map_mod, "", areacell=areacell_mod, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(map_obs, "", areacell=areacell_obs, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del areacell_mod, areacell_obs
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Standard deviation computation
            map_mod = Std(map_mod)
            map_obs = Std(map_obs)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                tau_mod, tau_obs, _ = TwoVarRegrid(tau_mod, tau_obs, "", region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
            # Meridional average
            tau_mod, keyerror_mod = AverageMeridional(tau_mod)
            tau_obs, keyerror_obs = AverageMeridional(tau_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="map of the standard deviation of the mean annual cycle of Taux")
            dict_metric, dict_nc = fill_dict_axis(
                tau_mod, tau_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 5, ovar[2], "hov", units, "01", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean annual cycle of Taux across longitudes")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_mod),
                     "description": "standard deviation of the mean annual cycle of Taux across longitudes"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_obs),
                     "description": "standard deviation of the mean annual cycle of Taux across longitudes"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tauStdLon_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tauStdLon_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalTauyLatRmse(taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                        taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs,
                        box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                        netcdf_name="", metname="", **kwargs):
    """
    The SeasonalTauyLatRmse() function computes the climatological (12 months) Tauy (meridional wind stress) meridional
    (latitude) standard deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the
    equatorial_pacific_LatExt)

    Inputs:
    ------
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Tauy
    :param taunamemod: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufilemod'
    :param tauareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for Tauy
    :param tauareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauareafilemod'
    :param taulandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for Tauy
    :param taulandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfilemod'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Tauy
    :param taunameobs: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufileobs'
    :param tauareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for Tauy
    :param tauareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauareafileobs'
    :param taulandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for Tauy
    :param taulandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for Tauy
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalTauyLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled Tauy file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed Tauy file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "Tauy meridional seasonality RMSE"
    units = "1e-3 N/m2"
    method = "Meridional root mean square error of " + box + " climatological meridional wind stress (Tauy) STD"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalTauyLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        tau_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, box, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, box, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(tau_mod.shape[0] / 12.))
        nbr_year_obs = int(round(tau_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(tau_mod)
        actualtimebounds_obs = TimeBounds(tau_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 Tauy averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod = PreProcessTS(
            tau_mod, method, areacell=areacell_mod, compute_sea_cycle=True, region=box, **kwargs)
        tau_obs, _, keyerror_obs = PreProcessTS(
            tau_obs, "", areacell=areacell_obs, compute_sea_cycle=True, region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation and zonal average
        # ------------------------------------------------
        # 3.1 Standard deviation computation
        tauStd_mod = Std(tau_mod)
        tauStd_obs = Std(tau_obs)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStd_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tauStd_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tauStd_mod.shape), "shape2": "(obs) " + str(tauStd_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tauStd_mod, tauStd_obs, method = TwoVarRegrid(
                tauStd_mod, tauStd_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauStd_mod.shape), "shape2": "(obs) " + str(tauStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Zonal average
        tauStdLat_mod, keyerror_mod = AverageZonal(tauStd_mod)
        tauStdLat_obs, keyerror_obs = AverageZonal(tauStd_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStdLat_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tauStdLat_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tauStdLat_mod.shape), "shape2": "(obs) " + str(tauStdLat_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsMeridional(tauStdLat_mod, tauStdLat_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(tauStdLat_mod, tauStdLat_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tauStdLat_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tauStdLat_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmaskfilemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmaskfileobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess Tauy (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(map_mod, "", areacell=areacell_mod, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(map_obs, "", areacell=areacell_obs, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del areacell_mod, areacell_obs
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Standard deviation computation
            map_mod = Std(map_mod)
            map_obs = Std(map_obs)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                tau_mod, tau_obs, _ = TwoVarRegrid(tau_mod, tau_obs, "", region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
            # Zonal average
            tau_mod, keyerror_mod = AverageZonal(tau_mod)
            tau_obs, keyerror_obs = AverageZonal(tau_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="map of the standard deviation of the mean annual cycle of Tauy")
            dict_metric, dict_nc = fill_dict_axis(
                tau_mod, tau_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 5, ovar[2], "hov", units, "01", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean annual cycle of Tauy across latitudes")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_mod),
                     "description": "standard deviation of the mean annual cycle of Tauy across latitudes"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_obs),
                     "description": "standard deviation of the mean annual cycle of Tauy across latitudes"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tauStdLat_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tauStdLat_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalTauyLonRmse(taufilemod, taunamemod, tauareafilemod, tauareanamemod, taulandmaskfilemod, taulandmasknamemod,
                        taufileobs, taunameobs, tauareafileobs, tauareanameobs, taulandmaskfileobs, taulandmasknameobs,
                        box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                        netcdf_name="", metname="", **kwargs):
    """
    The SeasonalTauyLonRmse() function computes the climatological (12 months) Tauy (meridional wind stress) zonal
    (longitude) standard deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the Equatorial
    Pacific)

    Inputs:
    ------
    :param taufilemod: string
        path_to/filename of the file (NetCDF) of the modeled Tauy
    :param taunamemod: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufilemod'
    :param tauareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for Tauy
    :param tauareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauareafilemod'
    :param taulandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for Tauy
    :param taulandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfilemod'
    :param taufileobs: string
        path_to/filename of the file (NetCDF) of the observed Tauy
    :param taunameobs: string
        name of Tauy variable (tauv, tauvo, tauy) in 'taufileobs'
    :param tauareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for Tauy
    :param tauareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauareafileobs'
    :param taulandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for Tauy
    :param taulandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'taulandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for Tauy
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalTauyLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled Tauy file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed Tauy file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # Test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "Tauy zonal seasonality RMSE"
    units = "1e-3 N/m2"
    method = "Zonal root mean square error of " + box + " climatological meridional wind stress (Tauy) STD"
    ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalTauyLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Values in case of error (failure)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod, nbr_year_obs, keyerror = None, None, None, None, None

    # Create a fake loop to be able to break out if an error occur
    for break_loop in range(1):
        # ------------------------------------------------
        # 1. Read files
        # ------------------------------------------------
        if debug is True:
            EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
        # 1.1 Read file and select the right region
        tau_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
            taufilemod, taunamemod, "wind stress", metric, box, file_area=tauareafilemod, name_area=tauareanamemod,
            file_mask=taulandmaskfilemod, name_mask=taulandmasknamemod, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
        tau_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
            taufileobs, taunameobs, "wind stress", metric, box, file_area=tauareafileobs, name_area=tauareanameobs,
            file_mask=taulandmaskfileobs, name_mask=taulandmasknameobs, maskland=True, maskocean=False,
            time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        # 1.2 Compute number of years for model and observation
        nbr_year_mod = int(round(tau_mod.shape[0] / 12.))
        nbr_year_obs = int(round(tau_obs.shape[0] / 12.))
        # 1.3 Read time period used for model and observation
        actualtimebounds_mod = TimeBounds(tau_mod)
        actualtimebounds_obs = TimeBounds(tau_obs)

        # ------------------------------------------------
        # 2. Preprocess
        # ------------------------------------------------
        # 2.1 Tauy averaged in 'box' are normalized / detrended / smoothed (running average) if applicable
        tau_mod, method, keyerror_mod = PreProcessTS(
            tau_mod, method, areacell=areacell_mod, compute_sea_cycle=True, region=box, **kwargs)
        tau_obs, _, keyerror_obs = PreProcessTS(
            tau_obs, "", areacell=areacell_obs, compute_sea_cycle=True, region=box, **kwargs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        del areacell_mod, areacell_obs
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

        # ------------------------------------------------
        # 3. Standard deviation and meridional average
        # ------------------------------------------------
        # 3.1 Standard deviation computation
        tauStd_mod = Std(tau_mod)
        tauStd_obs = Std(tau_obs)
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStd_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tauStd_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tauStd_mod.shape), "shape2": "(obs) " + str(tauStd_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)
        # 3.2 Regridding
        if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                          "regridTool", "regridMethod"}
            extra_args = set(kwargs["regridding"]) - known_args
            if extra_args:
                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
            tauStd_mod, tauStd_obs, method = TwoVarRegrid(
                tauStd_mod, tauStd_obs, method, region=box, **kwargs["regridding"])
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauStd_mod.shape), "shape2": "(obs) " + str(tauStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
        # 3.3 Meridional average
        tauStdLon_mod, keyerror_mod = AverageMeridional(tauStd_mod)
        tauStdLon_obs, keyerror_obs = AverageMeridional(tauStd_obs)
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is not None:
            break
        if debug is True:
            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauStdLon_mod.getAxisList()]),
                          "axes2": "(obs) " + str([ax.id for ax in tauStdLon_obs.getAxisList()]),
                          "shape1": "(mod) " + str(tauStdLon_mod.shape), "shape2": "(obs) " + str(tauStdLon_obs.shape)}
            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

        # ------------------------------------------------
        # 4. Metric value and supplementary metric values
        # ------------------------------------------------
        # 4.1 Computes the root mean square difference
        mv, keyerror = RmsZonal(tauStdLon_mod, tauStdLon_obs, centered=centered_rmse, biased=biased_rmse)
        mv_error = None
        if keyerror is not None:
            break
        # 4.2 Supplementary metrics
        sm_corr = float(Correlation(tauStdLon_mod, tauStdLon_obs, axis=0, centered=1, biased=1))
        sm_corr_error = None
        sm_std_mod = float(Std(tauStdLon_mod, weights=None, axis=0, centered=1, biased=1))
        sm_std_obs = float(Std(tauStdLon_obs, weights=None, axis=0, centered=1, biased=1))
        sm_std = sm_std_mod / sm_std_obs
        sm_std_error = None

        # ------------------------------------------------
        # 5. Supplementary dive down diagnostics
        # ------------------------------------------------
        if netcdf is True:
            # Read file and select the right region
            map_mod, areacell_mod, keyerror_mod = Read_data_mask_area(
                taufilemod, taunamemod, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafilemod,
                name_area=tauareanamemod, file_mask=taulandmaskfilemod, name_mask=taulandmaskfilemod, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
            map_obs, areacell_obs, keyerror_obs = Read_data_mask_area(
                taufileobs, taunameobs, "wind stress", metric, "equatorial_pacific_LatExt2", file_area=tauareafileobs,
                name_area=tauareanameobs, file_mask=taulandmaskfileobs, name_mask=taulandmaskfileobs, maskland=True,
                maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            # Preprocess Tauy (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
            map_mod, _, keyerror_mod = PreProcessTS(map_mod, "", areacell=areacell_mod, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            map_obs, _, keyerror_obs = PreProcessTS(map_obs, "", areacell=areacell_obs, compute_sea_cycle=True,
                                                    region="equatorial_pacific_LatExt2", **kwargs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            del areacell_mod, areacell_obs
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                              "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 15, **dict_debug)
            # Standard deviation computation
            map_mod = Std(map_mod)
            map_obs = Std(map_obs)
            # Regridding
            if "regridding" in list(kwargs.keys()) and isinstance(kwargs["regridding"], dict) is True:
                map_mod, map_obs, _ = TwoVarRegrid(
                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(map_mod.shape), "shape2": "(obs) " + str(map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                tau_mod, tau_obs, _ = TwoVarRegrid(tau_mod, tau_obs, "", region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
            # Meridional average
            tau_mod, keyerror_mod = AverageMeridional(tau_mod)
            tau_obs, keyerror_obs = AverageMeridional(tau_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is not None:
                break
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tau_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tau_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tau_mod.shape), "shape2": "(obs) " + str(tau_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional: netcdf", 15, **dict_debug)
            # Supplementary metrics
            dict_metric, dict_nc = dict(), dict()
            dict_metric, dict_nc = fill_dict_axis(
                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 3, ovar[1], "map", units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="map of the standard deviation of the mean annual cycle of Tauy")
            dict_metric, dict_nc = fill_dict_axis(
                tau_mod, tau_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs, nbr_year_mod,
                nbr_year_obs, 5, ovar[2], "hov", units, "01", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                dict_metric=dict_metric, description="mean annual cycle of Tauy across longitudes")
            # Save netCDF
            if ".nc" in netcdf_name:
                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
            else:
                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
            dict1 = {"units": units, "number_of_years_used": nbr_year_mod, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_mod),
                     "description": "standard deviation of the mean annual cycle of Tauy across longitudes"}
            dict2 = {"units": units, "number_of_years_used": nbr_year_obs, "arraySTD": sm_std_obs,
                     "time_period": str(actualtimebounds_obs),
                     "description": "standard deviation of the mean annual cycle of Tauy across longitudes"}
            dict3 = {
                "metric_name": name, "metric_method": method, "metric_reference": ref, "frequency": kwargs["frequency"],
                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
            dict3.update(dict_metric)
            SaveNetcdf(file_name, global_attributes=dict3,
                       var1=tauStdLon_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                       var2=tauStdLon_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # Metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "ref": ref,
        "nyears_model": nbr_year_mod, "nyears_observations": nbr_year_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output
# ---------------------------------------------------------------------------------------------------------------------#


# ---------------------------------------------------------------------------------------------------------------------#
#
# Old library to compute ENSO metrics
# I (Y. Y. Planton; yann.planton@locean.ipsl.fr) do not know if they still work. They probably must be updated
#
def BiasSstSkLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                     sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                     box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                     netcdf_name="", metname="", **kwargs):
    """
    The BiasSstSkLonRmse() function computes the SST zonal (longitude) skewness and then its root mean square error
    (RMSE) in a 'box' (usually the Equatorial Pacific)

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSstSkLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'regridding', 'smoothing',
                    'time_bounds_mod', 'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'sstA Skewness Zonal RMSE'
    if kwargs['normalization']:
        Units = ''
    else:
        Units = 'C'
    Method = 'Zonal root mean square error of ' + box + ' sstA skewness'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = "BiasSstSkLonRmse"
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = \
        Read_data_mask_area(sstfilemod, sstnamemod, 'temperature', metric, box, file_area=sstareafilemod,
                            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod,
                            maskland=True, maskocean=False, time_bounds=kwargs['time_bounds_mod'], debug=debug,
                            **kwargs)
    sst_obs, obs_areacell, keyerror_obs = \
        Read_data_mask_area(sstfileobs, sstnameobs, 'temperature', metric, box, file_area=sstareafileobs,
                            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs,
                            maskland=True, maskocean=False, time_bounds=kwargs['time_bounds_obs'], debug=debug,
                            **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        sst_mod, Method, keyerror_mod = PreProcessTS(
            sst_mod, Method, areacell=mod_areacell, average=False, compute_anom=True, region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average=False, compute_anom=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sst_mod, sst_obs, Method = TwoVarRegrid(sst_mod, sst_obs, Method, region=box, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # Meridional average
            sst_mod, keyerror_mod = AverageMeridional(sst_mod)
            sst_obs, keyerror_obs = AverageMeridional(sst_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                # skewness
                sst_mod = SkewnessTemporal(sst_mod)
                sst_obs = SkewnessTemporal(sst_obs)
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after SkewnessTemporal', 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Dive down diagnostic
                dive_down_diag = {'model': ArrayToList(sst_mod), 'observations': ArrayToList(sst_obs),
                                  'axis': list(sst_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        sstfilemod, sstnamemod, 'temperature', metric, 'equatorial_pacific_LatExt2',
                        file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                        name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        sstfileobs, sstnameobs, 'temperature', metric, 'equatorial_pacific_LatExt2',
                        file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                        name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, '', areacell=mod_areacell, average=False, compute_anom=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, '', areacell=obs_areacell, average=False, compute_anom=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in map_mod.getAxisList()]),
                                              'axes2': '(obs) ' + str([ax.id for ax in map_obs.getAxisList()]),
                                              'line1': '(mod) minmax' + str(MinMax(map_mod)),
                                              'line2': '(obs) minmax' + str(MinMax(map_obs)),
                                              'shape1': '(mod) ' + str(map_mod.shape),
                                              'shape2': '(obs) ' + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    '\033[92m', 'after PreProcessTS: netcdf', 15, **dict_debug)
                            # skewness
                            ske_map_mod = SkewnessTemporal(map_mod)
                            ske_map_obs = SkewnessTemporal(map_obs)
                            if debug is True:
                                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in ske_map_mod.getAxisList()]),
                                              'axes2': '(obs) ' + str([ax.id for ax in ske_map_obs.getAxisList()]),
                                              'line1': '(mod) minmax' + str(MinMax(ske_map_mod)),
                                              'line2': '(obs) minmax' + str(MinMax(ske_map_obs)),
                                              'shape1': '(mod) ' + str(ske_map_mod.shape),
                                              'shape2': '(obs) ' + str(ske_map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    '\033[92m', 'after SkewnessTemporal: netcdf', 15, **dict_debug)
                            # Regridding
                            if isinstance(kwargs['regridding'], dict):
                                ske_map_mod, ske_map_obs, _ = TwoVarRegrid(
                                    ske_map_mod, ske_map_obs, '', region='equatorial_pacific_LatExt2',
                                    **kwargs['regridding'])
                                if debug is True:
                                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in ske_map_mod.getAxisList()]),
                                                  'axes2': '(obs) ' + str([ax.id for ax in ske_map_obs.getAxisList()]),
                                                  'line1': '(mod) minmax' + str(MinMax(ske_map_mod)),
                                                  'line2': '(obs) minmax' + str(MinMax(ske_map_obs)),
                                                  'shape1': '(mod) ' + str(ske_map_mod.shape),
                                                  'shape2': '(obs) ' + str(ske_map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        '\033[92m', 'after TwoVarRegrid: netcdf', 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod)}
                            dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs)}
                            dict3 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod)}
                            dict4 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs)}
                            dict5 = {'metric_name': Name, 'metric_value_' + dataset2: mv,
                                     'metric_value_error_' + dataset2: mv_error, 'metric_method': Method,
                                     'metric_reference': Ref, 'frequency': kwargs['frequency']}
                            SaveNetcdf(
                                file_name, var1=sst_mod, var1_attributes=dict1, var1_name='sstSke_lon__' + dataset1,
                                var2=sst_obs, var2_attributes=dict2, var2_name='sstSke_lon__' + dataset2,
                                var3=ske_map_mod, var3_attributes=dict3, var3_name='sstSke_map__' + dataset1,
                                var4=ske_map_obs, var4_attributes=dict4, var4_name='sstSke_map__' + dataset2,
                                global_attributes=dict5)
                            del dict1, dict2, dict3, dict4, dict5
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(mv), 'line2': 'metric value_error: ' + str(mv_error)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        'name': Name, 'value': mv, 'value_error': mv_error, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return metric_output


def EnsoDiversity(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
                  dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoDiversity() function computes a zonal composite of El Nino and La Nina events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > (<) 'threshold' during 'season' are considered as El Nino (La Nina) events
        2.) diversity of the zonal location of the maximum (minimum) SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the maximum (minimum) SSTA for each selected event
            2.3) compute the percentage of EP events (maximum/minimum SSTA eastward of the given threshold)
            2.4) compute the ratio EP events during La Nina divided by EP events during El Nino

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoDiversity_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param treshold_ep_ev: float, optional
        see EnsoToolsLib.percentage_val_eastward
        longitude, in degree east, of the westward boundary of eastern Pacific event
        default value is -140°E (i.e., 140°W)
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, ref, keyerror,
        dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    threshold = event_definition["threshold"]
    normalize = event_definition["normalization"]
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "treshold_ep_ev",
                    "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO Diversity (percentage of eastern Pacific El Nino / La Nina)"
    lat = ReferenceRegions(box)["latitude"]
    lon = ReferenceRegions(box)["longitude"]
    Method = "Nino (Nina) events = " + region_ev + " sstA > (<) " + str(threshold) + " during " + season_ev + \
             ", zonal SSTA " + "(meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) + \
             "]), westward boundary of EP events " + str(kwargs["treshold_ep_ev"]) + "E"
    Units = "%"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoDiversity"
    if metname == "":
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    mv, mv_error, nino_years, nina_years, dive_down_diag = None, None, None, None, {"value": None, "axis": None}
    if keyerror is None:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, _, keyerror = PreProcessTS(
            sst, "", areacell=areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        del areacell
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "time1": "(sst) " + str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # 1.2 SSTA > (<) 'threshold' during 'season' are considered as El Nino (La Nina) events
            # Lists event years
            nino_years = DetectEvents(sst, season_ev, threshold, normalization=normalize, nino=True)
            nina_years = DetectEvents(sst, season_ev, -threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {"nino1": "nbr(" + str(len(nino_years)) + "): " + str(nino_years),
                              "nina1": "nbr(" + str(len(nina_years)) + "): " + str(nina_years)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the minimum SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst, areacell, keyerror = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, box, file_area=sstareafile, name_area=sstareaname,
                file_mask=sstlandmaskfile, name_mask=sstlandmaskfile, maskland=True, maskocean=False, debug=debug,
                **kwargs)
            if keyerror is None:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst, Method, keyerror = PreProcessTS(
                    sst, Method, areacell=areacell, average=False, compute_anom=False, region=box, **kwargs)
                del areacell
                if keyerror is None:
                    if debug is True:
                        dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                                      "shape1": "(sst) " + str(sst.shape), "time1": "(sst) " + str(TimeBounds(sst))}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

                    # Seasonal mean
                    sst = SeasonalMean(sst, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                                      "shape1": "(sst) " + str(sst.shape)}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs["regridding"], dict):
                        known_args = {"newgrid", "missing", "order", "mask", "newgrid_name", "regridder", "regridTool",
                                      "regridMethod"}
                        extra_args = set(kwargs["regridding"]) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst = Regrid(sst, None, region=box, **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                                          "shape1": "(sst) " + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

                    # Meridional average
                    sst, keyerror = AverageMeridional(sst)
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                                          "shape1": "(sst) " + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                        # samples
                        sample_nino = Event_selection(sst, kwargs["frequency"], list_event_years=nino_years)
                        sample_nina = Event_selection(sst, kwargs["frequency"], list_event_years=nina_years)

                        # 2.2 find the zonal position of the maximum/minimum SSTA for each selected event
                        lon_sstmax = FindXYMinMaxInTs(
                            sample_nino, return_val="maxi", smooth=True, axis=0, window=5, method="triangle")
                        lon_sstmin = FindXYMinMaxInTs(
                            sample_nina, return_val="mini", smooth=True, axis=0, window=5, method="triangle")
                        if debug is True:
                            dict_debug = {"line1": "longitude of the maximum SSTA (nino): " + str(lon_sstmax),
                                          "line2": "longitude of the minimum SSTA (nina): " + str(lon_sstmin)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after FindXYMinMaxInTs", 15, **dict_debug)

                        # 2.3 compute the percentage of EP events (maximum/minimum SSTA eastward of the given threshold)
                        ep_event_nino, keyerror_nino = percentage_val_eastward(
                            lon_sstmax, metric, box, threshold=kwargs["treshold_ep_ev"])
                        ep_event_nina, keyerror_nina = percentage_val_eastward(
                            lon_sstmin, metric, box, threshold=kwargs["treshold_ep_ev"])

                        keyerror = add_up_errors([keyerror_nino, keyerror_nina])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"nino1": "percentage of EP event + " + str(ep_event_nino),
                                              "nina1": "percentage of EP event + " + str(ep_event_nina)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

                            # 2.4 compute the ratio EP events during La Nina divided by EP events during El Nino
                            mv = float(ep_event_nina / ep_event_nino)
                            # Standard Error of the Standard Deviation (function of nyears)
                            mv_error = None

                            # Dive down diagnostic
                            dive_down_diag = {"value": ArrayToList(lon_sstmax), "axis": list(lon_sstmax.getAxis(0)[:])}
                            if netcdf is True:
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": "longitude (E)", "number_of_years_used": yearN,
                                         "time_period": str(actualtimebounds), "nino_years": str(nino_years),
                                         "diagnostic_value_" + dataset: mv,
                                         "diagnostic_value_error_" + dataset: mv_error}
                                dict2 = {"units": "longitude (E)", "number_of_years_used": yearN,
                                         "time_period": str(actualtimebounds), "nina_years": str(nina_years),
                                         "diagnostic_value_" + dataset: mv,
                                         "diagnostic_value_error_" + dataset: mv_error}
                                dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                         "frequency": kwargs["frequency"]}
                                SaveNetcdf(file_name, var1=lon_sstmax, var1_attributes=dict1,
                                           var1_name="Nino_lon_pos_maxSSTA__" + dataset, var2=lon_sstmin,
                                           var2_attributes=dict2, var2_name="Nina_lon_pos_minSSTA__" + dataset,
                                           global_attributes=dict3)
                                del dict1, dict2, dict3
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "nyears": yearN,
        "events": nino_years + nina_years, "time_frequency": kwargs["frequency"], "time_period": actualtimebounds,
        "ref": Ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoPrMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod, prfilemod,
              prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs, sstnameobs,
              sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs, prnameobs,
              prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox, event_definition,
              centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
              metname="", **kwargs):
    """
    The EnsoPrMap() function computes precipitation anomalies pattern associated with ENSO on the globe.
    It is the regression of 'prbox' prA (precipitation anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during boreal winter (usually the regression of global prA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param prbox: string
        name of box (e.g. 'global') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrMap_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO PRA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "mm/day/C"
    Method = "map of " + prbox + " precipitation anomalies regressed onto " + region_ev + " averaged SSTA during " + \
             season_ev + enso_method3
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoPrMap"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitation", metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitation", metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, pr_mod, keyerror_mod3 = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, pr_obs, keyerror_obs3 = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        # 1.2 PR 'prbox' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        kwargs["smoothing"] = smooth
        del mod_areacell, obs_areacell, pr_mod_areacell, pr_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
            pr_mod = SeasonalMean(pr_mod, season_ev, compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=prbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod.shape), "shape2": "(pr obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            pr_mod_slope = LinearRegressionTsAgainstMap(pr_mod, enso_mod, return_stderr=False)
            pr_obs_slope = LinearRegressionTsAgainstMap(pr_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {
                    "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                    "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                    "shape1": "(pr mod) " + str(pr_mod_slope.shape), "shape2": "(pr obs) " + str(pr_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            pr_mod_slope, keyerror_mod = BasinMask(
                pr_mod_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            pr_obs_slope, keyerror_obs = BasinMask(
                pr_obs_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod_slope.shape),
                                  "shape2": "(pr obs) " + str(pr_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                prRmse, keyerror = RmsAxis(
                    pr_mod_slope, pr_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                prRmseErr = None
                # Metric 2
                prCorr = float(Correlation(pr_mod_slope, pr_obs_slope, axis="xy", centered=1, biased=1))
                prCorrErr = None
                # Metric 3
                std_mod = Std(pr_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(pr_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                prStd = float(std_mod) / float(std_obs)
                prStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    pr_mod_land, pr_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitation", metric, prbox, file_area=prareafilemod,
                        name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    pr_obs_land, pr_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitation", metric, prbox, file_area=prareafileobs,
                        name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, pr_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, pr_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, pr_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, pr_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        pr_mod_land, _, keyerror_mod = PreProcessTS(
                            pr_mod_land, "", areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
                        pr_obs_land, _, keyerror_obs = PreProcessTS(
                            pr_obs_land, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
                        kwargs["smoothing"] = smooth
                        del pr_mod_areacell, pr_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            pr_mod_land = SeasonalMean(pr_mod_land, season_ev, compute_anom=True)
                            pr_obs_land = SeasonalMean(pr_obs_land, season_ev, compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                pr_mod_land, pr_obs_land, _ = TwoVarRegrid(
                                    pr_mod_land, pr_obs_land, "", region=prbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                        "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                        "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                        "shape2": "(pr obs) " + str(pr_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            pr_mod_land_slope = LinearRegressionTsAgainstMap(pr_mod_land, enso_mod, return_stderr=False)
                            pr_obs_land_slope = LinearRegressionTsAgainstMap(pr_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land_slope.getAxisList()]),
                                    "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land_slope.getAxisList()]),
                                    "shape1": "(pr mod) " + str(pr_mod_land_slope.shape),
                                    "shape2": "(pr obs) " + str(pr_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # samples
                            en_mod = Composite(pr_mod, en_years_mod, kwargs["frequency"])
                            ln_mod = Composite(pr_mod, ln_years_mod, kwargs["frequency"])
                            en_obs = Composite(pr_obs, en_years_obs, kwargs["frequency"])
                            ln_obs = Composite(pr_obs, ln_years_obs, kwargs["frequency"])
                            en_mod_land = Composite(pr_mod_land, en_years_mod, kwargs["frequency"])
                            ln_mod_land = Composite(pr_mod_land, ln_years_mod, kwargs["frequency"])
                            en_obs_land = Composite(pr_obs_land, en_years_obs, kwargs["frequency"])
                            ln_obs_land = Composite(pr_obs_land, ln_years_obs, kwargs["frequency"])
                            # mask Pacific
                            en_mod, keyerror_mod1 = BasinMask(
                                en_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_mod, keyerror_mod2 = BasinMask(
                                ln_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            en_obs, keyerror_obs1 = BasinMask(
                                en_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_obs, keyerror_obs2 = BasinMask(
                                ln_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                            if keyerror is None:
                                my_units = "" if kwargs["normalization"] is True else "mm/day"
                                # Metrics ENSO events global
                                dict_metric, dict_nc = dict(), dict()
                                nbr = 3
                                my_de = [
                                    "map of " + prbox + " PRA of La Nina events composite during " + season_ev +
                                    "; Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev +
                                    enso_method,
                                    "map of " + prbox + " PRA of El Nino events composite during " + season_ev +
                                    "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev +
                                    enso_method]
                                for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                        zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                            [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=de)
                                    nbr += 2
                                list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                for ii, reg in enumerate(list_region):
                                    # select region
                                    dictreg = ReferenceRegions(reg)
                                    tmp1 = pr_mod_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    tmp2 = pr_obs_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, description=Method.split(", ")[0].replace(prbox, reg))
                                    nbr += 2
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                [ln_years_obs, en_years_obs])):
                                        tmp1 = tab1(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = tab2(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg + "_" + evn,
                                            my_units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de.replace(prbox, reg))
                                        nbr += 2
                                    del dictreg, tmp1, tmp2
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                         "description": Method.split(", ")[0]}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                         "description": Method.split(", ")[0]}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: prRmse,
                                    "metric_valueRMSE_error_" + dataset2: prRmseErr,
                                    "metric_valueCORR_" + dataset2: prCorr,
                                    "metric_valueCORR_error_" + dataset2: prCorrErr,
                                    "metric_valueSTD_" + dataset2: prStd, "metric_valueSTD_error_" + dataset2: prStdErr}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=pr_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=pr_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if prCorr is not None:
        prCorr = 1 - prCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(prRmse), "line2": "metric value_error: " + str(prRmseErr),
                      "line3": "metric value: " + str(prCorr), "line4": "metric value_error: " + str(prCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": prRmse, "Rmse__value_error": prRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": prCorr, "Corr__value_error": prCorrErr, "Corr__units": "", "Std__value": prStd,
        "Std__value_error": prStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "ref": Ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoPrMapDjf(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                 prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs,
                 sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs,
                 prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox,
                 event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                 netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrMapDjf() function computes precipitation anomalies pattern associated with ENSO on the globe.
    It is the regression of 'prbox' prA (precipitation anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during DJF (usually the regression of global prA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param prbox: string
        name of box (e.g. 'global') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrMapDjf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO DJF PRA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "mm/day/C"
    Method = "map of " + prbox + " precipitation anomalies regressed onto " + region_ev + " averaged SSTA during DJF"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoPrMapDjf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitation", metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitation", metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod_box, pr_mod, keyerror_mod3 = CheckTime(sst_mod_box, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs_box, pr_obs, keyerror_obs3 = CheckTime(sst_obs_box, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod_box.shape[0] / 12))
    yearN_obs = int(round(sst_obs_box.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod_box)
    actualtimebounds_obs = TimeBounds(sst_obs_box)

    prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 PR in 'prbox' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        del pr_mod_areacell, pr_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "DJF", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "DJF", compute_anom=True)
            pr_mod = SeasonalMean(pr_mod, "DJF", compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, "DJF", compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=prbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod.shape), "shape2": "(pr obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            pr_mod_slope = LinearRegressionTsAgainstMap(pr_mod, enso_mod, return_stderr=False)
            pr_obs_slope = LinearRegressionTsAgainstMap(pr_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                    "axes2": "(obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                    "shape1": "(mod) " + str(pr_mod_slope.shape), "shape2": "(obs) " + str(pr_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            pr_mod_slope, keyerror_mod = BasinMask(
                pr_mod_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            pr_obs_slope, keyerror_obs = BasinMask(
                pr_obs_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod_slope.shape),
                                  "shape2": "(pr obs) " + str(pr_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                prRmse, keyerror = RmsAxis(
                    pr_mod_slope, pr_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                prRmseErr = None
                # Metric 2
                prCorr = float(Correlation(pr_mod_slope, pr_obs_slope, axis="xy", centered=1, biased=1))
                prCorrErr = None
                # Metric 3
                std_mod = Std(pr_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(pr_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                prStd = float(std_mod) / float(std_obs)
                prStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    pr_mod_land, pr_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitation", metric, prbox, file_area=prareafilemod,
                        name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    pr_obs_land, pr_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitation", metric, prbox, file_area=prareafileobs,
                        name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, pr_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, pr_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, pr_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, pr_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        pr_mod_land, _, keyerror_mod = PreProcessTS(
                            pr_mod_land, "", areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
                        pr_obs_land, _, keyerror_obs = PreProcessTS(
                            pr_obs_land, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
                        del pr_mod_areacell, pr_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            pr_mod_land = SeasonalMean(pr_mod_land, "DJF", compute_anom=True)
                            pr_obs_land = SeasonalMean(pr_obs_land, "DJF", compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                pr_mod_land, pr_obs_land, _ = TwoVarRegrid(
                                    pr_mod_land, pr_obs_land, "", region=prbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                        "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                        "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                        "shape2": "(pr obs) " + str(pr_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            pr_mod_land_slope = LinearRegressionTsAgainstMap(pr_mod_land, enso_mod, return_stderr=False)
                            pr_obs_land_slope = LinearRegressionTsAgainstMap(pr_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land_slope.getAxisList()]),
                                    "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land_slope.getAxisList()]),
                                    "shape1": "(pr mod) " + str(pr_mod_land_slope.shape),
                                    "shape2": "(pr obs) " + str(pr_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(pr_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(pr_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(pr_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(pr_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(pr_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(pr_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(pr_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(pr_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "mm/day"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + prbox + " PRA of La Nina events composite during DJF; Nina = " +
                                        region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                        "map of " + prbox + " PRA of El Nino events composite during DJF; Nino = " +
                                        region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = pr_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = pr_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(prbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(prbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: prRmse,
                                             "metric_valueRMSE_error_" + dataset2: prRmseErr,
                                             "metric_valueCORR_" + dataset2: prCorr,
                                             "metric_valueCORR_error_" + dataset2: prCorrErr,
                                             "metric_valueSTD_" + dataset2: prStd,
                                             "metric_valueSTD_error_" + dataset2: prStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=pr_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=pr_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if prCorr is not None:
        prCorr = 1 - prCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(prRmse), "line2": "metric value_error: " + str(prRmseErr),
                      "line3": "metric value: " + str(prCorr), "line4": "metric value_error: " + str(prCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": prRmse, "Rmse__value_error": prRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": prCorr, "Corr__value_error": prCorrErr, "Corr__units": "", "Std__value": prStd,
        "Std__value_error": prStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoPrMapJja(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                 prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs,
                 sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs,
                 prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox,
                 event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                 netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrMapJja() function computes precipitation anomalies pattern associated with ENSO on the globe.
    It is the regression of 'prbox' prA (precipitation anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during JJA (usually the regression of global prA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param prbox: string
        name of box (e.g. 'global') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrMapJja_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO JJA PRA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "mm/day/C"
    Method = "map of " + prbox + " precipitation anomalies regressed onto " + region_ev + " averaged SSTA during JJA"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoPrMapJja"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitation", metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitation", metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod_box, pr_mod, keyerror_mod3 = CheckTime(sst_mod_box, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs_box, pr_obs, keyerror_obs3 = CheckTime(sst_obs_box, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod_box.shape[0] / 12))
    yearN_obs = int(round(sst_obs_box.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod_box)
    actualtimebounds_obs = TimeBounds(sst_obs_box)

    prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 PR in 'prbox' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        del pr_mod_areacell, pr_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "JJA", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "JJA", compute_anom=True)
            pr_mod = SeasonalMean(pr_mod, "JJA", compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, "JJA", compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=prbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod.shape), "shape2": "(pr obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            pr_mod_slope = LinearRegressionTsAgainstMap(pr_mod, enso_mod, return_stderr=False)
            pr_obs_slope = LinearRegressionTsAgainstMap(pr_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {
                    "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                    "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                    "shape1": "(pr mod) " + str(pr_mod_slope.shape), "shape2": "(pr obs) " + str(pr_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            pr_mod_slope, keyerror_mod = BasinMask(
                pr_mod_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            pr_obs_slope, keyerror_obs = BasinMask(
                pr_obs_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod_slope.shape),
                                  "shape2": "(pr obs) " + str(pr_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                prRmse, keyerror = RmsAxis(
                    pr_mod_slope, pr_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                prRmseErr = None
                # Metric 2
                prCorr = float(Correlation(pr_mod_slope, pr_obs_slope, axis="xy", centered=1, biased=1))
                prCorrErr = None
                # Metric 3
                std_mod = Std(pr_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(pr_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                prStd = float(std_mod) / float(std_obs)
                prStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    pr_mod_land, pr_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitation", metric, prbox, file_area=prareafilemod,
                        name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    pr_obs_land, pr_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitation", metric, prbox, file_area=prareafileobs,
                        name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, pr_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, pr_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, pr_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, pr_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        pr_mod_land, _, keyerror_mod = PreProcessTS(
                            pr_mod_land, "", areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
                        pr_obs_land, _, keyerror_obs = PreProcessTS(
                            pr_obs_land, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
                        del pr_mod_areacell, pr_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            pr_mod_land = SeasonalMean(pr_mod_land, "JJA", compute_anom=True)
                            pr_obs_land = SeasonalMean(pr_obs_land, "JJA", compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                pr_mod_land, pr_obs_land, _ = TwoVarRegrid(
                                    pr_mod_land, pr_obs_land, "", region=prbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                        "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                        "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                        "shape2": "(pr obs) " + str(pr_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            pr_mod_land_slope = LinearRegressionTsAgainstMap(pr_mod_land, enso_mod, return_stderr=False)
                            pr_obs_land_slope = LinearRegressionTsAgainstMap(pr_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land_slope.getAxisList()]),
                                    "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land_slope.getAxisList()]),
                                    "shape1": "(pr mod) " + str(pr_mod_land_slope.shape),
                                    "shape2": "(pr obs) " + str(pr_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(pr_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(pr_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(pr_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(pr_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(pr_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(pr_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(pr_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(pr_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "mm/day"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + prbox + " PRA of La Nina events composite during JJA (before " +
                                        "events); Nina = " + region_ev + " SSTA < -" + my_thresh + " during " +
                                        season_ev + enso_method,
                                        "map of " + prbox + " PRA of El Nino events composite during JJA (before " +
                                        "events); Nino = " + region_ev + " SSTA > " + my_thresh + " during " +
                                        season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = pr_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = pr_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(prbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(prbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: prRmse,
                                             "metric_valueRMSE_error_" + dataset2: prRmseErr,
                                             "metric_valueCORR_" + dataset2: prCorr,
                                             "metric_valueCORR_error_" + dataset2: prCorrErr,
                                             "metric_valueSTD_" + dataset2: prStd,
                                             "metric_valueSTD_error_" + dataset2: prStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=pr_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=pr_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if prCorr is not None:
        prCorr = 1 - prCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(prRmse), "line2": "metric value_error: " + str(prRmseErr),
                      "line3": "metric value: " + str(prCorr), "line4": "metric value_error: " + str(prCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": prRmse, "Rmse__value_error": prRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": prCorr, "Corr__value_error": prCorrErr, "Corr__units": "", "Std__value": prStd,
        "Std__value_error": prStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoPrDjfTel(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                 prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs,
                 sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs,
                 prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox,
                 event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                 netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrDjfTel() function computes precipitations anomalies associated with El Nino and La Nina events in many AR5
        reference regions, then precipitations during DJF preceding the events are composited for each selected event
        and the difference (El Nino PR - La Nina PR) is computed in each region.
    The first rmse(observations vs model) is the metric.
    The second metric is the number of regions where observations and models agree on the sign of the teleconnection

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param box: string
        name of box (e.g. 'nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrDjfTel_2'
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, Rmse__value (rms [NinoPr-NinaPr]), Rmse__value_error, Rmse__units, method,
        SignAgree__value (sign agreement [NinoPr-NinaPr]), SignAgree__value_error, SignAgree__units, nyears_model,
        nyears_observations, nina_model, nino_model, nina_observations, nino_observations, time_frequency,
        time_period_model, time_period_observations, ref, keyerror, dive_down_diag, units

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    threshold = event_definition["threshold"]
    normalize = event_definition["normalization"]
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Nino composite minus Nina composite during DJF preceeding the events in each region"
    Method = "Nino events = " + region_ev + " sstA > " + str(threshold) + ", Nina events = " + region_ev + " sstA < -" \
             + str(threshold) + " during " + season_ev + "; Precipitations associated with El Nino/La Nina events " + \
             " during the preceeding DJF are composited and the difference (El Nino PR - La Nina PR) is computed in" + \
             " each region"
    if kwargs["normalization"]:
        Units = ""
    else:
        Units = "mm/day"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoPrDjfTel"
    if metname == "":
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    if not isinstance(prbox, list):
        prbox = [prbox]
    prbox = sorted(prbox, key=str.lower)
    prmap_mod, _, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitation", metric, prbox[0], file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    prmap_obs, _, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitation", metric, prbox[0], file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, prmap_mod, keyerror_mod3 = CheckTime(sst_mod, prmap_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, prmap_obs, keyerror_obs3 = CheckTime(sst_obs, prmap_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        compRmse, compRmseErr, signAgreement, signAgreementErr = None, None, None, None
        nina_years_mod, nina_years_obs, nino_years_mod, nino_years_obs = None, None, None, None
        dive_down_diag = {"model": None, "observations": None, "axis": None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs:
            compRmse, compRmseErr, signAgreement, signAgreementErr = None, None, None, None
            nina_years_mod, nina_years_obs, nino_years_mod, nino_years_obs = None, None, None, None
            dive_down_diag = {"model": None, "observations": None, "axis": None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape),
                              "time1": "(mod) " + str(TimeBounds(sst_mod)),
                              "time2": "(obs) " + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # 1.2 SSTA < 'threshold' (SSTA > 'threshold') during 'season' are considered as La Nina (El Nino) events
            # Lists event years
            nina_years_mod = DetectEvents(sst_mod, season_ev, -threshold, normalization=normalize, nino=False)
            nino_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            nina_years_obs = DetectEvents(sst_obs, season_ev, -threshold, normalization=normalize, nino=False)
            nino_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {"nina1": "(mod) " + str(nina_years_mod), "nina2": "(obs) " + str(nina_years_obs),
                              "nino1": "(mod) " + str(nino_years_mod), "nino2": "(obs) " + str(nino_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

            # ------------------------------------------------
            # 2. compute composite
            # ------------------------------------------------
            # smoothing is not applied
            if "smoothing" in list(kwargs.keys()):
                smooth = deepcopy(kwargs["smoothing"])
                kwargs["smoothing"] = False
            else:
                smooth = False
            list_composite_mod, list_composite_obs = list(), list()
            loop_keyerror = None
            loop_box = list()
            for reg in prbox:
                if debug is True:
                    EnsoErrorsWarnings.debug_mode("\033[92m", "region = " + str(reg), 15)
                # Read if the given region is defined as a land region, an oceanic region, or both
                dict_reg = ReferenceRegions(reg)
                maskland = dict_reg["maskland"] if "maskland" in list(dict_reg.keys()) else False
                maskoce = dict_reg["maskocean"] if "maskocean" in list(dict_reg.keys()) else False
                pr_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                    prfilemod, prnamemod, "precipitation", metric, reg, file_area=prareafilemod,
                    name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod,
                    maskland=maskland, maskocean=maskoce, time_bounds=kwargs["time_bounds_mod"], debug=debug,
                    **kwargs)
                pr_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                    prfileobs, prnameobs, "precipitation", metric, reg, file_area=prareafileobs,
                    name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs,
                    maskland=maskland, maskocean=maskoce, time_bounds=kwargs["time_bounds_obs"], debug=debug,
                    **kwargs)
                if keyerror_mod is not None or keyerror_obs is not None:
                    loop_keyerror = add_up_errors([loop_keyerror, keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                      "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                      "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape),
                                      "time1": "(mod) " + str(TimeBounds(pr_mod)),
                                      "time2": "(obs) " + str(TimeBounds(pr_obs))}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after Read_data_mask_area", 20, **dict_debug)
                    # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                    pr_mod, Method, keyerror_mod = PreProcessTS(
                        pr_mod, Method, areacell=mod_areacell, average="horizontal", compute_anom=False, region=reg,
                        **kwargs)
                    pr_obs, _, keyerror_obs = PreProcessTS(
                        pr_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=reg,
                        **kwargs)
                    del mod_areacell, obs_areacell
                    if keyerror_mod is not None or keyerror_obs is not None:
                        loop_keyerror = add_up_errors([loop_keyerror, keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                          "shape1": "(mod) " + str(pr_mod.shape),
                                          "shape2": "(obs) " + str(pr_obs.shape),
                                          "time1": "(mod) " + str(TimeBounds(pr_mod)),
                                          "time2": "(obs) " + str(TimeBounds(pr_obs))}
                            EnsoErrorsWarnings.debug_mode(
                                "\033[92m", "after PreProcessTS " + str(reg), 20, **dict_debug)

                        # Seasonal mean
                        pr_mod = SeasonalMean(pr_mod, "DJF", compute_anom=False)
                        pr_obs = SeasonalMean(pr_obs, "DJF", compute_anom=False)

                        # composites
                        composite_nina_mod = Composite(pr_mod, nina_years_mod, kwargs["frequency"])
                        composite_nino_mod = Composite(pr_mod, nino_years_mod, kwargs["frequency"])
                        composite_nina_obs = Composite(pr_obs, nina_years_obs, kwargs["frequency"])
                        composite_nino_obs = Composite(pr_obs, nino_years_obs, kwargs["frequency"])

                        # list composites
                        list_composite_mod.append(float(composite_nino_mod - composite_nina_mod))
                        list_composite_obs.append(float(composite_nino_obs - composite_nina_obs))
                        loop_box.append(reg)
                        del composite_nina_mod, composite_nina_obs, composite_nino_mod, composite_nino_obs
                del dict_reg, keyerror_mod, keyerror_obs, maskland, maskoce, mod_areacell, obs_areacell, pr_mod, pr_obs

            # create arrays
            ar5 = "AR5 reference regions"
            ref = "https://www.ipcc-data.org/guidelines/pages/ar5_regions.html"
            list_composite_mod = ArrayListAx(
                list_composite_mod, loop_box, ax_name_ax="box", ax_long_name=ar5, ax_ref=ref)
            list_composite_obs = ArrayListAx(
                list_composite_obs, loop_box, ax_name_ax="box", ax_long_name=ar5, ax_ref=ref)

            if "smoothing" in list(kwargs.keys()):
                kwargs["smoothing"] = smooth
                del smooth

            # Computes the root mean square difference
            compRmse, keyerror = RmsAxis(
                list_composite_mod, list_composite_obs, centered=centered_rmse, biased=biased_rmse)
            compRmseErr = None

            # Computes the percentage of regions where observations and model agree on the sign of the teleconnection
            signAgreement = sum([1. for vmod, vobs in zip(list_composite_mod, list_composite_obs)
                                 if NUMPYsign(vmod) == NUMPYsign(vobs)]) / len(list_composite_mod)
            signAgreementErr = NUMPYsqrt(signAgreement * (1 - signAgreement) / len(list_composite_mod)) * 1.65

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(list_composite_mod), "observations": ArrayToList(list_composite_obs),
                              "axis": loop_box}
            if keyerror is not None or loop_keyerror is not None:
                keyerror = add_up_errors([keyerror, loop_keyerror])

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "nina_years": str(nina_years_mod), "nino_years": str(nino_years_mod)}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "nina_years": str(nina_years_obs), "nino_years": str(nino_years_obs)}
                dict3 = {
                    "metric_name": Name, "metric_valueRMSE_" + dataset2: compRmse,
                    "metric_valueRMSE_error_" + dataset2: compRmseErr,
                    "metric_valueSignAgree_" + dataset2: signAgreement,
                    "metric_valueSignAgree_error_" + dataset2: signAgreementErr, "metric_method": Method,
                    "metric_reference": Ref, "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, var1=list_composite_mod, var1_attributes=dict1,
                           var1_name="prComp_box__" + dataset1, var2=list_composite_obs, var2_attributes=dict2,
                           var2_name="prComp_box__" + dataset2, global_attributes=dict3)
                del dict1, dict2, dict3, file_name

    # Create output
    metric_output = {
        "name": Name, "Rmse__value": compRmse, "Rmse__value_error": signAgreement, "Rmse__units": Units,
        "method": Method, "SignAgree__value": signAgreement, "SignAgree__value_error": signAgreementErr,
        "SignAgree__units": "%", "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "nina_model": nina_years_mod, "nino_model": nino_years_mod, "nina_observations": nina_years_obs,
        "nino_observations": nino_years_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoPrJjaTel(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                 prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs,
                 sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs,
                 prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox,
                 event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                 netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrJjaTel() function computes precipitations anomalies associated with El Nino and La Nina events in many AR5
        reference regions, then precipitations during JJA preceding the events are composited for each selected event
        and the difference (El Nino PR - La Nina PR) is computed in each region.
    The first rmse(observations vs model) is the metric.
    The second metric is the number of regions where observations and models agree on the sign of the teleconnection

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param box: string
        name of box (e.g. 'nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrJjaTel_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, Rmse__value (rms [NinoPr-NinaPr]), Rmse__value_error, Rmse__units, method,
        SignAgree__value (sign agreement [NinoPr-NinaPr]), SignAgree__value_error, SignAgree__units, nyears_model,
        nyears_observations, nina_model, nino_model, nina_observations, nino_observations, time_frequency,
        time_period_model, time_period_observations, ref, keyerror, dive_down_diag, units

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    threshold = event_definition["threshold"]
    normalize = event_definition["normalization"]
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Nino composite minus Nina composite during JJA preceeding the events in each region"
    Method = "Nino events = " + region_ev + " sstA > " + str(threshold) + ", Nina events = " + region_ev + " sstA < -" \
             + str(threshold) + " during " + season_ev + "; Precipitations associated with El Nino/La Nina events " + \
             " during the preceeding JJA are composited and the difference (El Nino PR - La Nina PR) is computed in" + \
             " each region"
    if kwargs["normalization"]:
        Units = ""
    else:
        Units = "mm/day"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoPrJjaTel"
    if metname == "":
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    if not isinstance(prbox, list):
        prbox = [prbox]
    prbox = sorted(prbox, key=str.lower)
    prmap_mod, _, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitation", metric, prbox[0], file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    prmap_obs, _, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitation", metric, prbox[0], file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, prmap_mod, keyerror_mod3 = CheckTime(sst_mod, prmap_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, prmap_obs, keyerror_obs3 = CheckTime(sst_obs, prmap_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        compRmse, compRmseErr, signAgreement, signAgreementErr = None, None, None, None
        nina_years_mod, nina_years_obs, nino_years_mod, nino_years_obs = None, None, None, None
        dive_down_diag = {"model": None, "observations": None, "axis": None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs:
            compRmse, compRmseErr, signAgreement, signAgreementErr = None, None, None, None
            nina_years_mod, nina_years_obs, nino_years_mod, nino_years_obs = None, None, None, None
            dive_down_diag = {"model": None, "observations": None, "axis": None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape),
                              "time1": "(mod) " + str(TimeBounds(sst_mod)),
                              "time2": "(obs) " + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # 1.2 SSTA < 'threshold' (SSTA > 'threshold') during 'season' are considered as La Nina (El Nino) events
            # Lists event years
            nina_years_mod = DetectEvents(sst_mod, season_ev, -threshold, normalization=normalize, nino=False)
            nino_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            nina_years_obs = DetectEvents(sst_obs, season_ev, -threshold, normalization=normalize, nino=False)
            nino_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {"nina1": "(mod) " + str(nina_years_mod), "nina2": "(obs) " + str(nina_years_obs),
                              "nino1": "(mod) " + str(nino_years_mod), "nino2": "(obs) " + str(nino_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

            # ------------------------------------------------
            # 2. compute composite
            # ------------------------------------------------
            # smoothing is not applied
            if "smoothing" in list(kwargs.keys()):
                smooth = deepcopy(kwargs["smoothing"])
                kwargs["smoothing"] = False
            else:
                smooth = False
            list_composite_mod, list_composite_obs = list(), list()
            loop_keyerror = None
            loop_box = list()
            for reg in prbox:
                if debug is True:
                    EnsoErrorsWarnings.debug_mode("\033[92m", "region = " + str(reg), 15)
                # Read if the given region is defined as a land region, an oceanic region, or both
                dict_reg = ReferenceRegions(reg)
                maskland = dict_reg["maskland"] if "maskland" in list(dict_reg.keys()) else False
                maskoce = dict_reg["maskocean"] if "maskocean" in list(dict_reg.keys()) else False
                pr_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                    prfilemod, prnamemod, "precipitation", metric, reg, file_area=prareafilemod,
                    name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod,
                    maskland=maskland, maskocean=maskoce, time_bounds=kwargs["time_bounds_mod"], debug=debug,
                    **kwargs)
                pr_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                    prfileobs, prnameobs, "precipitation", metric, reg, file_area=prareafileobs,
                    name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs,
                    maskland=maskland, maskocean=maskoce, time_bounds=kwargs["time_bounds_obs"], debug=debug,
                    **kwargs)
                if keyerror_mod is not None or keyerror_obs is not None:
                    loop_keyerror = add_up_errors([loop_keyerror, keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                      "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                      "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape),
                                      "time1": "(mod) " + str(TimeBounds(pr_mod)),
                                      "time2": "(obs) " + str(TimeBounds(pr_obs))}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after Read_data_mask_area", 20, **dict_debug)
                    # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                    pr_mod, Method, keyerror_mod = PreProcessTS(
                        pr_mod, Method, areacell=mod_areacell, average="horizontal", compute_anom=False, region=reg,
                        **kwargs)
                    pr_obs, _, keyerror_obs = PreProcessTS(
                        pr_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=reg,
                        **kwargs)
                    del mod_areacell, obs_areacell
                    if keyerror_mod is not None or keyerror_obs is not None:
                        loop_keyerror = add_up_errors([loop_keyerror, keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                          "shape1": "(mod) " + str(pr_mod.shape),
                                          "shape2": "(obs) " + str(pr_obs.shape),
                                          "time1": "(mod) " + str(TimeBounds(pr_mod)),
                                          "time2": "(obs) " + str(TimeBounds(pr_obs))}
                            EnsoErrorsWarnings.debug_mode(
                                "\033[92m", "after PreProcessTS " + str(reg), 20, **dict_debug)

                        # Seasonal mean
                        pr_mod = SeasonalMean(pr_mod, "JJA", compute_anom=False)
                        pr_obs = SeasonalMean(pr_obs, "JJA", compute_anom=False)

                        # composites
                        composite_nina_mod = Composite(pr_mod, nina_years_mod, kwargs["frequency"])
                        composite_nino_mod = Composite(pr_mod, nino_years_mod, kwargs["frequency"])
                        composite_nina_obs = Composite(pr_obs, nina_years_obs, kwargs["frequency"])
                        composite_nino_obs = Composite(pr_obs, nino_years_obs, kwargs["frequency"])

                        # list composites
                        list_composite_mod.append(float(composite_nino_mod - composite_nina_mod))
                        list_composite_obs.append(float(composite_nino_obs - composite_nina_obs))
                        loop_box.append(reg)
                        del composite_nina_mod, composite_nina_obs, composite_nino_mod, composite_nino_obs
                del dict_reg, keyerror_mod, keyerror_obs, maskland, maskoce, mod_areacell, obs_areacell, pr_mod, pr_obs

            # create arrays
            ar5 = "AR5 reference regions"
            ref = "https://www.ipcc-data.org/guidelines/pages/ar5_regions.html"
            list_composite_mod = ArrayListAx(
                list_composite_mod, loop_box, ax_name_ax="box", ax_long_name=ar5, ax_ref=ref)
            list_composite_obs = ArrayListAx(
                list_composite_obs, loop_box, ax_name_ax="box", ax_long_name=ar5, ax_ref=ref)

            if "smoothing" in list(kwargs.keys()):
                kwargs["smoothing"] = smooth
                del smooth

            # Computes the root mean square difference
            compRmse, keyerror = RmsAxis(
                list_composite_mod, list_composite_obs, centered=centered_rmse, biased=biased_rmse)
            compRmseErr = None

            # Computes the percentage of regions where observations and model agree on the sign of the teleconnection
            signAgreement = sum([1. for vmod, vobs in zip(list_composite_mod, list_composite_obs)
                                 if NUMPYsign(vmod) == NUMPYsign(vobs)]) / len(list_composite_mod)
            signAgreementErr = NUMPYsqrt(signAgreement * (1 - signAgreement) / len(list_composite_mod)) * 1.65

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(list_composite_mod), "observations": ArrayToList(list_composite_obs),
                              "axis": loop_box}
            if keyerror is not None or loop_keyerror is not None:
                keyerror = add_up_errors([keyerror, loop_keyerror])

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "nina_years": str(nina_years_mod), "nino_years": str(nino_years_mod)}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "nina_years": str(nina_years_obs), "nino_years": str(nino_years_obs)}
                dict3 = {
                    "metric_name": Name, "metric_valueRMSE_" + dataset2: compRmse,
                    "metric_valueRMSE_error_" + dataset2: compRmseErr,
                    "metric_valueSignAgree_" + dataset2: signAgreement,
                    "metric_valueSignAgree_error_" + dataset2: signAgreementErr, "metric_method": Method,
                    "metric_reference": Ref, "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, var1=list_composite_mod, var1_attributes=dict1,
                           var1_name="prComp_box__" + dataset1, var2=list_composite_obs, var2_attributes=dict2,
                           var2_name="prComp_box__" + dataset2, global_attributes=dict3)
                del dict1, dict2, dict3, file_name

    # Create output
    metric_output = {
        "name": Name, "Rmse__value": compRmse, "Rmse__value_error": signAgreement, "Rmse__units": Units,
        "method": Method, "SignAgree__value": signAgreement, "SignAgree__value_error": signAgreementErr,
        "SignAgree__units": "%", "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "nina_model": nina_years_mod, "nino_model": nino_years_mod, "nina_observations": nina_years_obs,
        "nino_observations": nino_years_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSlpMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               slpfilemod, slpnamemod, slpareafilemod, slpareanamemod, slplandmaskfilemod, slplandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
               slpfileobs, slpnameobs, slpareafileobs, slpareanameobs, slplandmaskfileobs, slplandmasknameobs, sstbox,
               slpbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False,
               netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSlpMap() function computes sea level pressure anomalies pattern associated with ENSO on the globe.
    It is the regression of 'slpbox' slpA (sea level pressure anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during boreal winter (usually the regression of global slpA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param slpfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SLP
    :param slpnamemod: string
        name of SLP variable (psl) in 'slpfilemod'
    :param slpareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP areacell
    :param slpareanamemod: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafilemod'
    :param slplandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP landmask
    :param slplandmasknamemod: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param slpfileobs: string
        path_to/filename of the file (NetCDF) of the observed SLP
    :param slpnameobs: string
        name of SLP variable (slp) in 'slpfileobs'
    :param slpareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP areacell
    :param slpareanameobs: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafileobs'
    :param slplandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP landmask
    :param slplandmasknameobs: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param slpbox: string
        name of box (e.g. 'global') for SLP
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSlpMap_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO SLPA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "hPa/C"
    Method = "map of " + slpbox + " sea level pressure anomalies regressed onto " + region_ev + " averaged SSTA " + \
             "during " + season_ev + enso_method3
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSlpMap"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    slp_mod, slp_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod, name_area=slpareanamemod,
        file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    slp_obs, slp_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs, name_area=slpareanameobs,
        file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, slp_mod, keyerror_mod3 = CheckTime(sst_mod, slp_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, slp_obs, keyerror_obs3 = CheckTime(sst_obs, slp_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        # 1.2 SLP in 'slpbox' are normalized / detrended / smoothed (running average) if applicable
        slp_mod, Method, keyerror_mod2 = PreProcessTS(
            slp_mod, Method, areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
        slp_obs, _, keyerror_obs2 = PreProcessTS(
            slp_obs, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
        kwargs["smoothing"] = smooth
        del mod_areacell, obs_areacell, slp_mod_areacell, slp_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
            slp_mod = SeasonalMean(slp_mod, season_ev, compute_anom=True) * 1e-2
            slp_obs = SeasonalMean(slp_obs, season_ev, compute_anom=True) * 1e-2
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                slp_mod, slp_obs, Method = TwoVarRegrid(slp_mod, slp_obs, Method, region=slpbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                        "shape1": "(slp mod) " + str(slp_mod.shape), "shape2": "(slp obs) " + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            slp_mod_slope = LinearRegressionTsAgainstMap(slp_mod, enso_mod, return_stderr=False)
            slp_obs_slope = LinearRegressionTsAgainstMap(slp_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                              "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                              "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            slp_mod_slope, keyerror_mod = BasinMask(
                slp_mod_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            slp_obs_slope, keyerror_obs = BasinMask(
                slp_obs_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                                  "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                                  "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                                  "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                slpRmse, keyerror = RmsAxis(
                    slp_mod_slope, slp_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                slpRmseErr = None
                # Metric 2
                slpCorr = float(Correlation(slp_mod_slope, slp_obs_slope, axis="xy", centered=1, biased=1))
                slpCorrErr = None
                # Metric 3
                std_mod = Std(slp_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(slp_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                slpStd = float(std_mod) / float(std_obs)
                slpStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    slp_mod_land, slp_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod,
                        name_area=slpareanamemod, file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    slp_obs_land, slp_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs,
                        name_area=slpareanameobs, file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, slp_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, slp_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, slp_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, slp_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # slpocess
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        slp_mod_land, _, keyerror_mod = PreProcessTS(
                            slp_mod_land, "", areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
                        slp_obs_land, _, keyerror_obs = PreProcessTS(
                            slp_obs_land, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
                        kwargs["smoothing"] = smooth
                        del slp_mod_areacell, slp_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            slp_mod_land = SeasonalMean(slp_mod_land, season_ev, compute_anom=True) * 1e-2
                            slp_obs_land = SeasonalMean(slp_obs_land, season_ev, compute_anom=True) * 1e-2
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                slp_mod_land, slp_obs_land, _ = TwoVarRegrid(
                                    slp_mod_land, slp_obs_land, "", region=slpbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                        "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                        "shape2": "(slp obs) " + str(slp_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            slp_mod_land_slope = LinearRegressionTsAgainstMap(
                                slp_mod_land, enso_mod, return_stderr=False)
                            slp_obs_land_slope = LinearRegressionTsAgainstMap(
                                slp_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land_slope.getAxisList()]),
                                    "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land_slope.getAxisList()]),
                                    "shape1": "(slp mod) " + str(slp_mod_land_slope.shape),
                                    "shape2": "(slp obs) " + str(slp_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # samples
                            en_mod = Composite(slp_mod, en_years_mod, kwargs["frequency"])
                            ln_mod = Composite(slp_mod, ln_years_mod, kwargs["frequency"])
                            en_obs = Composite(slp_obs, en_years_obs, kwargs["frequency"])
                            ln_obs = Composite(slp_obs, ln_years_obs, kwargs["frequency"])
                            en_mod_land = Composite(slp_mod_land, en_years_mod, kwargs["frequency"])
                            ln_mod_land = Composite(slp_mod_land, ln_years_mod, kwargs["frequency"])
                            en_obs_land = Composite(slp_obs_land, en_years_obs, kwargs["frequency"])
                            ln_obs_land = Composite(slp_obs_land, ln_years_obs, kwargs["frequency"])
                            # mask Pacific
                            en_mod, keyerror_mod1 = BasinMask(
                                en_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_mod, keyerror_mod2 = BasinMask(
                                ln_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            en_obs, keyerror_obs1 = BasinMask(
                                en_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_obs, keyerror_obs2 = BasinMask(
                                ln_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                            if keyerror is None:
                                my_units = "" if kwargs["normalization"] is True else "hPa"
                                # Metrics ENSO events global
                                dict_metric, dict_nc = dict(), dict()
                                nbr = 3
                                my_de = [
                                    "map of " + slpbox + " SLPA of La Nina events composite during " + season_ev +
                                    "; Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev +
                                    enso_method,
                                    "map of " + slpbox + " SLPA of El Nino events composite during " + season_ev +
                                    "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev +
                                    enso_method]
                                for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                        zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                            [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=de)
                                    nbr += 2
                                list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                for ii, reg in enumerate(list_region):
                                    # select region
                                    dictreg = ReferenceRegions(reg)
                                    tmp1 = slp_mod_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    tmp2 = slp_obs_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, description=Method.split(", ")[0].replace(slpbox, reg))
                                    nbr += 2
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                [ln_years_obs, en_years_obs])):
                                        tmp1 = tab1(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = tab2(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg + "_" + evn,
                                            my_units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de.replace(slpbox, reg))
                                        nbr += 2
                                    del dictreg, tmp1, tmp2
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                         "description": Method.split(", ")[0]}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                         "description": Method.split(", ")[0]}
                                dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                         "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: slpRmse,
                                         "metric_valueRMSE_error_" + dataset2: slpRmseErr,
                                         "metric_valueCORR_" + dataset2: slpCorr,
                                         "metric_valueCORR_error_" + dataset2: slpCorrErr,
                                         "metric_valueSTD_" + dataset2: slpStd,
                                         "metric_valueSTD_error_" + dataset2: slpStdErr}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=slp_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=slp_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if slpCorr is not None:
        slpCorr = 1 - slpCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(slpRmse), "line2": "metric value_error: " + str(slpRmseErr),
                      "line3": "metric value: " + str(slpCorr), "line4": "metric value_error: " + str(slpCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": slpRmse, "Rmse__value_error": slpRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": slpCorr, "Corr__value_error": slpCorrErr, "Corr__units": "", "Std__value": slpStd,
        "Std__value_error": slpStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "ref": Ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSlpMapDjf(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  slpfilemod, slpnamemod, slpareafilemod, slpareanamemod, slplandmaskfilemod, slplandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  slpfileobs, slpnameobs, slpareafileobs, slpareanameobs, slplandmaskfileobs, slplandmasknameobs,
                  sstbox, slpbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                  debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSlpMapDjf() function computes sea level pressure anomalies pattern associated with ENSO on the globe.
    It is the regression of 'slpbox' slpA (sea level pressure anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during DJF (usually the regression of global slpA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param slpfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SLP
    :param slpnamemod: string
        name of SLP variable (slp) in 'slpfilemod'
    :param slpareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP areacell
    :param slpareanamemod: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafilemod'
    :param slplandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP landmask
    :param slplandmasknamemod: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param slpfileobs: string
        path_to/filename of the file (NetCDF) of the observed SLP
    :param slpnameobs: string
        name of SLP variable (slp) in 'slpfileobs'
    :param slpareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP areacell
    :param slpareanameobs: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafileobs'
    :param slplandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP landmask
    :param slplandmasknameobs: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param slpbox: string
        name of box (e.g. 'global') for SLP
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSlpMapDjf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO DJF SLPA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "hPa/C"
    Method = "map of " + slpbox + " sea level pressure anomalies regressed onto " + region_ev + \
             " averaged SSTA during DJF"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSlpMapDjf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    slp_mod, slp_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod, name_area=slpareanamemod,
        file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    slp_obs, slp_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs, name_area=slpareanameobs,
        file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod_box, slp_mod, keyerror_mod3 = CheckTime(sst_mod_box, slp_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs_box, slp_obs, keyerror_obs3 = CheckTime(sst_obs_box, slp_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod_box.shape[0] / 12))
    yearN_obs = int(round(sst_obs_box.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod_box)
    actualtimebounds_obs = TimeBounds(sst_obs_box)

    slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 SLP in 'slpbox' are normalized / detrended / smoothed (running average) if applicable
        slp_mod, Method, keyerror_mod2 = PreProcessTS(
            slp_mod, Method, areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
        slp_obs, _, keyerror_obs2 = PreProcessTS(
            slp_obs, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
        del slp_mod_areacell, slp_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "DJF", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "DJF", compute_anom=True)
            slp_mod = SeasonalMean(slp_mod, "DJF", compute_anom=True) * 1e-2
            slp_obs = SeasonalMean(slp_obs, "DJF", compute_anom=True) * 1e-2
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                slp_mod, slp_obs, Method = TwoVarRegrid(slp_mod, slp_obs, Method, region=slpbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                        "shape1": "(slp mod) " + str(slp_mod.shape), "shape2": "(slp obs) " + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            slp_mod_slope = LinearRegressionTsAgainstMap(slp_mod, enso_mod, return_stderr=False)
            slp_obs_slope = LinearRegressionTsAgainstMap(slp_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                              "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                              "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            slp_mod_slope, keyerror_mod = BasinMask(
                slp_mod_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            slp_obs_slope, keyerror_obs = BasinMask(
                slp_obs_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                                  "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                                  "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                                  "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                slpRmse, keyerror = RmsAxis(
                    slp_mod_slope, slp_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                slpRmseErr = None
                # Metric 2
                slpCorr = float(Correlation(slp_mod_slope, slp_obs_slope, axis="xy", centered=1, biased=1))
                slpCorrErr = None
                # Metric 3
                std_mod = Std(slp_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(slp_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                slpStd = float(std_mod) / float(std_obs)
                slpStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    slp_mod_land, slp_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod,
                        name_area=slpareanamemod, file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    slp_obs_land, slp_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs,
                        name_area=slpareanameobs, file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, slp_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, slp_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, slp_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, slp_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # slpocess
                        slp_mod_land, _, keyerror_mod = PreProcessTS(
                            slp_mod_land, "", areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
                        slp_obs_land, _, keyerror_obs = PreProcessTS(
                            slp_obs_land, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
                        del slp_mod_areacell, slp_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            slp_mod_land = SeasonalMean(slp_mod_land, "DJF", compute_anom=True) * 1e-2
                            slp_obs_land = SeasonalMean(slp_obs_land, "DJF", compute_anom=True) * 1e-2
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                slp_mod_land, slp_obs_land, _ = TwoVarRegrid(
                                    slp_mod_land, slp_obs_land, "", region=slpbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                        "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                        "shape2": "(slp obs) " + str(slp_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            slp_mod_land_slope = LinearRegressionTsAgainstMap(
                                slp_mod_land, enso_mod, return_stderr=False)
                            slp_obs_land_slope = LinearRegressionTsAgainstMap(
                                slp_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land_slope.getAxisList()]),
                                    "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land_slope.getAxisList()]),
                                    "shape1": "(slp mod) " + str(slp_mod_land_slope.shape),
                                    "shape2": "(slp obs) " + str(slp_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(slp_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(slp_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(slp_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(slp_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(slp_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(slp_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(slp_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(slp_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "hPa"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + slpbox + " SLPA of La Nina events composite during DJF; Nina = " +
                                        region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                        "map of " + slpbox + " SLPA of El Nino events composite during DJF; Nino = " +
                                        region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = slp_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = slp_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(slpbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(slpbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: slpRmse,
                                             "metric_valueRMSE_error_" + dataset2: slpRmseErr,
                                             "metric_valueCORR_" + dataset2: slpCorr,
                                             "metric_valueCORR_error_" + dataset2: slpCorrErr,
                                             "metric_valueSTD_" + dataset2: slpStd,
                                             "metric_valueSTD_error_" + dataset2: slpStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=slp_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=slp_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if slpCorr is not None:
        slpCorr = 1 - slpCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(slpRmse), "line2": "metric value_error: " + str(slpRmseErr),
                      "line3": "metric value: " + str(slpCorr), "line4": "metric value_error: " + str(slpCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": slpRmse, "Rmse__value_error": slpRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": slpCorr, "Corr__value_error": slpCorrErr, "Corr__units": "", "Std__value": slpStd,
        "Std__value_error": slpStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSlpMapJja(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  slpfilemod, slpnamemod, slpareafilemod, slpareanamemod, slplandmaskfilemod, slplandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  slpfileobs, slpnameobs, slpareafileobs, slpareanameobs, slplandmaskfileobs, slplandmasknameobs,
                  sstbox, slpbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                  debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSlpMapJja() function computes sea level pressure anomalies pattern associated with ENSO on the globe.
    It is the regression of 'slpbox' slpA (sea level pressure anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during JJA (usually the regression of global slpA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param slpfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SLP
    :param slpnamemod: string
        name of SLP variable (slp) in 'slpfilemod'
    :param slpareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP areacell
    :param slpareanamemod: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafilemod'
    :param slplandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP landmask
    :param slplandmasknamemod: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param slpfileobs: string
        path_to/filename of the file (NetCDF) of the observed SLP
    :param slpnameobs: string
        name of SLP variable (slp) in 'slpfileobs'
    :param slpareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP areacell
    :param slpareanameobs: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafileobs'
    :param slplandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP landmask
    :param slplandmasknameobs: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param slpbox: string
        name of box (e.g. 'global') for SLP
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSlpMapJja_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO JJA SLPA pattern"
    Units = "" if kwargs["normalization"] else "hPa/C"
    Method = "map of " + slpbox + " sea level pressure anomalies regressed onto " + region_ev + \
             " averaged SSTA during JJA"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSlpMapJja"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    slp_mod, slp_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod, name_area=slpareanamemod,
        file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    slp_obs, slp_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs, name_area=slpareanameobs,
        file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod_box, slp_mod, keyerror_mod3 = CheckTime(sst_mod_box, slp_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs_box, slp_obs, keyerror_obs3 = CheckTime(sst_obs_box, slp_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod_box.shape[0] / 12))
    yearN_obs = int(round(sst_obs_box.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod_box)
    actualtimebounds_obs = TimeBounds(sst_obs_box)

    slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 SLP in 'slpbox' are normalized / detrended / smoothed (running average) if applicable
        slp_mod, Method, keyerror_mod2 = PreProcessTS(
            slp_mod, Method, areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
        slp_obs, _, keyerror_obs2 = PreProcessTS(
            slp_obs, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
        del slp_mod_areacell, slp_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "JJA", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "JJA", compute_anom=True)
            slp_mod = SeasonalMean(slp_mod, "JJA", compute_anom=True) * 1e-2
            slp_obs = SeasonalMean(slp_obs, "JJA", compute_anom=True) * 1e-2
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                slp_mod, slp_obs, Method = TwoVarRegrid(slp_mod, slp_obs, Method, region=slpbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                        "shape1": "(slp mod) " + str(slp_mod.shape), "shape2": "(slp obs) " + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            slp_mod_slope = LinearRegressionTsAgainstMap(slp_mod, enso_mod, return_stderr=False)
            slp_obs_slope = LinearRegressionTsAgainstMap(slp_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                              "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                              "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            slp_mod_slope, keyerror_mod = BasinMask(
                slp_mod_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            slp_obs_slope, keyerror_obs = BasinMask(
                slp_obs_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                                  "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                                  "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                                  "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                slpRmse, keyerror = RmsAxis(
                    slp_mod_slope, slp_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                slpRmseErr = None
                # Metric 2
                slpCorr = float(Correlation(slp_mod_slope, slp_obs_slope, axis="xy", centered=1, biased=1))
                slpCorrErr = None
                # Metric 3
                std_mod = Std(slp_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(slp_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                slpStd = float(std_mod) / float(std_obs)
                slpStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    slp_mod_land, slp_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod,
                        name_area=slpareanamemod, file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    slp_obs_land, slp_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs,
                        name_area=slpareanameobs, file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, slp_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, slp_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, slp_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, slp_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # slpocess
                        slp_mod_land, _, keyerror_mod = PreProcessTS(
                            slp_mod_land, "", areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
                        slp_obs_land, _, keyerror_obs = PreProcessTS(
                            slp_obs_land, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
                        del slp_mod_areacell, slp_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            slp_mod_land = SeasonalMean(slp_mod_land, "JJA", compute_anom=True) * 1e-2
                            slp_obs_land = SeasonalMean(slp_obs_land, "JJA", compute_anom=True) * 1e-2
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                slp_mod_land, slp_obs_land, _ = TwoVarRegrid(
                                    slp_mod_land, slp_obs_land, "", region=slpbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                        "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                        "shape2": "(slp obs) " + str(slp_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            slp_mod_land_slope = LinearRegressionTsAgainstMap(
                                slp_mod_land, enso_mod, return_stderr=False)
                            slp_obs_land_slope = LinearRegressionTsAgainstMap(
                                slp_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land_slope.getAxisList()]),
                                    "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land_slope.getAxisList()]),
                                    "shape1": "(slp mod) " + str(slp_mod_land_slope.shape),
                                    "shape2": "(slp obs) " + str(slp_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(slp_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(slp_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(slp_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(slp_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(slp_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(slp_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(slp_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(slp_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "hPa"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + slpbox + " SLPA of La Nina events composite during JJA (before " +
                                        "events); Nina = " + region_ev + " SSTA < -" + my_thresh + " during " +
                                        season_ev + enso_method,
                                        "map of " + slpbox + " SLPA of El Nino events composite during JJA (before " +
                                        "events); Nino = " + region_ev + " SSTA > " + my_thresh + " during " +
                                        season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = slp_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = slp_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(slpbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(slpbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: slpRmse,
                                             "metric_valueRMSE_error_" + dataset2: slpRmseErr,
                                             "metric_valueCORR_" + dataset2: slpCorr,
                                             "metric_valueCORR_error_" + dataset2: slpCorrErr,
                                             "metric_valueSTD_" + dataset2: slpStd,
                                             "metric_valueSTD_error_" + dataset2: slpStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=slp_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=slp_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if slpCorr is not None:
        slpCorr = 1 - slpCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(slpRmse), "line2": "metric value_error: " + str(slpRmseErr),
                      "line3": "metric value: " + str(slpCorr), "line4": "metric value_error: " + str(slpCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": slpRmse, "Rmse__value_error": slpRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": slpCorr, "Corr__value_error": slpCorrErr, "Corr__units": "", "Std__value": slpStd,
        "Std__value_error": slpStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSstMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, tasbox,
               event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
               netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstMap() function computes surface temperature anomalies pattern associated with ENSO on the globe.
    It is the regression of 'tasbox' TASA (surface temperature anomalies) onto 'region_ev' averaged SSTA (sea surface
    temperature anomalies) during boreal winter (usually the regression of global TASA onto nino3.4 SSTA).
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in "sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param tasbox: string
        name of box (e.g. 'global') for TAS
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstMap_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return EnsoSstMapMetric: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO TASA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "C/C"
    Method = "map of " + tasbox + " surface temperature anomalies regressed onto " + region_ev + \
             " averaged SSTA during " + season_ev + enso_method3
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSstMap"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    tas_mod, tas_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tas_obs, tas_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tas_mod, keyerror_mod3 = CheckTime(sst_mod, tas_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tas_obs, keyerror_obs3 = CheckTime(sst_obs, tas_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    tasCorr, tasCorrErr, tasRmse, tasRmseErr, tasStd, tasStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        # 1.2 TAS in 'tasbox' are normalized / detrended / smoothed (running average) if applicable
        tas_mod, Method, keyerror_mod2 = PreProcessTS(
            tas_mod, Method, areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
        tas_obs, _, keyerror_obs2 = PreProcessTS(
            tas_obs, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
        kwargs["smoothing"] = smooth
        del mod_areacell, obs_areacell, tas_mod_areacell, tas_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
            tas_mod = SeasonalMean(tas_mod, season_ev, compute_anom=True)
            tas_obs = SeasonalMean(tas_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tas_mod, tas_obs, Method = TwoVarRegrid(
                    tas_mod, tas_obs, Method, region=tasbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                        "shape1": "(tas mod) " + str(tas_mod.shape), "shape2": "(tas obs) " + str(tas_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            tas_mod_slope = LinearRegressionTsAgainstMap(tas_mod, enso_mod, return_stderr=False)
            tas_obs_slope = LinearRegressionTsAgainstMap(tas_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                              "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                              "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            tas_mod_slope, keyerror_mod = BasinMask(
                tas_mod_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            tas_obs_slope, keyerror_obs = BasinMask(
                tas_obs_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                                  "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                                  "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                                  "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                tasRmse, keyerror = RmsAxis(
                    tas_mod_slope, tas_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                tasRmseErr = None
                # Metric 2
                tasCorr = float(Correlation(tas_mod_slope, tas_obs_slope, axis="xy", centered=1, biased=1))
                tasCorrErr = None
                # Metric 3
                std_mod = Std(tas_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(tas_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                tasStd = float(std_mod) / float(std_obs)
                tasStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    tas_mod_land, tas_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod,
                        name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    tas_obs_land, tas_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs,
                        name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, tas_mod_land, keyerror_mod2 = CheckTime(
                        sst_mod, tas_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, tas_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, tas_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        tas_mod_land, _, keyerror_mod = PreProcessTS(
                            tas_mod_land, "", areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
                        tas_obs_land, _, keyerror_obs = PreProcessTS(
                            tas_obs_land, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
                        kwargs["smoothing"] = smooth
                        del tas_mod_areacell, tas_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            tas_mod_land = SeasonalMean(tas_mod_land, season_ev, compute_anom=True)
                            tas_obs_land = SeasonalMean(tas_obs_land, season_ev, compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                tas_mod_land, ts_obs, _ = TwoVarRegrid(
                                    tas_mod_land, tas_obs_land, "", region=tasbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                        "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                        "shape2": "(tas obs) " + str(tas_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            tas_mod_land_slope = LinearRegressionTsAgainstMap(
                                tas_mod_land, enso_mod, return_stderr=False)
                            tas_obs_land_slope = LinearRegressionTsAgainstMap(
                                tas_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land_slope.getAxisList()]),
                                    "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land_slope.getAxisList()]),
                                    "shape1": "(tas mod) " + str(tas_mod_land_slope.shape),
                                    "shape2": "(tas obs) " + str(tas_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # samples
                            en_mod = Composite(tas_mod, en_years_mod, kwargs["frequency"])
                            ln_mod = Composite(tas_mod, ln_years_mod, kwargs["frequency"])
                            en_obs = Composite(tas_obs, en_years_obs, kwargs["frequency"])
                            ln_obs = Composite(tas_obs, ln_years_obs, kwargs["frequency"])
                            en_mod_land = Composite(tas_mod_land, en_years_mod, kwargs["frequency"])
                            ln_mod_land = Composite(tas_mod_land, ln_years_mod, kwargs["frequency"])
                            en_obs_land = Composite(tas_obs_land, en_years_obs, kwargs["frequency"])
                            ln_obs_land = Composite(tas_obs_land, ln_years_obs, kwargs["frequency"])
                            # mask Pacific
                            en_mod, keyerror_mod1 = BasinMask(
                                en_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_mod, keyerror_mod2 = BasinMask(
                                ln_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            en_obs, keyerror_obs1 = BasinMask(
                                en_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_obs, keyerror_obs2 = BasinMask(
                                ln_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                            if keyerror is None:
                                my_units = "" if kwargs["normalization"] is True else "C"
                                # Metrics ENSO events global
                                dict_metric, dict_nc = dict(), dict()
                                nbr = 3
                                my_de = [
                                    "map of " + tasbox + " TASA of La Nina events composite during " + season_ev +
                                    "; Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev +
                                    enso_method,
                                    "map of " + tasbox + " TASA of El Nino events composite during " + season_ev +
                                    "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev +
                                    enso_method]
                                for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                        zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                            [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=de)
                                    nbr += 2
                                list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                for ii, reg in enumerate(list_region):
                                    # select region
                                    dictreg = ReferenceRegions(reg)
                                    tmp1 = tas_mod_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    tmp2 = tas_obs_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, description=Method.split(", ")[0].replace(tasbox, reg))
                                    nbr += 2
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                [ln_years_obs, en_years_obs])):
                                        tmp1 = tab1(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = tab2(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg + "_" + evn,
                                            my_units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de.replace(tasbox, reg))
                                        nbr += 2
                                    del dictreg, tmp1, tmp2
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                         "description": Method.split(", ")[0]}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                         "description": Method.split(", ")[0]}
                                dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                         "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: tasRmse,
                                         "metric_valueRMSE_error_" + dataset2: tasRmseErr,
                                         "metric_valueCORR_" + dataset2: tasCorr,
                                         "metric_valueCORR_error_" + dataset2: tasCorrErr,
                                         "metric_valueSTD_" + dataset2: tasStd,
                                         "metric_valueSTD_error_" + dataset2: tasStdErr}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=tas_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=tas_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if tasCorr is not None:
        tasCorr = 1 - tasCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(tasRmse), "line2": "metric value_error: " + str(tasRmseErr),
                      "line3": "metric value: " + str(tasCorr), "line4": "metric value_error: " + str(tasCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": tasRmse, "Rmse__value_error": tasRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": tasCorr, "Corr__value_error": tasCorrErr, "Corr__units": "", "Std__value": tasStd,
        "Std__value_error": tasStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "ref": Ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSstMapDjf(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  tasbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False,
                  netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstMapDjf() function computes surface temperature anomalies pattern associated with ENSO on the globe.
    It is the regression of 'tasbox' TASA (surface temperature anomalies) onto 'region_ev' averaged SSTA (sea surface
    temperature anomalies) during DJF (usually the regression of global TASA onto nino3.4 SSTA).
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param tasbox: string
        name of box (e.g. 'global') for TAS
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstMapDjf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return EnsoSstMapMetric: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO DJF TASA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "C/C"
    Method = "map of " + tasbox + " surface temperature anomalies regressed onto " + region_ev + \
             " averaged SSTA during DJF"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSstMapDjf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    tas_mod, tas_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tas_obs, tas_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tas_mod, keyerror_mod3 = CheckTime(sst_mod_box, tas_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tas_obs, keyerror_obs3 = CheckTime(sst_obs_box, tas_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    tasCorr, tasCorrErr, tasRmse, tasRmseErr, tasStd, tasStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 TAS in 'tasbox' are normalized / detrended / smoothed (running average) if applicable
        tas_mod, Method, keyerror_mod2 = PreProcessTS(
            tas_mod, Method, areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
        tas_obs, _, keyerror_obs2 = PreProcessTS(
            tas_obs, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
        del tas_mod_areacell, tas_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "DJF", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "DJF", compute_anom=True)
            tas_mod = SeasonalMean(tas_mod, "DJF", compute_anom=True)
            tas_obs = SeasonalMean(tas_obs, "DJF", compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tas_mod, tas_obs, Method = TwoVarRegrid(tas_mod, tas_obs, Method, region=tasbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                                  "axes2": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                                  "shape1": "(tas mod) " + str(tas_mod.shape),
                                  "shape2": "(tas obs) " + str(tas_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            tas_mod_slope = LinearRegressionTsAgainstMap(tas_mod, enso_mod, return_stderr=False)
            tas_obs_slope = LinearRegressionTsAgainstMap(tas_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                              "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                              "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            tas_mod_slope, keyerror_mod = BasinMask(
                tas_mod_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            tas_obs_slope, keyerror_obs = BasinMask(
                tas_obs_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {
                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                        "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                        "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                tasRmse, keyerror = RmsAxis(
                    tas_mod_slope, tas_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                tasRmseErr = None
                # Metric 2
                tasCorr = float(Correlation(tas_mod_slope, tas_obs_slope, axis="xy", centered=1, biased=1))
                tasCorrErr = None
                # Metric 3
                std_mod = Std(tas_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(tas_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                tasStd = float(std_mod) / float(std_obs)
                tasStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    tas_mod_land, tas_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod,
                        name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    tas_obs_land, tas_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs,
                        name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, tas_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, tas_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, tas_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, tas_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        tas_mod_land, _, keyerror_mod = PreProcessTS(
                            tas_mod_land, "", areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
                        tas_obs_land, _, keyerror_obs = PreProcessTS(
                            tas_obs_land, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
                        del tas_mod_areacell, tas_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            tas_mod_land = SeasonalMean(tas_mod_land, "DJF", compute_anom=True)
                            tas_obs_land = SeasonalMean(tas_obs_land, "DJF", compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                tas_mod_land, tas_obs_land, _ = TwoVarRegrid(
                                    tas_mod_land, tas_obs_land, "", region=tasbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                        "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                        "shape2": "(tas obs) " + str(tas_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            tas_mod_land_slope = LinearRegressionTsAgainstMap(
                                tas_mod_land, enso_mod, return_stderr=False)
                            tas_obs_land_slope = LinearRegressionTsAgainstMap(
                                tas_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land_slope.getAxisList()]),
                                    "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land_slope.getAxisList()]),
                                    "shape1": "(tas mod) " + str(tas_mod_land_slope.shape),
                                    "shape2": "(tas obs) " + str(tas_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(tas_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(tas_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(tas_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(tas_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(tas_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(tas_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(tas_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(tas_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "C"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + tasbox + " TASA of La Nina events composite during DJF; Nina = " +
                                        region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                        "map of " + tasbox + " TASA of El Nino events composite during DJF; Nino = " +
                                        region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = tas_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = tas_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(tasbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(tasbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: tasRmse,
                                             "metric_valueRMSE_error_" + dataset2: tasRmseErr,
                                             "metric_valueCORR_" + dataset2: tasCorr,
                                             "metric_valueCORR_error_" + dataset2: tasCorrErr,
                                             "metric_valueSTD_" + dataset2: tasStd,
                                             "metric_valueSTD_error_" + dataset2: tasStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=tas_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=tas_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if tasCorr is not None:
        tasCorr = 1 - tasCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(tasRmse), "line2": "metric value_error: " + str(tasRmseErr),
                      "line3": "metric value: " + str(tasCorr), "line4": "metric value_error: " + str(tasCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": tasRmse, "Rmse__value_error": tasRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": tasCorr, "Corr__value_error": tasCorrErr, "Corr__units": "", "Std__value": tasStd,
        "Std__value_error": tasStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSstMapJja(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  tasbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False,
                  netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstMapJja() function computes surface temperature anomalies pattern associated with ENSO on the globe.
    It is the regression of 'tasbox' TASA (surface temperature anomalies) onto 'region_ev' averaged SSTA (sea surface
    temperature anomalies) during JJA (usually the regression of global TASA onto nino3.4 SSTA).
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param tasbox: string
        name of box (e.g. 'global') for TAS
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstMapJja_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return EnsoSstMapMetric: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO JJA TASA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "C/C"
    Method = "map of " + tasbox + " surface temperature anomalies regressed onto " + region_ev + \
             " averaged SSTA during JJA"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSstMapJja"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    tas_mod, tas_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tas_obs, tas_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tas_mod, keyerror_mod3 = CheckTime(sst_mod_box, tas_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tas_obs, keyerror_obs3 = CheckTime(sst_obs_box, tas_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    tasCorr, tasCorrErr, tasRmse, tasRmseErr, tasStd, tasStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 TAS in 'tasbox' are normalized / detrended / smoothed (running average) if applicable
        tas_mod, Method, keyerror_mod2 = PreProcessTS(
            tas_mod, Method, areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
        tas_obs, _, keyerror_obs2 = PreProcessTS(
            tas_obs, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
        del tas_mod_areacell, tas_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "JJA", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "JJA", compute_anom=True)
            tas_mod = SeasonalMean(tas_mod, "JJA", compute_anom=True)
            tas_obs = SeasonalMean(tas_obs, "JJA", compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tas_mod, tas_obs, Method = TwoVarRegrid(tas_mod, tas_obs, Method, region=tasbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                                  "axes2": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                                  "shape1": "(tas mod) " + str(tas_mod.shape),
                                  "shape2": "(tas obs) " + str(tas_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            tas_mod_slope = LinearRegressionTsAgainstMap(tas_mod, enso_mod, return_stderr=False)
            tas_obs_slope = LinearRegressionTsAgainstMap(tas_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                              "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                              "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            tas_mod_slope, keyerror_mod = BasinMask(
                tas_mod_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            tas_obs_slope, keyerror_obs = BasinMask(
                tas_obs_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {
                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                        "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                        "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                tasRmse, keyerror = RmsAxis(
                    tas_mod_slope, tas_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                tasRmseErr = None
                # Metric 2
                tasCorr = float(Correlation(tas_mod_slope, tas_obs_slope, axis="xy", centered=1, biased=1))
                tasCorrErr = None
                # Metric 3
                std_mod = Std(tas_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(tas_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                tasStd = float(std_mod) / float(std_obs)
                tasStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    tas_mod_land, tas_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod,
                        name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    tas_obs_land, tas_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs,
                        name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, tas_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, tas_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, tas_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, tas_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        tas_mod_land, _, keyerror_mod = PreProcessTS(
                            tas_mod_land, "", areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
                        tas_obs_land, _, keyerror_obs = PreProcessTS(
                            tas_obs_land, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
                        del tas_mod_areacell, tas_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            tas_mod_land = SeasonalMean(tas_mod_land, "JJA", compute_anom=True)
                            tas_obs_land = SeasonalMean(tas_obs_land, "JJA", compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                tas_mod_land, tas_obs_land, _ = TwoVarRegrid(
                                    tas_mod_land, tas_obs_land, "", region=tasbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                        "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                        "shape2": "(tas obs) " + str(tas_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            tas_mod_land_slope = LinearRegressionTsAgainstMap(
                                tas_mod_land, enso_mod, return_stderr=False)
                            tas_obs_land_slope = LinearRegressionTsAgainstMap(
                                tas_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land_slope.getAxisList()]),
                                    "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land_slope.getAxisList()]),
                                    "shape1": "(tas mod) " + str(tas_mod_land_slope.shape),
                                    "shape2": "(tas obs) " + str(tas_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(tas_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(tas_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(tas_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(tas_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(tas_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(tas_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(tas_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(tas_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "C"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + tasbox + " TASA of La Nina events composite during JJA (before " +
                                        "events); Nina = " + region_ev + " SSTA < -" + my_thresh + " during " +
                                        season_ev + enso_method,
                                        "map of " + tasbox + " TASA of El Nino events composite during JJA (before " +
                                        "events); Nino = " + region_ev + " SSTA > " + my_thresh + " during " +
                                        season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = tas_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = tas_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(tasbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(tasbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: tasRmse,
                                             "metric_valueRMSE_error_" + dataset2: tasRmseErr,
                                             "metric_valueCORR_" + dataset2: tasCorr,
                                             "metric_valueCORR_error_" + dataset2: tasCorrErr,
                                             "metric_valueSTD_" + dataset2: tasStd,
                                             "metric_valueSTD_error_" + dataset2: tasStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=tas_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=tas_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if tasCorr is not None:
        tasCorr = 1 - tasCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(tasRmse), "line2": "metric value_error: " + str(tasRmseErr),
                      "line3": "metric value: " + str(tasCorr), "line4": "metric value_error: " + str(tasCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": tasRmse, "Rmse__value_error": tasRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": tasCorr, "Corr__value_error": tasCorrErr, "Corr__units": "", "Std__value": tasStd,
        "Std__value_error": tasStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def NinaPrMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod, prfilemod,
              prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs, sstnameobs,
              sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs, prnameobs,
              prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox, event_definition,
              centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False, netcdf=False, netcdf_name='',
              metname='', **kwargs):
    """
    The NinaPrMap() function computes a precipitation anomalies composite of during the peak of La Nina events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA < 'threshold' during 'season' are considered as La Nina events
    Then the PRA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param prbox: string
        name of box (e.g. 'global') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaPrMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina PRA Composite'
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + \
             ', Nina PRA Composited'
    Units = '' if kwargs['normalization'] else 'mm/day'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinaPrMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, 'precipitation', metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, 'precipitation', metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, pr_mod, keyerror_mod3 = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, pr_obs, keyerror_obs3 = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, '', areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        del mod_areacell, obs_areacell, pr_mod_areacell, pr_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1 is not None) or \
                (keyerror_mod2 is not None or keyerror_obs2 is not None):
            prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod pr) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes4': '(obs pr) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod pr) ' + str(pr_mod.shape), 'shape4': '(obs pr) ' + str(pr_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod pr) ' + str(TimeBounds(pr_mod)), 'time4': '(obs pr) ' + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite PRA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            pr_mod = SeasonalMean(pr_mod, season_ev, compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(pr_mod)), 'time2': '(obs) ' + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=prbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            pr_mod = Composite(pr_mod, event_years_mod, kwargs['frequency'])
            pr_obs = Composite(pr_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            pr_mod, keyerror_mod = BasinMask(
                pr_mod, 'pacific', box=prbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            pr_obs, keyerror_obs = BasinMask(
                pr_obs, 'pacific', box=prbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                prRmse, keyerror = RmsAxis(pr_mod, pr_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                prRmseErr = None
                # Metric 2
                prCorr = float(Correlation(pr_mod, pr_obs, axis='xy', centered=1, biased=1))
                prCorrErr = None
                # Metric 3
                std_mod = Std(pr_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(pr_obs, weights=None, axis='xy', centered=1, biased=1)
                prStd = float(std_mod) / float(std_obs)
                prStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}

                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {
                        'units': Units, 'number_of_years_used': yearN_mod, 'time_period': str(actualtimebounds_mod),
                        'nina_years': str(event_years_mod)}
                    dict2 = {
                        'units': Units, 'number_of_years_used': yearN_obs, 'time_period': str(actualtimebounds_obs),
                        'nina_years': str(event_years_obs)}
                    dict3 = {
                        'metric_name': Name, 'metric_valueRMSE_' + dataset2: prRmse,
                        'metric_valueRMSE_error_' + dataset2: prRmseErr, 'metric_valueCORR_' + dataset2: prCorr,
                        'metric_valueCORR_error_' + dataset2: prCorrErr, 'metric_valueSTD_' + dataset2: prStd,
                        'metric_valueCORR_error_' + dataset2: prStdErr, 'metric_method': Method,
                        'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(
                        file_name, var1=pr_mod, var1_attributes=dict1, var1_name='prComp_map__' + dataset1, var2=pr_obs,
                        var2_attributes=dict2, var2_name='prComp_map__' + dataset2, global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if prCorr is not None:
        prCorr = 1 - prCorr
    # Create output
    NinaPrMapMetric = {
        'name': Name, 'Rmse__value': prRmse, 'Rmse__value_error': prRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': prCorr, 'Corr__value_error': prCorrErr, 'Corr__units': '', 'Std__value': prStd,
        'Std__value_error': prStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinaPrMapMetric


def NinaSstDiv(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
               dataset='', debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSstDiv() function computes a zonal composite of La Nina events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA < 'threshold' during 'season' are considered as La Nina events
        2.) diversity of the zonal location of the minimum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the minimum SSTA for each selected event
            2.3) compute the percentage of EP events (minimum SSTA eastward of the given threshold)

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param treshold_ep_ev: float, optional
        see EnsoToolsLib.percentage_val_eastward
        longitude, in degree east, of the westward boundary of eastern Pacific event
        default value is -140°E (i.e., 140°W)
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaDivMetric: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, ref, keyerror,
        dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'treshold_ep_ev',
                    'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina Diversity (percentage of eastern Pacific La Nina)'
    lat = ReferenceRegions(box)['latitude']
    lon = ReferenceRegions(box)['longitude']
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' + \
             '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']), westward boundary of EP events' + \
             str(kwargs['treshold_ep_ev']) + 'E'
    Units = '%'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinaSstDiv'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst, areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    if keyerror is not None:
        ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, _, keyerror = PreProcessTS(
            sst, '', areacell=areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del areacell
        if keyerror is not None:
            ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
        else:
            if debug is True:
                dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                              'shape1': '(sst) ' + str(sst.shape), 'time1': '(sst) ' + str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years = DetectEvents(sst, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': 'nbr(' + str(len(event_years)) + '): ' + str(event_years)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the minimum SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst, areacell, keyerror = Read_data_mask_area(
                sstfile, sstname, 'temperature', metric, box, file_area=sstareafile, name_area=sstareaname,
                file_mask=sstlandmaskfile, name_mask=sstlandmaskfile, maskland=True, maskocean=False, debug=debug,
                **kwargs)
            if keyerror is not None:
                ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst, Method, keyerror = PreProcessTS(
                    sst, Method, areacell=areacell, average=False, compute_anom=False, region=box, **kwargs)
                del areacell
                if keyerror is not None:
                    ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                      'shape1': '(sst) ' + str(sst.shape), 'time1': '(sst) ' + str(TimeBounds(sst))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst = SeasonalMean(sst, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                      'shape1': '(sst) ' + str(sst.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder', 'regridTool',
                                      'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst = Regrid(sst, None, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                          'shape1': '(sst) ' + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst, keyerror = AverageMeridional(sst)
                    if keyerror is not None:
                        ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                          'shape1': '(sst) ' + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sample = Event_selection(sst, kwargs['frequency'], list_event_years=event_years)

                        # 2.2 find the zonal position of the minimum SSTA for each selected event
                        lon_sstmax = FindXYMinMaxInTs(
                            sample, return_val='mini', smooth=True, axis=0, window=5, method='triangle')
                        if debug is True:
                            dict_debug = {'line1': 'longitude of the minimum SSTA: ' + str(lon_sstmax)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)

                        # 2.3 compute the percentage of EP events (minimum SSTA eastward of the given threshold)
                        ep_event, keyerror = percentage_val_eastward(
                            lon_sstmax, metric, box, threshold=kwargs['treshold_ep_ev'])
                        ep_event = float(ep_event)

                        if keyerror is not None:
                            StdErr, dive_down_diag = None, {'value': None, 'axis': None}
                        else:
                            # Standard Error of the Standard Deviation (function of nyears)
                            StdErr = None

                            # Dive down diagnostic
                            dive_down_diag = {'value': ArrayToList(lon_sstmax), 'axis': list(lon_sstmax.getAxis(0)[:])}

                            if netcdf is True:
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {'units': 'longitude (E)', 'number_of_years_used': yearN,
                                         'time_period': str(actualtimebounds), 'nina_years': str(event_years),
                                         'diagnostic_value_' + dataset: ep_event,
                                         'diagnostic_value_error_' + dataset: StdErr}
                                dict2 = {'metric_name': Name, 'metric_method': Method, 'metric_reference': Ref,
                                         'frequency': kwargs['frequency']}
                                SaveNetcdf(file_name, var1=lon_sstmax, var1_attributes=dict1,
                                           var1_name='Nina_lon_pos_minSSTA__' + dataset, global_attributes=dict2)
                                del dict1, dict2
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(ep_event), 'line2': 'metric value_error: ' + str(StdErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinaDivMetric = {
        'name': Name, 'value': ep_event, 'value_error': StdErr, 'units': Units, 'method': Method, 'nyears': yearN,
        'events': event_years, 'time_frequency': kwargs['frequency'], 'time_period': actualtimebounds, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinaDivMetric


def NinaSstDivRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
                   netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSstDivRmse() function computes a zonal minimum of La Nina events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA < 'threshold' during 'season' are considered as La Nina events
        2.) diversity of the zonal location of the minimum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the minimum SSTA for each selected event and compute a pdf

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaDivMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyword, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'PDF of zonal min(SSTA) during La Nina'
    lat = ReferenceRegions(box)['latitude']
    lon = ReferenceRegions(box)['longitude']
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' \
             + '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']'
    Units = 'density'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinaSstDivRmse'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axis': None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                              'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the minimum SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, 'temperature', metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
                file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
            sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, 'temperature', metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
                file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
            if keyerror_mod is not None or keyerror_obs is not None:
                pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst_mod, Method, keyerror_mod = PreProcessTS(
                    sst_mod, Method, areacell=mod_areacell, average=False, compute_anom=False, region=box, **kwargs)
                sst_obs, _, keyerror_obs = PreProcessTS(
                    sst_obs, '', areacell=obs_areacell, average=False, compute_anom=False, region=box, **kwargs)
                del mod_areacell, obs_areacell
                if keyerror_mod is not None or keyerror_obs is not None:
                    pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                    dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                                      'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                                      'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
                    sst_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name',
                                      'regridder', 'regridTool', 'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst_mod, sst_obs, Method = TwoVarRegrid(
                            sst_mod, sst_obs, Method, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst_mod, keyerror_mod = AverageMeridional(sst_mod)
                    sst_obs, keyerror_obs = AverageMeridional(sst_obs)
                    if keyerror_mod is not None or keyerror_obs is not None:
                        pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sample_mod = Event_selection(sst_mod, kwargs['frequency'], list_event_years=event_years_mod)
                        sample_obs = Event_selection(sst_obs, kwargs['frequency'], list_event_years=event_years_obs)

                        # 2.2 find the zonal position of the minimum SSTA for each selected event and compute a pdf
                        # longitude of the minimum SSTA for each selected event
                        lon_min_mod = FindXYMinMaxInTs(
                            sample_mod, return_val='mini', smooth=True, axis=0, window=5, method='triangle')
                        lon_min_obs = FindXYMinMaxInTs(
                            sample_obs, return_val='mini', smooth=True, axis=0, window=5, method='triangle')
                        if debug is True:
                            dict_debug = {'line1': '(mod) longitude  of the minimum SSTA: ' + str(lon_min_mod),
                                          'line2': '(obs) longitude  of the minimum SSTA: ' + str(lon_min_obs)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)

                        # compute PDFs
                        if debug is True:
                            dict_debug = {'line1': 'lon ' + str(lon) + '  ;  nbr_bins old = ' +
                                                   str((lon[1] - lon[0]) / 10) + '  ;  nbr_bins new = ' +
                                                   str(int(round((lon[1] - lon[0]) / 10)))}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'before ComputePDF', 15, **dict_debug)
                        pdf_mod = ComputePDF(lon_min_mod, nbr_bins=int(round((lon[1] - lon[0]) / 10)), interval=lon,
                                             axis_name='longitude')
                        pdf_obs = ComputePDF(lon_min_obs, nbr_bins=int(round((lon[1] - lon[0]) / 10)), interval=lon,
                                             axis_name='longitude')

                        # Computes the root mean square difference
                        pdfRmse, keyerror = RmsZonal(pdf_mod, pdf_obs, centered=centered_rmse, biased=biased_rmse)

                        # Error on the metric
                        pdfRmseErr = None

                        # Dive down diagnostic
                        dive_down_diag = {'model': ArrayToList(pdf_mod), 'observations': ArrayToList(pdf_obs),
                                          'axis': list(pdf_mod.getAxis(0)[:])}
                        if netcdf is True:
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod), 'nina_years': str(event_years_mod)}
                            dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs), 'nina_years': str(event_years_obs)}
                            dict3 = {'metric_name': Name, 'metric_value_' + dataset2: pdfRmse,
                                     'metric_value_error_' + dataset2: pdfRmseErr, 'metric_method': Method,
                                     'metric_reference': Ref, 'frequency': kwargs['frequency']}
                            SaveNetcdf(file_name, var1=pdf_mod, var1_attributes=dict1, var1_name='pdf__' + dataset1,
                                       var2=pdf_obs, var2_attributes=dict2, var2_name='pdf__' + dataset2,
                                       global_attributes=dict3)
                            del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(pdfRmse), 'line2': 'metric value_error: ' + str(pdfRmseErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinaDivMetric = {
        'name': Name, 'value': pdfRmse, 'value_error': pdfRmseErr, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'events_model': event_years_mod,
        'events_observations': event_years_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinaDivMetric


def NinaSstDur(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
               nbr_years_window, dataset='', debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSstDurRmse() function computes a duration of La Nina events.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA < 'threshold' during 'season' are considered as La Nina events
        2.) La Nina duration
            2.1) get a time series of 2 years before and 2 years after the La Nina peak (4 years time series)
            2.2) count the number of consecutive month below a threshold

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaDurMetric: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, time_period, ref,
        keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina Duration'
    Units = 'months'
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + \
             ', number of consecutive months when sstA < -0.5' + Units
    Ref = 'Using CDAT'
    metric = 'NinaSstDur'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst, sst_areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    if keyerror is not None:
        duration_mean, duration_err, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, Method, keyerror = PreProcessTS(
            sst, Method, areacell=sst_areacell, average='horizontal', compute_anom=True, region=region_ev, **kwargs)
        del sst_areacell
        if keyerror is not None:
            duration_mean, duration_err, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
        else:
            if debug is True:
                dict_debug = {'axes1': str([ax.id for ax in sst.getAxisList()]), 'shape1': str(sst.shape),
                              'time1': str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years = DetectEvents(sst, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': str(event_years)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. La Nina duration
            # ------------------------------------------------
            # 2.1 get a time series of 2 years before and 2 years after the La Nina peak (4 years time series)
            # composites
            sample = Event_selection(
                sst, kwargs['frequency'], nbr_years_window=nbr_years_window, list_event_years=event_years)
            if debug is True:
                dict_debug = {'axes1': str([ax.id for ax in sample.getAxisList()]), 'shape1': str(sample.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Event_selection', 15, **dict_debug)

            # 2.2 count the number of consecutive month below a threshold
            if normalize is True:
                duration = DurationAllEvent(sample, -0.5 * float(Std(sst)), nino=False, debug=debug)
            else:
                duration = DurationAllEvent(sample, -0.5, nino=False, debug=debug)

            duration_err = float(Std(duration) / NUMPYsqrt(len(duration)))
            duration_mean = float(duration.mean())

            # Dive down diagnostic
            dive_down_diag = {'value': ArrayToList(duration), 'axis': list(duration.getAxis(0)[:])}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {'units': Units, 'number_of_years_used': yearN, 'time_period': str(actualtimebounds),
                         'nina_years': str(event_years), 'description': "La duration of Nina events",
                         'diagnostic_value': duration_mean, 'diagnostic_value_error': duration_err}
                dict2 = {'metric_name': Name, 'metric_method': Method, 'metric_reference': Ref,
                         'frequency': kwargs['frequency']}
                SaveNetcdf(file_name, var1=duration, var1_attributes=dict1, var1_name='Nina_duration__' + dataset,
                           global_attributes=dict2)
                del dict1, dict2, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(duration_mean),
                      'line2': 'metric value_error: ' + str(duration_err)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinaDurMetric = {
        'name': Name, 'value': duration_mean, 'value_error': duration_err, 'units': Units, 'method': Method,
        'nyears': yearN, 'events': event_years, 'time_frequency': kwargs['frequency'], 'time_period': actualtimebounds,
        'ref': Ref, 'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinaDurMetric


def NinaSstLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
                   netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSstLonRmse() function computes a zonal composite of La Nina events during the peak of the event
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA < 'threshold' during 'season' are considered as La Nina events
    Then the zonal SSTA at the peak of the event is composited for each selected event

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaLonMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina Zonal Composite'
    lat = ReferenceRegions(box)['latitude']
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' \
             + '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']'
    Units = '' if kwargs['normalization'] else 'C'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinaSstLonRmse'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axis': None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                              'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. zonal composite of SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, 'temperature', metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
                file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
            sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, 'temperature', metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
                file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
            if keyerror_mod is not None or keyerror_obs is not None:
                compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst_mod, Method, keyerror_mod = PreProcessTS(
                    sst_mod, Method, areacell=mod_areacell, average=False, compute_anom=False, region=box, **kwargs)
                sst_obs, _, keyerror_obs = PreProcessTS(
                    sst_obs, '', areacell=obs_areacell, average=False, compute_anom=False, region=box, **kwargs)
                del mod_areacell, obs_areacell
                if keyerror_mod is not None or keyerror_obs is not None:
                    compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                    dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                                      'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                                      'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
                    sst_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name',
                                      'regridder', 'regridTool', 'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst_mod, sst_obs, Method = TwoVarRegrid(
                            sst_mod, sst_obs, Method, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst_mod, keyerror_mod = AverageMeridional(sst_mod)
                    sst_obs, keyerror_obs = AverageMeridional(sst_obs)
                    if keyerror_mod is not None or keyerror_obs is not None:
                        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sst_mod = Composite(sst_mod, event_years_mod, kwargs['frequency'])
                        sst_obs = Composite(sst_obs, event_years_obs, kwargs['frequency'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

                        # Computes the root mean square difference
                        compRmse, keyerror = RmsZonal(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)

                        # Error on the metric
                        compRmseErr = None

                        # Dive down diagnostic
                        dive_down_diag = {'model': ArrayToList(sst_mod), 'observations': ArrayToList(sst_obs),
                                          'axis': list(sst_mod.getAxis(0)[:])}
                        if netcdf is True:
                            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                                sstfilemod, sstnamemod, 'temperature', metric, 'equatorial_pacific_LatExt2',
                                file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                                name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
                                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
                            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                                sstfileobs, sstnameobs, 'temperature', metric, 'equatorial_pacific_LatExt2',
                                file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                                name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
                                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
                            if keyerror_mod is not None or keyerror_obs is not None:
                                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            else:
                                map_mod, _, keyerror_mod = PreProcessTS(
                                    map_mod, '', areacell=mod_areacell, average=False, compute_anom=False,
                                    region="equatorial_pacific_LatExt2", **kwargs)
                                map_obs, _, keyerror_obs = PreProcessTS(
                                    map_obs, '', areacell=obs_areacell, average=False, compute_anom=False,
                                    region="equatorial_pacific_LatExt2", **kwargs)
                                del mod_areacell, obs_areacell
                                if keyerror_mod is not None or keyerror_obs is not None:
                                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                                else:
                                    if debug is True:
                                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in map_mod.getAxisList()]),
                                                      'axes2': '(obs) ' + str([ax.id for ax in map_obs.getAxisList()]),
                                                      'shape1': '(mod) ' + str(map_mod.shape),
                                                      'shape2': '(obs) ' + str(map_obs.shape),
                                                      'time1': '(mod) ' + str(TimeBounds(map_mod)),
                                                      'time2': '(obs) ' + str(TimeBounds(map_obs))}
                                        EnsoErrorsWarnings.debug_mode(
                                            '\033[92m', 'after PreProcessTS: netcdf', 15, **dict_debug)
                                    # Seasonal mean
                                    map_mod = SeasonalMean(map_mod, season_ev, compute_anom=True)
                                    map_obs = SeasonalMean(map_obs, season_ev, compute_anom=True)
                                    # Regridding
                                    if isinstance(kwargs['regridding'], dict):
                                        map_mod, map_obs, _ = TwoVarRegrid(
                                            map_mod, map_obs, '', region='equatorial_pacific_LatExt2',
                                            **kwargs['regridding'])
                                        if debug is True:
                                            dict_debug = {
                                                'axes1': '(mod) ' + str([ax.id for ax in map_mod.getAxisList()]),
                                                'axes2': '(obs) ' + str([ax.id for ax in map_obs.getAxisList()]),
                                                'shape1': '(mod) ' + str(map_mod.shape),
                                                'shape2': '(obs) ' + str(map_obs.shape)}
                                            EnsoErrorsWarnings.debug_mode(
                                                '\033[92m', 'after TwoVarRegrid: netcdf', 15, **dict_debug)
                                    # samples
                                    map_mod = Composite(map_mod, event_years_mod, kwargs['frequency'])
                                    map_obs = Composite(map_obs, event_years_obs, kwargs['frequency'])
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                             'time_period': str(actualtimebounds_mod),
                                             'nina_years': str(event_years_mod)}
                                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                             'time_period': str(actualtimebounds_obs),
                                             'nina_years': str(event_years_obs)}
                                    dict3 = {'units': Units, 'number_of_years_used': yearN_mod,
                                             'time_period': str(actualtimebounds_mod),
                                             'nina_years': str(event_years_mod)}
                                    dict4 = {'units': Units, 'number_of_years_used': yearN_obs,
                                             'time_period': str(actualtimebounds_obs),
                                             'nina_years': str(event_years_obs)}
                                    dict5 = {'metric_name': Name, 'metric_value_' + dataset2: compRmse,
                                             'metric_value_error_' + dataset2: compRmseErr, 'metric_method': Method,
                                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                                    SaveNetcdf(
                                        file_name, var1=sst_mod, var1_attributes=dict1,
                                        var1_name='sst_lon__' + dataset1, var2=sst_obs, var2_attributes=dict2,
                                        var2_name='sst_lon__' + dataset2, var3=map_mod, var3_attributes=dict3,
                                        var3_name='sst_map__' + dataset1, var4=map_obs, var4_attributes=dict4,
                                        var4_name='sst_map__' + dataset2, global_attributes=dict5)
                                    del dict1, dict2, dict3, dict4, dict5, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(compRmse), 'line2': 'metric value_error: ' + str(compRmseErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinaLonMetric = {
        'name': Name, 'value': compRmse, 'value_error': compRmseErr, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'events_model': event_years_mod,
        'events_observations': event_years_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinaLonMetric


def NinaSlpMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               slpfilemod, slpnamemod, slpareafilemod, slpareanamemod, slplandmaskfilemod, slplandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
               slpfileobs, slpnameobs, slpareafileobs, slpareanameobs, slplandmaskfileobs, slplandmasknameobs, sstbox,
               slpbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
               netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSlpMap() function computes a sea level pressure anomalies composite of during the peak of La Nina events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA < 'threshold' during 'season' are considered as La Nina events
    Then the SLPA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param slpfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SLP
    :param slpnamemod: string
        name of SLP variable (slp) in 'slpfilemod'
    :param slpareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP areacell
    :param slpareanamemod: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafilemod'
    :param slplandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP landmask
    :param slplandmasknamemod: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param slpfileobs: string
        path_to/filename of the file (NetCDF) of the observed SLP
    :param slpnameobs: string
        name of SLP variable (slp) in 'slpfileobs'
    :param slpareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP areacell
    :param slpareanameobs: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafileobs'
    :param slplandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP landmask
    :param slplandmasknameobs: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param slpbox: string
        name of box (e.g. 'global') for SLP
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaSlpMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina SLPA Composite'
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + \
             ', Nina SLPA Composited'
    Units = '' if kwargs['normalization'] else 'hPa'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinaSlpMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    slp_mod, slp_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        slpfilemod, slpnamemod, 'pressure', metric, slpbox, file_area=slpareafilemod, name_area=slpareanamemod,
        file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    slp_obs, slp_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        slpfileobs, slpnameobs, 'pressure', metric, slpbox, file_area=slpareafileobs, name_area=slpareanameobs,
        file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, slp_mod, keyerror_mod3 = CheckTime(sst_mod, slp_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, slp_obs, keyerror_obs3 = CheckTime(sst_obs, slp_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        slp_mod, Method, keyerror_mod2 = PreProcessTS(
            slp_mod, Method, areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
        slp_obs, _, keyerror_obs2 = PreProcessTS(
            slp_obs, '', areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
        del mod_areacell, obs_areacell, slp_mod_areacell, slp_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1) or \
                (keyerror_mod2 is not None or keyerror_obs2):
            slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                    'axes4': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod slp) ' + str(slp_mod.shape), 'shape4': '(obs slp) ' + str(slp_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod slp) ' + str(TimeBounds(slp_mod)), 'time4': '(obs slp) ' + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite SLPA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            slp_mod = SeasonalMean(slp_mod, season_ev, compute_anom=True) * 1e-2
            slp_obs = SeasonalMean(slp_obs, season_ev, compute_anom=True) * 1e-2
            if debug is True:
                dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                              'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                              'shape1': '(mod slp) ' + str(slp_mod.shape), 'shape2': '(obs slp) ' + str(slp_obs.shape),
                              'time1': '(mod slp) ' + str(TimeBounds(slp_mod)),
                              'time2': '(obs slp) ' + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                slp_mod, slp_obs, Method = TwoVarRegrid(slp_mod, slp_obs, Method, region=slpbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                                  'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                                  'shape1': '(mod slp) ' + str(slp_mod.shape),
                                  'shape2': '(obs slp) ' + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            slp_mod = Composite(slp_mod, event_years_mod, kwargs['frequency'])
            slp_obs = Composite(slp_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                              'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                              'shape1': '(mod slp) ' + str(slp_mod.shape), 'shape2': '(obs slp) ' + str(slp_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            slp_mod, keyerror_mod = BasinMask(
                slp_mod, 'pacific', box=slpbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            slp_obs, keyerror_obs = BasinMask(
                slp_obs, 'pacific', box=slpbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(slp_mod.shape), 'shape2': '(obs) ' + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                slpRmse, keyerror = RmsAxis(slp_mod, slp_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                slpRmseErr = None
                # Metric 2
                slpCorr = float(Correlation(slp_mod, slp_obs, axis='xy', centered=1, biased=1))
                slpCorrErr = None
                # Metric 3
                std_mod = Std(slp_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(slp_obs, weights=None, axis='xy', centered=1, biased=1)
                slpStd = float(std_mod) / float(std_obs)
                slpStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                             'time_period': str(actualtimebounds_mod),
                             'nina_years': str(event_years_mod)}
                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                             'time_period': str(actualtimebounds_obs),
                             'nina_years': str(event_years_obs)}
                    dict3 = {'metric_name': Name, 'metric_valueRMSE_' + dataset2: slpRmse,
                             'metric_valueRMSE_error_' + dataset2: slpRmseErr, 'metric_valueCORR_' + dataset2: slpCorr,
                             'metric_valueCORR_error_' + dataset2: slpCorrErr, 'metric_valueSTD_' + dataset2: slpStd,
                             'metric_valueCORR_error_' + dataset2: slpStdErr, 'metric_method': Method,
                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(file_name, var1=slp_mod, var1_attributes=dict1, var1_name='slp_map__' + dataset1,
                               var2=slp_obs, var2_attributes=dict2, var2_name='slp_map__' + dataset2,
                               global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if slpCorr is not None:
        slpCorr = 1 - slpCorr
    # Create output
    NinaSlpMapMetric = {
        'name': Name, 'Rmse__value': slpRmse, 'Rmse__value_error': slpRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': slpCorr, 'Corr__value_error': slpCorrErr, 'Corr__units': '', 'Std__value': slpStd,
        'Std__value_error': slpStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinaSlpMapMetric


def NinaSstMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, tsbox,
               event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False, netcdf=False,
               netcdf_name='', metname='', **kwargs):
    """
    The NinaSstMap() function computes a surface temperature anomalies composite of during the peak of La Nina events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA < 'threshold' during 'season' are considered as La Nina events
    Then the TSA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaSstMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina TSA Composite'
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + \
             ', Nina TSA Composited'
    Units = '' if kwargs['normalization'] else 'mm/day'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinaSstMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    tsmap_mod, tsmap_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, tsbox, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    tsmap_obs, tsmap_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, tsbox, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tsmap_mod, keyerror_mod3 = CheckTime(sst_mod, tsmap_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tsmap_obs, keyerror_obs3 = CheckTime(sst_obs, tsmap_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        tsmap_mod, Method, keyerror_mod2 = PreProcessTS(
            tsmap_mod, Method, areacell=tsmap_mod_areacell, compute_anom=False, region=tsbox, **kwargs)
        tsmap_obs, _, keyerror_obs2 = PreProcessTS(
            tsmap_obs, '', areacell=tsmap_obs_areacell, compute_anom=False, region=tsbox, **kwargs)
        del mod_areacell, obs_areacell, tsmap_mod_areacell, tsmap_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1 is not None) or \
                (keyerror_obs2 is not None or keyerror_mod2 is not None):
            tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                    'axes4': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod ts) ' + str(tsmap_mod.shape), 'shape4': '(obs ts) ' + str(tsmap_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod ts) ' + str(TimeBounds(tsmap_mod)),
                    'time4': '(obs ts) ' + str(TimeBounds(tsmap_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite TSA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            tsmap_mod = SeasonalMean(tsmap_mod, season_ev, compute_anom=True)
            tsmap_obs = SeasonalMean(tsmap_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                              'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                              'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                              'shape2': '(obs ts) ' + str(tsmap_obs.shape),
                              'time1': '(mod ts) ' + str(TimeBounds(tsmap_mod)),
                              'time2': '(obs ts) ' + str(TimeBounds(tsmap_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tsmap_mod, tsmap_obs, Method = TwoVarRegrid(
                    tsmap_mod, tsmap_obs, Method, region=tsbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                                  'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                                  'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                                  'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            tsmap_mod = Composite(tsmap_mod, event_years_mod, kwargs['frequency'])
            tsmap_obs = Composite(tsmap_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                              'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                              'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                              'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            tsmap_mod, keyerror_mod = BasinMask(
                tsmap_mod, 'pacific', box=tsbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            tsmap_obs, keyerror_obs = BasinMask(
                tsmap_obs, 'pacific', box=tsbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                                  'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                                  'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                                  'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                tsRmse, keyerror = RmsAxis(tsmap_mod, tsmap_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                tsRmseErr = None
                # Metric 2
                tsCorr = float(Correlation(tsmap_mod, tsmap_obs, axis='xy', centered=1, biased=1))
                tsCorrErr = None
                # Metric 3
                std_mod = Std(tsmap_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(tsmap_obs, weights=None, axis='xy', centered=1, biased=1)
                tsStd = float(std_mod) / float(std_obs)
                tsStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                             'time_period': str(actualtimebounds_mod), 'nina_years': str(event_years_mod)}
                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                             'time_period': str(actualtimebounds_obs), 'nina_years': str(event_years_obs)}
                    dict3 = {'metric_name': Name, 'metric_valueRMSE_' + dataset2: tsRmse,
                             'metric_valueRMSE_error_' + dataset2: tsRmseErr, 'metric_valueCORR_' + dataset2: tsCorr,
                             'metric_valueCORR_error_' + dataset2: tsCorrErr, 'metric_valueSTD_' + dataset2: tsStd,
                             'metric_valueCORR_error_' + dataset2: tsStdErr, 'metric_method': Method,
                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(file_name, var1=tsmap_mod, var1_attributes=dict1, var1_name='ts_map__' + dataset1,
                               var2=tsmap_obs, var2_attributes=dict2, var2_name='ts_map__' + dataset2,
                               global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if tsCorr is not None:
        tsCorr = 1 - tsCorr
    # Create output
    NinaSstMapMetric = {
        'name': Name, 'Rmse__value': tsRmse, 'Rmse__value_error': tsRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': tsCorr, 'Corr__value_error': tsCorrErr, 'Corr__units': '', 'Std__value': tsStd,
        'Std__value_error': tsStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinaSstMapMetric


def NinaSstTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  box, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='',
                  debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSstTsRmse() function computes a time composite of La Nina events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA < 'threshold' during 'season' are considered as La Nina events
        Then a 'nbr_years_window' long time series centered on selected events is composited for each selected event

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaTsMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina Composite Time Series'
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + ', time series of ' \
             + str(nbr_years_window) + ' years (centered on events)'
    Units = '' if kwargs['normalization'] else 'C'
    Ref = 'Using CDAT rms (uncentered and biased) calculation'
    metric = 'NinaSstTsRmse'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axis': None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                              'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. temporal composite of SSTA
            # ------------------------------------------------
            # interannual anomalies
            sst_mod = ComputeInterannualAnomalies(sst_mod)
            sst_obs = ComputeInterannualAnomalies(sst_obs)

            # composites
            composite_mod = Composite(sst_mod, event_years_mod, kwargs['frequency'], nbr_years_window=nbr_years_window)
            composite_obs = Composite(sst_obs, event_years_obs, kwargs['frequency'], nbr_years_window=nbr_years_window)
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in composite_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in composite_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(composite_mod.shape),
                              'shape2': '(obs) ' + str(composite_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # Computes the root mean square difference
            compRmse, keyerror = RmsAxis(
                composite_mod, composite_obs, axis=0, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            compRmseErr = None

            # Dive down diagnostic
            dive_down_diag = {'model': ArrayToList(composite_mod), 'observations': ArrayToList(composite_obs),
                              'axis': list(composite_mod.getAxis(0)[:])}
            if netcdf is True:
                # Read file and select the right region
                sst_hov_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                    sstfilemod, sstnamemod, 'temperature', metric, 'equatorial_pacific', file_area=sstareafilemod,
                    name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True,
                    maskocean=False, time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
                sst_hov_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                    sstfileobs, sstnameobs, 'temperature', metric, 'equatorial_pacific', file_area=sstareafileobs,
                    name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True,
                    maskocean=False, time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                if keyerror is None:
                    # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                    sst_hov_mod, _, keyerror_mod = PreProcessTS(
                        sst_hov_mod, Method, areacell=mod_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    sst_hov_obs, _, keyerror_obs = PreProcessTS(
                        sst_hov_obs, '', areacell=obs_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del mod_areacell, obs_areacell
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_hov_mod.shape),
                                          'shape2': '(obs) ' + str(sst_hov_obs.shape),
                                          'time1': '(mod) ' + str(TimeBounds(sst_hov_mod)),
                                          'time2': '(obs) ' + str(TimeBounds(sst_hov_obs))}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)
                        # Regridding
                        if 'regridding' not in list(kwargs.keys()):
                            kwargs['regridding'] = {'regridder': 'cdms', 'regridTool': 'esmf', 'regridMethod': 'linear',
                                                    'newgrid_name': 'generic_1x1deg'}
                        else:
                            if not isinstance(kwargs['regridding'], dict):
                                kwargs['regridding'] = {'regridder': 'cdms', 'regridTool': 'esmf',
                                                        'regridMethod': 'linear', 'newgrid_name': 'generic_1x1deg'}
                        sst_hov_mod, sst_hov_obs, Method = TwoVarRegrid(
                            sst_hov_mod, sst_hov_obs, Method, region='equatorial_pacific', **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_hov_mod.shape),
                                          'shape2': '(obs) ' + str(sst_hov_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)
                        # Meridional average
                        sst_hov_mod, keyerror_mod = AverageMeridional(sst_hov_mod)
                        sst_hov_obs, keyerror_obs = AverageMeridional(sst_hov_obs)
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                              'axes2': '(obs) ' + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                              'shape1': '(mod) ' + str(sst_hov_mod.shape),
                                              'shape2': '(obs) ' + str(sst_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)
                            # samples
                            sst_hov_mod = Composite(
                                sst_hov_mod, event_years_mod, kwargs['frequency'], nbr_years_window=nbr_years_window)
                            sst_hov_obs = Composite(
                                sst_hov_obs, event_years_obs, kwargs['frequency'], nbr_years_window=nbr_years_window)
                            if debug is True:
                                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                              'axes2': '(obs) ' + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                              'shape1': '(mod) ' + str(sst_hov_mod.shape),
                                              'shape2': '(obs) ' + str(sst_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod), 'nina_years': str(event_years_mod),
                                     'description': "time series of " + box + " sstA centered on La Nina peak"}
                            dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs), 'nina_years': str(event_years_obs),
                                     'description': "time series of " + box + " sstA centered on La Nina peak"}
                            dict3 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod), 'nina_years': str(event_years_mod),
                                     'description': "zonal monthly of equatorial_pacific sstA centered on La Nina peak"}
                            dict4 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs), 'nina_years': str(event_years_obs),
                                     'description': "zonal monthly of equatorial_pacific sstA centered on La Nina peak"}
                            dict5 = {'metric_name': Name, 'metric_value_' + dataset2: compRmse,
                                     'metric_value_error_' + dataset2: compRmseErr, 'metric_method': Method,
                                     'metric_reference': Ref, 'frequency': kwargs['frequency']}
                            SaveNetcdf(
                                file_name, var1=composite_mod, var1_attributes=dict1, var1_name='sst_ts__' + dataset1,
                                var2=composite_obs, var2_attributes=dict2, var2_name='sst_ts__' + dataset2,
                                var3=sst_hov_mod, var3_attributes=dict3, var3_name='sst_hov__' + dataset1,
                                var4=sst_hov_obs, var4_attributes=dict4, var4_name='sst_hov__' + dataset2,
                                global_attributes=dict5)
                            del dict1, dict2, dict3, dict4, dict5, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(compRmse), 'line2': 'metric value_error: ' + str(compRmseErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinaTsMetric = {
        'name': Name, 'value': compRmse, 'value_error': compRmseErr, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'events_model': event_years_mod,
        'events_observations': event_years_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinaTsMetric


def NinoPrMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod, prfilemod,
              prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs, sstnameobs,
              sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs, prnameobs,
              prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox, event_definition,
              centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False, netcdf=False, netcdf_name='',
              metname='', **kwargs):
    """
    The NinoPrMap() function computes a precipitation anomalies composite of during the peak of El Nino events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA > 'threshold' during 'season' are considered as El Nino events
    Then the PRA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param prbox: string
        name of box (e.g. 'global') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoPrMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino PRA Composite'
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + \
             ', Nino PRA Composited'
    Units = '' if kwargs['normalization'] else 'mm/day'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinoPrMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, 'precipitation', metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, 'precipitation', metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, pr_mod, keyerror_mod3 = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, pr_obs, keyerror_obs3 = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, '', areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        del mod_areacell, obs_areacell, pr_mod_areacell, pr_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1 is not None) or \
                (keyerror_mod2 is not None or keyerror_obs2 is not None):
            prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod pr) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes4': '(obs pr) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod pr) ' + str(pr_mod.shape), 'shape4': '(obs pr) ' + str(pr_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod pr) ' + str(TimeBounds(pr_mod)), 'time4': '(obs pr) ' + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': '(mod) ' + str(event_years_mod), 'nino2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite PRA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            pr_mod = SeasonalMean(pr_mod, season_ev, compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(pr_mod)), 'time2': '(obs) ' + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=prbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            pr_mod = Composite(pr_mod, event_years_mod, kwargs['frequency'])
            pr_obs = Composite(pr_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            pr_mod, keyerror_mod = BasinMask(
                pr_mod, 'pacific', box=prbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            pr_obs, keyerror_obs = BasinMask(
                pr_obs, 'pacific', box=prbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                prRmse, keyerror = RmsAxis(pr_mod, pr_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                prRmseErr = None
                # Metric 2
                prCorr = float(Correlation(pr_mod, pr_obs, axis='xy', centered=1, biased=1))
                prCorrErr = None
                # Metric 3
                std_mod = Std(pr_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(pr_obs, weights=None, axis='xy', centered=1, biased=1)
                prStd = float(std_mod) / float(std_obs)
                prStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}

                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {
                        'units': Units, 'number_of_years_used': yearN_mod, 'time_period': str(actualtimebounds_mod),
                        'nino_years': str(event_years_mod)}
                    dict2 = {
                        'units': Units, 'number_of_years_used': yearN_obs, 'time_period': str(actualtimebounds_obs),
                        'nino_years': str(event_years_obs)}
                    dict3 = {
                        'metric_name': Name, 'metric_valueRMSE_' + dataset2: prRmse,
                        'metric_valueRMSE_error_' + dataset2: prRmseErr, 'metric_valueCORR_' + dataset2: prCorr,
                        'metric_valueCORR_error_' + dataset2: prCorrErr, 'metric_valueSTD_' + dataset2: prStd,
                        'metric_valueCORR_error_' + dataset2: prStdErr, 'metric_method': Method,
                        'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(
                        file_name, var1=pr_mod, var1_attributes=dict1, var1_name='prComp_map__' + dataset1, var2=pr_obs,
                        var2_attributes=dict2, var2_name='prComp_map__' + dataset2, global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if prCorr is not None:
        prCorr = 1 - prCorr
    # Create output
    NinoPrMapMetric = {
        'name': Name, 'Rmse__value': prRmse, 'Rmse__value_error': prRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': prCorr, 'Corr__value_error': prCorrErr, 'Corr__units': '', 'Std__value': prStd,
        'Std__value_error': prStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinoPrMapMetric


def NinoSstDiv(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
               dataset='', debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSstDiv() function computes a zonal composite of El Nino events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > 'threshold' during 'season' are considered as El Nino events
        2.) diversity of the zonal location of the maximum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the maximum SSTA for each selected event
            2.3) compute the percentage of EP events (maximum SSTA eastward of the given threshold)

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param treshold_ep_ev: float, optional
        see EnsoToolsLib.percentage_val_eastward
        longitude, in degree east, of the westward boundary of eastern Pacific event
        default value is -140°E (i.e., 140°W)
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoDivMetric: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, ref, keyerror,
        dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'treshold_ep_ev',
                    'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino Diversity (percentage of eastern Pacific El Nino)'
    lat = ReferenceRegions(box)['latitude']
    lon = ReferenceRegions(box)['longitude']
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' + \
             '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']), westward boundary of EP events' + \
             str(kwargs['treshold_ep_ev']) + 'E'
    Units = '%'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinoSstDiv'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst, areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    if keyerror is not None:
        ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, _, keyerror = PreProcessTS(
            sst, '', areacell=areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del areacell
        if keyerror is not None:
            ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
        else:
            if debug is True:
                dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                              'shape1': '(sst) ' + str(sst.shape), 'time1': '(sst) ' + str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years = DetectEvents(sst, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': 'nbr(' + str(len(event_years)) + '): ' + str(event_years)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the maximum SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst, areacell, keyerror = Read_data_mask_area(
                sstfile, sstname, 'temperature', metric, box, file_area=sstareafile, name_area=sstareaname,
                file_mask=sstlandmaskfile, name_mask=sstlandmaskfile, maskland=True, maskocean=False, debug=debug,
                **kwargs)
            if keyerror is not None:
                ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst, Method, keyerror = PreProcessTS(
                    sst, Method, areacell=areacell, average=False, compute_anom=False, region=box, **kwargs)
                del areacell
                if keyerror is not None:
                    ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                      'shape1': '(sst) ' + str(sst.shape), 'time1': '(sst) ' + str(TimeBounds(sst))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst = SeasonalMean(sst, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                      'shape1': '(sst) ' + str(sst.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder', 'regridTool',
                                      'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst = Regrid(sst, None, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                          'shape1': '(sst) ' + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst, keyerror = AverageMeridional(sst)
                    if keyerror is not None:
                        ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                          'shape1': '(sst) ' + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sample = Event_selection(sst, kwargs['frequency'], list_event_years=event_years)

                        # 2.2 find the zonal position of the maximum SSTA for each selected event
                        lon_sstmax = FindXYMinMaxInTs(
                            sample, return_val='maxi', smooth=True, axis=0, window=5, method='triangle')
                        if debug is True:
                            dict_debug = {'line1': 'longitude of the maximum SSTA: ' + str(lon_sstmax)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)

                        # 2.3 compute the percentage of EP events (maximum SSTA eastward of the given threshold)
                        ep_event, keyerror = percentage_val_eastward(
                            lon_sstmax, metric, box, threshold=kwargs['treshold_ep_ev'])
                        ep_event = float(ep_event)

                        if keyerror is not None:
                            StdErr, dive_down_diag = None, {'value': None, 'axis': None}
                        else:
                            # Standard Error of the Standard Deviation (function of nyears)
                            StdErr = None

                            # Dive down diagnostic
                            dive_down_diag = {'value': ArrayToList(lon_sstmax), 'axis': list(lon_sstmax.getAxis(0)[:])}

                            if netcdf is True:
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {'units': 'longitude (E)', 'number_of_years_used': yearN,
                                         'time_period': str(actualtimebounds), 'nino_years': str(event_years),
                                         'diagnostic_value_' + dataset: ep_event,
                                         'diagnostic_value_error_' + dataset: StdErr}
                                dict2 = {'metric_name': Name, 'metric_method': Method, 'metric_reference': Ref,
                                         'frequency': kwargs['frequency']}
                                SaveNetcdf(file_name, var1=lon_sstmax, var1_attributes=dict1,
                                           var1_name='Nino_lon_pos_minSSTA__' + dataset, global_attributes=dict2)
                                del dict1, dict2
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(ep_event), 'line2': 'metric value_error: ' + str(StdErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinoDivMetric = {
        'name': Name, 'value': ep_event, 'value_error': StdErr, 'units': Units, 'method': Method, 'nyears': yearN,
        'events': event_years, 'time_frequency': kwargs['frequency'], 'time_period': actualtimebounds, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinoDivMetric


def NinoSstDivRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
                   netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSstDivRmse() function computes a zonal maximum of El Nino events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > 'threshold' during 'season' are considered as El Nino events
        2.) diversity of the zonal location of the maximum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the maximum SSTA for each selected event and compute a pdf

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoDivMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyword, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'PDF of zonal min(SSTA) during El Nino'
    lat = ReferenceRegions(box)['latitude']
    lon = ReferenceRegions(box)['longitude']
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' \
             + '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']'
    Units = 'density'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinoSstDivRmse'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axis': None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                              'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': '(mod) ' + str(event_years_mod), 'nino2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the maximum SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, 'temperature', metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
                file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
            sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, 'temperature', metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
                file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
            if keyerror_mod is not None or keyerror_obs is not None:
                pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst_mod, Method, keyerror_mod = PreProcessTS(
                    sst_mod, Method, areacell=mod_areacell, average=False, compute_anom=False, region=box, **kwargs)
                sst_obs, _, keyerror_obs = PreProcessTS(
                    sst_obs, '', areacell=obs_areacell, average=False, compute_anom=False, region=box, **kwargs)
                del mod_areacell, obs_areacell
                if keyerror_mod is not None or keyerror_obs is not None:
                    pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                    dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                                      'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                                      'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
                    sst_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name',
                                      'regridder', 'regridTool', 'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst_mod, sst_obs, Method = TwoVarRegrid(
                            sst_mod, sst_obs, Method, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst_mod, keyerror_mod = AverageMeridional(sst_mod)
                    sst_obs, keyerror_obs = AverageMeridional(sst_obs)
                    if keyerror_mod is not None or keyerror_obs is not None:
                        pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sample_mod = Event_selection(sst_mod, kwargs['frequency'], list_event_years=event_years_mod)
                        sample_obs = Event_selection(sst_obs, kwargs['frequency'], list_event_years=event_years_obs)

                        # 2.2 find the zonal position of the maximum SSTA for each selected event and compute a pdf
                        # longitude of the maximum SSTA for each selected event
                        lon_min_mod = FindXYMinMaxInTs(
                            sample_mod, return_val='maxi', smooth=True, axis=0, window=5, method='triangle')
                        lon_min_obs = FindXYMinMaxInTs(
                            sample_obs, return_val='maxi', smooth=True, axis=0, window=5, method='triangle')
                        if debug is True:
                            dict_debug = {'line1': '(mod) longitude  of the maximum SSTA: ' + str(lon_min_mod),
                                          'line2': '(obs) longitude  of the maximum SSTA: ' + str(lon_min_obs)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)

                        # compute PDFs
                        if debug is True:
                            dict_debug = {'line1': 'lon ' + str(lon) + '  ;  nbr_bins old = ' +
                                                   str((lon[1] - lon[0]) / 10) + '  ;  nbr_bins new = ' +
                                                   str(int(round((lon[1] - lon[0]) / 10)))}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'before ComputePDF', 15, **dict_debug)
                        pdf_mod = ComputePDF(lon_min_mod, nbr_bins=int(round((lon[1] - lon[0]) / 10)), interval=lon,
                                             axis_name='longitude')
                        pdf_obs = ComputePDF(lon_min_obs, nbr_bins=int(round((lon[1] - lon[0]) / 10)), interval=lon,
                                             axis_name='longitude')

                        # Computes the root mean square difference
                        pdfRmse, keyerror = RmsZonal(pdf_mod, pdf_obs, centered=centered_rmse, biased=biased_rmse)

                        # Error on the metric
                        pdfRmseErr = None

                        # Dive down diagnostic
                        dive_down_diag = {'model': ArrayToList(pdf_mod), 'observations': ArrayToList(pdf_obs),
                                          'axis': list(pdf_mod.getAxis(0)[:])}
                        if netcdf is True:
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod), 'nino_years': str(event_years_mod)}
                            dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs), 'nino_years': str(event_years_obs)}
                            dict3 = {'metric_name': Name, 'metric_value_' + dataset2: pdfRmse,
                                     'metric_value_error_' + dataset2: pdfRmseErr, 'metric_method': Method,
                                     'metric_reference': Ref, 'frequency': kwargs['frequency']}
                            SaveNetcdf(file_name, var1=pdf_mod, var1_attributes=dict1, var1_name='pdf__' + dataset1,
                                       var2=pdf_obs, var2_attributes=dict2, var2_name='pdf__' + dataset2,
                                       global_attributes=dict3)
                            del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(pdfRmse), 'line2': 'metric value_error: ' + str(pdfRmseErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinoDivMetric = {
        'name': Name, 'value': pdfRmse, 'value_error': pdfRmseErr, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'events_model': event_years_mod,
        'events_observations': event_years_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinoDivMetric


def NinoSstDur(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
               nbr_years_window, dataset='', debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSstDurRmse() function computes a duration of El Nino events.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > 'threshold' during 'season' are considered as El Nino events
        2.) El Nino duration
            2.1) get a time series of 2 years before and 2 years after the El Nino peak (4 years time series)
            2.2) count the number of consecutive month above a threshold

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoDurMetric: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, time_period, ref,
        keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino Duration'
    Units = 'months'
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + \
             ', number of consecutive months when sstA > 0.5' + Units
    Ref = 'Using CDAT'
    metric = 'NinoSstDur'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst, sst_areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    if keyerror is not None:
        duration_mean, duration_err, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, Method, keyerror = PreProcessTS(
            sst, Method, areacell=sst_areacell, average='horizontal', compute_anom=True, region=region_ev, **kwargs)
        del sst_areacell
        if keyerror is not None:
            duration_mean, duration_err, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
        else:
            if debug is True:
                dict_debug = {'axes1': str([ax.id for ax in sst.getAxisList()]), 'shape1': str(sst.shape),
                              'time1': str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years = DetectEvents(sst, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': str(event_years)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. El Nino duration
            # ------------------------------------------------
            # 2.1 get a time series of 2 years before and 2 years after the El Nino peak (4 years time series)
            # composites
            sample = Event_selection(
                sst, kwargs['frequency'], nbr_years_window=nbr_years_window, list_event_years=event_years)
            if debug is True:
                dict_debug = {'axes1': str([ax.id for ax in sample.getAxisList()]), 'shape1': str(sample.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Event_selection', 15, **dict_debug)

            # 2.2 count the number of consecutive month above a threshold
            if normalize is True:
                duration = DurationAllEvent(sample, 0.5 * float(Std(sst)), nino=True, debug=debug)
            else:
                duration = DurationAllEvent(sample, 0.5, nino=True, debug=debug)

            duration_err = float(Std(duration) / NUMPYsqrt(len(duration)))
            duration_mean = float(duration.mean())

            # Dive down diagnostic
            dive_down_diag = {'value': ArrayToList(duration), 'axis': list(duration.getAxis(0)[:])}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {'units': Units, 'number_of_years_used': yearN, 'time_period': str(actualtimebounds),
                         'nino_years': str(event_years), 'description': "duration of El Nino events",
                         'diagnostic_value': duration_mean, 'diagnostic_value_error': duration_err}
                dict2 = {'metric_name': Name, 'metric_method': Method, 'metric_reference': Ref,
                         'frequency': kwargs['frequency']}
                SaveNetcdf(file_name, var1=duration, var1_attributes=dict1, var1_name='Nino_duration__' + dataset,
                           global_attributes=dict2)
                del dict1, dict2, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(duration_mean),
                      'line2': 'metric value_error: ' + str(duration_err)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinoDurMetric = {
        'name': Name, 'value': duration_mean, 'value_error': duration_err, 'units': Units, 'method': Method,
        'nyears': yearN, 'events': event_years, 'time_frequency': kwargs['frequency'], 'time_period': actualtimebounds,
        'ref': Ref, 'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinoDurMetric


def NinoSstLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
                   netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSstLonRmse() function computes a zonal composite of El Nino events during the peak of the event
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA > 'threshold' during 'season' are considered as El Nino events
    Then the zonal SSTA at the peak of the event is composited for each selected event

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoLonMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino Zonal Composite'
    lat = ReferenceRegions(box)['latitude']
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' \
             + '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']'
    Units = '' if kwargs['normalization'] else 'C'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinoSstLonRmse'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axis': None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                              'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': '(mod) ' + str(event_years_mod), 'nino2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. zonal composite of SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, 'temperature', metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
                file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
            sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, 'temperature', metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
                file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
            if keyerror_mod is not None or keyerror_obs is not None:
                compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst_mod, Method, keyerror_mod = PreProcessTS(
                    sst_mod, Method, areacell=mod_areacell, average=False, compute_anom=False, region=box, **kwargs)
                sst_obs, _, keyerror_obs = PreProcessTS(
                    sst_obs, '', areacell=obs_areacell, average=False, compute_anom=False, region=box, **kwargs)
                del mod_areacell, obs_areacell
                if keyerror_mod is not None or keyerror_obs is not None:
                    compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                    dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                                      'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                                      'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
                    sst_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name',
                                      'regridder', 'regridTool', 'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst_mod, sst_obs, Method = TwoVarRegrid(
                            sst_mod, sst_obs, Method, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst_mod, keyerror_mod = AverageMeridional(sst_mod)
                    sst_obs, keyerror_obs = AverageMeridional(sst_obs)
                    if keyerror_mod is not None or keyerror_obs is not None:
                        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sst_mod = Composite(sst_mod, event_years_mod, kwargs['frequency'])
                        sst_obs = Composite(sst_obs, event_years_obs, kwargs['frequency'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

                        # Computes the root mean square difference
                        compRmse, keyerror = RmsZonal(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)

                        # Error on the metric
                        compRmseErr = None

                        # Dive down diagnostic
                        dive_down_diag = {'model': ArrayToList(sst_mod), 'observations': ArrayToList(sst_obs),
                                          'axis': list(sst_mod.getAxis(0)[:])}
                        if netcdf is True:
                            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                                sstfilemod, sstnamemod, 'temperature', metric, 'equatorial_pacific_LatExt2',
                                file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                                name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
                                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
                            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                                sstfileobs, sstnameobs, 'temperature', metric, 'equatorial_pacific_LatExt2',
                                file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                                name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
                                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
                            if keyerror_mod is not None or keyerror_obs is not None:
                                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            else:
                                map_mod, _, keyerror_mod = PreProcessTS(
                                    map_mod, '', areacell=mod_areacell, average=False, compute_anom=False,
                                    region="equatorial_pacific_LatExt2", **kwargs)
                                map_obs, _, keyerror_obs = PreProcessTS(
                                    map_obs, '', areacell=obs_areacell, average=False, compute_anom=False,
                                    region="equatorial_pacific_LatExt2", **kwargs)
                                del mod_areacell, obs_areacell
                                if keyerror_mod is not None or keyerror_obs is not None:
                                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                                else:
                                    if debug is True:
                                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in map_mod.getAxisList()]),
                                                      'axes2': '(obs) ' + str([ax.id for ax in map_obs.getAxisList()]),
                                                      'shape1': '(mod) ' + str(map_mod.shape),
                                                      'shape2': '(obs) ' + str(map_obs.shape),
                                                      'time1': '(mod) ' + str(TimeBounds(map_mod)),
                                                      'time2': '(obs) ' + str(TimeBounds(map_obs))}
                                        EnsoErrorsWarnings.debug_mode(
                                            '\033[92m', 'after PreProcessTS: netcdf', 15, **dict_debug)
                                    # Seasonal mean
                                    map_mod = SeasonalMean(map_mod, season_ev, compute_anom=True)
                                    map_obs = SeasonalMean(map_obs, season_ev, compute_anom=True)
                                    # Regridding
                                    if isinstance(kwargs['regridding'], dict):
                                        map_mod, map_obs, _ = TwoVarRegrid(
                                            map_mod, map_obs, '', region='equatorial_pacific_LatExt2',
                                            **kwargs['regridding'])
                                        if debug is True:
                                            dict_debug = {
                                                'axes1': '(mod) ' + str([ax.id for ax in map_mod.getAxisList()]),
                                                'axes2': '(obs) ' + str([ax.id for ax in map_obs.getAxisList()]),
                                                'shape1': '(mod) ' + str(map_mod.shape),
                                                'shape2': '(obs) ' + str(map_obs.shape)}
                                            EnsoErrorsWarnings.debug_mode(
                                                '\033[92m', 'after TwoVarRegrid: netcdf', 15, **dict_debug)
                                    # samples
                                    map_mod = Composite(map_mod, event_years_mod, kwargs['frequency'])
                                    map_obs = Composite(map_obs, event_years_obs, kwargs['frequency'])
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                             'time_period': str(actualtimebounds_mod),
                                             'nino_years': str(event_years_mod)}
                                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                             'time_period': str(actualtimebounds_obs),
                                             'nino_years': str(event_years_obs)}
                                    dict3 = {'units': Units, 'number_of_years_used': yearN_mod,
                                             'time_period': str(actualtimebounds_mod),
                                             'nino_years': str(event_years_mod)}
                                    dict4 = {'units': Units, 'number_of_years_used': yearN_obs,
                                             'time_period': str(actualtimebounds_obs),
                                             'nino_years': str(event_years_obs)}
                                    dict5 = {'metric_name': Name, 'metric_value_' + dataset2: compRmse,
                                             'metric_value_error_' + dataset2: compRmseErr, 'metric_method': Method,
                                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                                    SaveNetcdf(
                                        file_name, var1=sst_mod, var1_attributes=dict1,
                                        var1_name='sst_lon__' + dataset1, var2=sst_obs, var2_attributes=dict2,
                                        var2_name='sst_lon__' + dataset2, var3=map_mod, var3_attributes=dict3,
                                        var3_name='sst_map__' + dataset1, var4=map_obs, var4_attributes=dict4,
                                        var4_name='sst_map__' + dataset2, global_attributes=dict5)
                                    del dict1, dict2, dict3, dict4, dict5, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(compRmse), 'line2': 'metric value_error: ' + str(compRmseErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinoLonMetric = {
        'name': Name, 'value': compRmse, 'value_error': compRmseErr, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'events_model': event_years_mod,
        'events_observations': event_years_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinoLonMetric


def NinoSlpMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               slpfilemod, slpnamemod, slpareafilemod, slpareanamemod, slplandmaskfilemod, slplandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
               slpfileobs, slpnameobs, slpareafileobs, slpareanameobs, slplandmaskfileobs, slplandmasknameobs, sstbox,
               slpbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
               netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSlpMap() function computes a sea level pressure anomalies composite of during the peak of El Nino events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA > 'threshold' during 'season' are considered as El Nino events
    Then the SLPA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param slpfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SLP
    :param slpnamemod: string
        name of SLP variable (slp) in 'slpfilemod'
    :param slpareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP areacell
    :param slpareanamemod: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafilemod'
    :param slplandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP landmask
    :param slplandmasknamemod: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param slpfileobs: string
        path_to/filename of the file (NetCDF) of the observed SLP
    :param slpnameobs: string
        name of SLP variable (slp) in 'slpfileobs'
    :param slpareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP areacell
    :param slpareanameobs: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafileobs'
    :param slplandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP landmask
    :param slplandmasknameobs: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param slpbox: string
        name of box (e.g. 'global') for SLP
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoSlpMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino SLPA Composite'
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + \
             ', Nino SLPA Composited'
    Units = '' if kwargs['normalization'] else 'hPa'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinoSlpMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    slp_mod, slp_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        slpfilemod, slpnamemod, 'pressure', metric, slpbox, file_area=slpareafilemod, name_area=slpareanamemod,
        file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    slp_obs, slp_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        slpfileobs, slpnameobs, 'pressure', metric, slpbox, file_area=slpareafileobs, name_area=slpareanameobs,
        file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, slp_mod, keyerror_mod3 = CheckTime(sst_mod, slp_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, slp_obs, keyerror_obs3 = CheckTime(sst_obs, slp_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        slp_mod, Method, keyerror_mod2 = PreProcessTS(
            slp_mod, Method, areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
        slp_obs, _, keyerror_obs2 = PreProcessTS(
            slp_obs, '', areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
        del mod_areacell, obs_areacell, slp_mod_areacell, slp_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1) or \
                (keyerror_mod2 is not None or keyerror_obs2):
            slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                    'axes4': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod slp) ' + str(slp_mod.shape), 'shape4': '(obs slp) ' + str(slp_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod slp) ' + str(TimeBounds(slp_mod)), 'time4': '(obs slp) ' + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': '(mod) ' + str(event_years_mod), 'nino2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite SLPA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            slp_mod = SeasonalMean(slp_mod, season_ev, compute_anom=True) * 1e-2
            slp_obs = SeasonalMean(slp_obs, season_ev, compute_anom=True) * 1e-2
            if debug is True:
                dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                              'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                              'shape1': '(mod slp) ' + str(slp_mod.shape), 'shape2': '(obs slp) ' + str(slp_obs.shape),
                              'time1': '(mod slp) ' + str(TimeBounds(slp_mod)),
                              'time2': '(obs slp) ' + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                slp_mod, slp_obs, Method = TwoVarRegrid(slp_mod, slp_obs, Method, region=slpbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                                  'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                                  'shape1': '(mod slp) ' + str(slp_mod.shape),
                                  'shape2': '(obs slp) ' + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            slp_mod = Composite(slp_mod, event_years_mod, kwargs['frequency'])
            slp_obs = Composite(slp_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                              'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                              'shape1': '(mod slp) ' + str(slp_mod.shape), 'shape2': '(obs slp) ' + str(slp_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            slp_mod, keyerror_mod = BasinMask(
                slp_mod, 'pacific', box=slpbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            slp_obs, keyerror_obs = BasinMask(
                slp_obs, 'pacific', box=slpbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(slp_mod.shape), 'shape2': '(obs) ' + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                slpRmse, keyerror = RmsAxis(slp_mod, slp_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                slpRmseErr = None
                # Metric 2
                slpCorr = float(Correlation(slp_mod, slp_obs, axis='xy', centered=1, biased=1))
                slpCorrErr = None
                # Metric 3
                std_mod = Std(slp_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(slp_obs, weights=None, axis='xy', centered=1, biased=1)
                slpStd = float(std_mod) / float(std_obs)
                slpStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                             'time_period': str(actualtimebounds_mod),
                             'nino_years': str(event_years_mod)}
                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                             'time_period': str(actualtimebounds_obs),
                             'nino_years': str(event_years_obs)}
                    dict3 = {'metric_name': Name, 'metric_valueRMSE_' + dataset2: slpRmse,
                             'metric_valueRMSE_error_' + dataset2: slpRmseErr, 'metric_valueCORR_' + dataset2: slpCorr,
                             'metric_valueCORR_error_' + dataset2: slpCorrErr, 'metric_valueSTD_' + dataset2: slpStd,
                             'metric_valueCORR_error_' + dataset2: slpStdErr, 'metric_method': Method,
                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(file_name, var1=slp_mod, var1_attributes=dict1, var1_name='slp_map__' + dataset1,
                               var2=slp_obs, var2_attributes=dict2, var2_name='slp_map__' + dataset2,
                               global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if slpCorr is not None:
        slpCorr = 1 - slpCorr
    # Create output
    NinoSlpMapMetric = {
        'name': Name, 'Rmse__value': slpRmse, 'Rmse__value_error': slpRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': slpCorr, 'Corr__value_error': slpCorrErr, 'Corr__units': '', 'Std__value': slpStd,
        'Std__value_error': slpStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinoSlpMapMetric


def NinoSstDiversity(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box,
                     event_definition, dataset='', debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSstDiversity() function computes a zonal composite of El Nino events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > 'threshold' during 'season' are considered as El Nino events
        2.) diversity of the zonal location of the maximum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the maximum SSTA for each selected event
            2.3) compute the spread of the distribution

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param treshold_ep_ev: float, optional
        see EnsoToolsLib.percentage_val_eastward
        longitude, in degree east, of the westward boundary of eastern Pacific event
        default value is -140°E (i.e., 140°W)
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoDivMetric: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, ref, keyerror,
        dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    my_thresh = 'std' if normalize is True else 'C'
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'treshold_ep_ev',
                    'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "El Nino Diversity (interquartile range)"
    lat = ReferenceRegions(box)['latitude']
    lon = ReferenceRegions(box)['longitude']

    Method = "Nino events = " + region_ev + " SSTA > " + str(threshold) + my_thresh + " during " + season_ev + \
             ", zonal SSTA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) + "]), the zonal SSTA " + \
             "maximum is located for each event, the diversity is the interquartile range (IQR = Q3 - Q1)"
    Units = 'long'
    Ref = 'Using CDAT regridding'
    metric = 'NinoSstDiversity'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst, sst_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    sstmap, sstmap_areacell, keyerror2 = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, box, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    keyerror = add_up_errors([keyerror1, keyerror2])
    if keyerror is not None:
        dispersion1, dispersion1_err, dispersion2, dispersion2_err, event_years = None, None, None, None, None
        dive_down_diag = None, None, {'value': None, 'axis': None}
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        enso, _, keyerror1 = PreProcessTS(
            sst, '', areacell=sst_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sstmap, Method, keyerror2 = PreProcessTS(
            sstmap, Method, areacell=sstmap_areacell, average=False, compute_anom=False, region=box, **kwargs)
        del sst_areacell, sstmap_areacell
        if keyerror1 is not None or keyerror2 is not None:
            dispersion1, dispersion1_err, dispersion2, dispersion2_err, event_years = None, None, None, None, None
            dive_down_diag = None, None, {'value': None, 'axis': None}
        else:
            if debug is True:
                dict_debug = {'axes1': '(sst enso) ' + str([ax.id for ax in enso.getAxisList()]),
                              'axes2': '(sst map) ' + str([ax.id for ax in sstmap.getAxisList()]),
                              'shape1': '(sst enso) ' + str(enso.shape), 'shape2': '(sst map) ' + str(sstmap.shape),
                              'time1': '(sst enso) ' + str(TimeBounds(enso)),
                              'time2': '(sst map) ' + str(TimeBounds(sstmap))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years = DetectEvents(enso, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': 'nbr(' + str(len(event_years)) + '): ' + str(event_years)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the maximum SSTA
            # ------------------------------------------------
            # 2.1 zonal SSTA at the peak of the event is computed for each selected event
            # Seasonal mean
            sstmap = SeasonalMean(sstmap, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sstmap.getAxisList()]),
                              'shape1': '(sst) ' + str(sstmap.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder', 'regridTool',
                              'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sstmap = Regrid(sstmap, None, region=box, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sstmap.getAxisList()]),
                                  'shape1': '(sst) ' + str(sstmap.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # Meridional average
            sstlon, keyerror = AverageMeridional(sstmap)
            if keyerror is not None:
                dispersion1, dispersion1_err, dispersion2, dispersion2_err = None, None, None, None
                dive_down_diag = None, None, {'value': None, 'axis': None}
            else:
                if debug is True:
                    dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sstlon.getAxisList()]),
                                  'shape1': '(sst) ' + str(sstlon.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                if len(event_years) > 0:
                    # samples
                    sample = Event_selection(sstlon, kwargs['frequency'], list_event_years=event_years)

                    # 2.2 find the zonal position of the maximum SSTA for each selected event
                    lon_sstmax = FindXYMinMaxInTs(
                        sample, return_val='maxi', smooth=True, axis=0, window=5, method='triangle')
                    if debug is True:
                        dict_debug = {'line1': 'longitude of the maximum SSTA: ' + str(lon_sstmax)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)

                    # 2.3 compute the spread of the distribution
                    dispersion1 = statistical_dispersion(lon_sstmax, method='IQR')
                    dispersion2 = statistical_dispersion(lon_sstmax, method='MAD')
                    dispersion1_err = None
                    dispersion2_err = None
                else:
                    lon_sstmax = MyEmpty(sstlon[:5, 0], time=True, time_id='years')
                    dispersion1 = None
                    dispersion2 = None
                    dispersion1_err = None
                    dispersion2_err = None

                # Dive down diagnostic
                dive_down_diag = {'value': ArrayToList(lon_sstmax), 'axis': list(lon_sstmax.getAxis(0)[:])}
                if netcdf is True:
                    # nina events
                    nina_years = DetectEvents(enso, season_ev, -threshold, normalization=normalize, nino=False)
                    if len(event_years) > 0:
                        # samples
                        sample = Event_selection(sstlon, kwargs['frequency'], list_event_years=nina_years)
                        # 2.2 find the zonal position of the maximum SSTA for each selected event
                        lon_sstmin = FindXYMinMaxInTs(
                            sample, return_val='mini', smooth=True, axis=0, window=5, method='triangle')
                        if debug is True:
                            dict_debug = {'line1': 'longitude of the minimum SSTA: ' + str(lon_sstmax)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)
                        # 2.3 compute the spread of the distribution
                        nina_disp1 = statistical_dispersion(lon_sstmin, method='IQR')
                        nina_disp2 = statistical_dispersion(lon_sstmin, method='MAD')
                        nina_disp1_err = None
                        nina_disp2_err = None
                    else:
                        lon_sstmin = MyEmpty(sstlon[:5, 0], time=True, time_id='years')
                        nina_disp1 = None
                        nina_disp2 = None
                        nina_disp1_err = None
                        nina_disp2_err = None
                    # save
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {'units': Units, 'number_of_years_used': yearN, 'time_period': str(actualtimebounds),
                             'nino_years': str(event_years),
                             'description':
                                 "Nino events = " + region_ev + " SSTA > " + str(threshold) + my_thresh + " during " +
                                 season_ev + ", zonal SSTA " + "(meridional averaged [" + str(lat[0]) + " ; " +
                                 str(lat[1]) + "]), the zonal SSTA maximum is located for each event, the diversity " +
                                 "is the interquartile range (IQR = Q3 - Q1), second value is the median absolute " +
                                 "deviation (MAD = median([Xi - median(tab)]))",
                             'diagnostic_value_' + dataset: dispersion1,
                             'diagnostic_value_error_' + dataset: dispersion1_err,
                             'diagnostic_value2_' + dataset: dispersion2,
                             'diagnostic_value_error2_' + dataset: dispersion2_err}
                    dict2 = {'units': Units, 'number_of_years_used': yearN, 'time_period': str(actualtimebounds),
                             'nino_years': str(nina_years),
                             'description':
                                 "Nina events = " + region_ev + " SSTA < -" + str(threshold) + my_thresh + " during " +
                                 season_ev + ", zonal SSTA " + "(meridional averaged [" + str(lat[0]) + " ; " +
                                 str(lat[1]) + "]), the zonal SSTA minimum is located for each event, the diversity " +
                                 "is the interquartile range (IQR = Q3 - Q1), second value is the median absolute " +
                                 "deviation (MAD = median([Xi - median(tab)]))",
                             'diagnostic_value_' + dataset: nina_disp1,
                             'diagnostic_value_error_' + dataset: nina_disp1_err,
                             'diagnostic_value2_' + dataset: nina_disp2,
                             'diagnostic_value_error2_' + dataset: nina_disp2_err}
                    dict3 = {'metric_name': Name, 'metric_method': Method, 'metric_reference': Ref,
                             'frequency': kwargs['frequency']}
                    SaveNetcdf(file_name, var1=lon_sstmax, var1_attributes=dict1,
                               var1_name='Nino_lon_pos_maxSSTA__' + dataset, var2=lon_sstmin, var2_attributes=dict2,
                               var2_name='Nina_lon_pos_minSSTA__' + dataset, global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'diagnostic (IQR) value: ' + str(dispersion1),
                      'line2': 'diagnostic (IQR) value_error: ' + str(dispersion1_err),
                      'line3': 'diagnostic (MAD) value: ' + str(dispersion2),
                      'line4': 'diagnostic (MAD) value_error: ' + str(dispersion2_err)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinoDivMetric = {
        'name': Name, 'value': dispersion1, 'value_error': dispersion1_err, 'value2': dispersion2,
        'value_error2': dispersion2_err, 'units': Units, 'method': Method, 'nyears': yearN, 'events': event_years,
        'time_frequency': kwargs['frequency'], 'time_period': actualtimebounds, 'ref': Ref, 'keyerror': keyerror,
        'dive_down_diag': dive_down_diag}
    return NinoDivMetric


def NinoSstMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, tsbox,
               event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False, netcdf=False,
               netcdf_name='', metname='', **kwargs):
    """
    The NinoSstMap() function computes a surface temperature anomalies composite of during the peak of El Nino events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA > 'threshold' during 'season' are considered as El Nino events
    Then the TSA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoSstMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino TSA Composite'
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + \
             ', Nino TSA Composited'
    Units = '' if kwargs['normalization'] else 'mm/day'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinoSstMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    tsmap_mod, tsmap_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, tsbox, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    tsmap_obs, tsmap_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, tsbox, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tsmap_mod, keyerror_mod3 = CheckTime(sst_mod, tsmap_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tsmap_obs, keyerror_obs3 = CheckTime(sst_obs, tsmap_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        tsmap_mod, Method, keyerror_mod2 = PreProcessTS(
            tsmap_mod, Method, areacell=tsmap_mod_areacell, compute_anom=False, region=tsbox, **kwargs)
        tsmap_obs, _, keyerror_obs2 = PreProcessTS(
            tsmap_obs, '', areacell=tsmap_obs_areacell, compute_anom=False, region=tsbox, **kwargs)
        del mod_areacell, obs_areacell, tsmap_mod_areacell, tsmap_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1 is not None) or \
                (keyerror_obs2 is not None or keyerror_mod2 is not None):
            tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                    'axes4': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod ts) ' + str(tsmap_mod.shape), 'shape4': '(obs ts) ' + str(tsmap_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod ts) ' + str(TimeBounds(tsmap_mod)),
                    'time4': '(obs ts) ' + str(TimeBounds(tsmap_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': '(mod) ' + str(event_years_mod), 'nino2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite TSA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            tsmap_mod = SeasonalMean(tsmap_mod, season_ev, compute_anom=True)
            tsmap_obs = SeasonalMean(tsmap_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                              'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                              'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                              'shape2': '(obs ts) ' + str(tsmap_obs.shape),
                              'time1': '(mod ts) ' + str(TimeBounds(tsmap_mod)),
                              'time2': '(obs ts) ' + str(TimeBounds(tsmap_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tsmap_mod, tsmap_obs, Method = TwoVarRegrid(
                    tsmap_mod, tsmap_obs, Method, region=tsbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                                  'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                                  'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                                  'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            tsmap_mod = Composite(tsmap_mod, event_years_mod, kwargs['frequency'])
            tsmap_obs = Composite(tsmap_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                              'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                              'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                              'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            tsmap_mod, keyerror_mod = BasinMask(
                tsmap_mod, 'pacific', box=tsbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            tsmap_obs, keyerror_obs = BasinMask(
                tsmap_obs, 'pacific', box=tsbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                                  'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                                  'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                                  'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                tsRmse, keyerror = RmsAxis(tsmap_mod, tsmap_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                tsRmseErr = None
                # Metric 2
                tsCorr = float(Correlation(tsmap_mod, tsmap_obs, axis='xy', centered=1, biased=1))
                tsCorrErr = None
                # Metric 3
                std_mod = Std(tsmap_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(tsmap_obs, weights=None, axis='xy', centered=1, biased=1)
                tsStd = float(std_mod) / float(std_obs)
                tsStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                             'time_period': str(actualtimebounds_mod), 'nino_years': str(event_years_mod)}
                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                             'time_period': str(actualtimebounds_obs), 'nino_years': str(event_years_obs)}
                    dict3 = {'metric_name': Name, 'metric_valueRMSE_' + dataset2: tsRmse,
                             'metric_valueRMSE_error_' + dataset2: tsRmseErr, 'metric_valueCORR_' + dataset2: tsCorr,
                             'metric_valueCORR_error_' + dataset2: tsCorrErr, 'metric_valueSTD_' + dataset2: tsStd,
                             'metric_valueCORR_error_' + dataset2: tsStdErr, 'metric_method': Method,
                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(file_name, var1=tsmap_mod, var1_attributes=dict1, var1_name='ts_map__' + dataset1,
                               var2=tsmap_obs, var2_attributes=dict2, var2_name='ts_map__' + dataset2,
                               global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if tsCorr is not None:
        tsCorr = 1 - tsCorr
    # Create output
    NinoSstMapMetric = {
        'name': Name, 'Rmse__value': tsRmse, 'Rmse__value_error': tsRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': tsCorr, 'Corr__value_error': tsCorrErr, 'Corr__units': '', 'Std__value': tsStd,
        'Std__value_error': tsStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinoSstMapMetric


def NinoSstTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  box, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                  debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The NinoSstTsRmse() function computes a time composite of El Nino events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA > 'threshold' during 'season' are considered as El Nino events
        Then a 'nbr_years_window' long time series centered on selected events is composited for each selected event

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoTsMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    threshold = event_definition["threshold"]
    normalize = event_definition["normalization"]
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "El Nino Composite Time Series"
    Method = "Nino events = " + region_ev + " sstA > " + str(threshold) + " during " + season_ev + ", time series of " \
             + str(nbr_years_window) + " years (centered on events)"
    Units = "" if kwargs["normalization"] else "C"
    Ref = "Using CDAT rms (uncentered and biased) calculation"
    metric = "NinoSstTsRmse"
    if metname == "":
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {"model": None, "observations": None, "axis": None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {"model": None, "observations": None, "axis": None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape),
                              "time1": "(mod) " + str(TimeBounds(sst_mod)),
                              "time2": "(obs) " + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {"nino1": "(mod) " + str(event_years_mod), "nino2": "(obs) " + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

            # ------------------------------------------------
            # 2. temporal composite of SSTA
            # ------------------------------------------------
            # interannual anomalies
            sst_mod = ComputeInterannualAnomalies(sst_mod)
            sst_obs = ComputeInterannualAnomalies(sst_obs)

            # composites
            composite_mod = Composite(sst_mod, event_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            composite_obs = Composite(sst_obs, event_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in composite_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in composite_obs.getAxisList()]),
                              "shape1": "(mod) " + str(composite_mod.shape),
                              "shape2": "(obs) " + str(composite_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Composite", 15, **dict_debug)

            # Computes the root mean square difference
            compRmse, keyerror = RmsAxis(
                composite_mod, composite_obs, axis=0, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            compRmseErr = None

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(composite_mod), "observations": ArrayToList(composite_obs),
                              "axis": list(composite_mod.getAxis(0)[:])}
            if netcdf is True:
                # Read file and select the right region
                sst_hov_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                    sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific", file_area=sstareafilemod,
                    name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True,
                    maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                sst_hov_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                    sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific", file_area=sstareafileobs,
                    name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True,
                    maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                if keyerror is None:
                    # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                    sst_hov_mod, _, keyerror_mod = PreProcessTS(
                        sst_hov_mod, Method, areacell=mod_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    sst_hov_obs, _, keyerror_obs = PreProcessTS(
                        sst_hov_obs, "", areacell=obs_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del mod_areacell, obs_areacell
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                          "axes2": "(obs) " + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                          "shape1": "(mod) " + str(sst_hov_mod.shape),
                                          "shape2": "(obs) " + str(sst_hov_obs.shape),
                                          "time1": "(mod) " + str(TimeBounds(sst_hov_mod)),
                                          "time2": "(obs) " + str(TimeBounds(sst_hov_obs))}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                        # Regridding
                        if "regridding" not in list(kwargs.keys()):
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        else:
                            if not isinstance(kwargs["regridding"], dict):
                                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf",
                                                        "regridMethod": "linear", "newgrid_name": "generic_1x1deg"}
                        sst_hov_mod, sst_hov_obs, Method = TwoVarRegrid(
                            sst_hov_mod, sst_hov_obs, Method, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                          "axes2": "(obs) " + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                          "shape1": "(mod) " + str(sst_hov_mod.shape),
                                          "shape2": "(obs) " + str(sst_hov_obs.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                        # Meridional average
                        sst_hov_mod, keyerror_mod = AverageMeridional(sst_hov_mod)
                        sst_hov_obs, keyerror_obs = AverageMeridional(sst_hov_obs)
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(sst_hov_mod.shape),
                                              "shape2": "(obs) " + str(sst_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # samples
                            sst_hov_mod = Composite(
                                sst_hov_mod, event_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            sst_hov_obs = Composite(
                                sst_hov_obs, event_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(sst_hov_mod.shape),
                                              "shape2": "(obs) " + str(sst_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Composite", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "nino_years": str(event_years_mod),
                                     "description": "time series of " + box + " sstA centered on El Nino peak"}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "nino_years": str(event_years_obs),
                                     "description": "time series of " + box + " sstA centered on El Nino peak"}
                            dict3 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "nino_years": str(event_years_mod),
                                     "description": "zonal monthly of equatorial_pacific sstA centered on El Nino peak"}
                            dict4 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "nino_years": str(event_years_obs),
                                     "description": "zonal monthly of equatorial_pacific sstA centered on El Nino peak"}
                            dict5 = {"metric_name": Name, "metric_value_" + dataset2: compRmse,
                                     "metric_value_error_" + dataset2: compRmseErr, "metric_method": Method,
                                     "metric_reference": Ref, "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, var1=composite_mod, var1_attributes=dict1, var1_name="sst_ts__" + dataset1,
                                var2=composite_obs, var2_attributes=dict2, var2_name="sst_ts__" + dataset2,
                                var3=sst_hov_mod, var3_attributes=dict3, var3_name="sst_hov__" + dataset1,
                                var4=sst_hov_obs, var4_attributes=dict4, var4_name="sst_hov__" + dataset2,
                                global_attributes=dict5)
                            del dict1, dict2, dict3, dict4, dict5, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(compRmse), "line2": "metric value_error: " + str(compRmseErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    NinoTsMetric = {
        "name": Name, "value": compRmse, "value_error": compRmseErr, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "events_model": event_years_mod,
        "events_observations": event_years_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return NinoTsMetric
# ---------------------------------------------------------------------------------------------------------------------#
