# -*- coding:UTF-8 -*-
from copy import deepcopy
from inspect import stack as INSPECTstack
from numpy import sign as NUMPYsign
from numpy import sqrt as NUMPYsqrt
from numpy import square as NUMPYsquare

# ENSO_metrics package functions:
from .EnsoCollectionsLib import ReferenceRegions
from . import EnsoErrorsWarnings
from .EnsoPlotLib import metric_variable_names
from .EnsoToolsLib import add_up_errors, percentage_val_eastward, statistical_dispersion
from .EnsoUvcdatToolsLib import ArrayListAx, ArrayToList, AverageMeridional, AverageTemporal, AverageZonal, BasinMask, \
    CheckTime, Composite, ComputeInterannualAnomalies, ComputePDF, Concatenate, Correlation, DetectEvents, \
    DurationAllEvent, DurationEvent, Event_selection, fill_dict_axis, FindXYMinMaxInTs, get_year_by_year, \
    kurtosis_temporal, LinearRegressionAndNonlinearity, LinearRegressionTsAgainstMap, LinearRegressionTsAgainstTs, \
    MinMax, MyEmpty, PreProcessTS, Read_data_mask_area, Read_data_mask_area_multifile, Regrid, RmsAxis, RmsHorizontal, \
    RmsMeridional, RmsZonal, SaveNetcdf, SeasonalMean, SkewnessTemporal, SlabOcean, Smoothing, Std, StdMonthly, \
    TimeBounds, TsToMap, TwoVarRegrid
from .KeyArgLib import default_arg_values


# ---------------------------------------------------------------------------------------------------------------------#
#
# Library to compute ENSO metrics
# These functions have file names and variable names as inputs and metric as output
#
def BiasPrMapRmse(prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                  prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, box,
                  centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                  metname="", **kwargs):
    """
    The BiasPrMapRmse() function computes the PR spatial root mean square error (RMSE) in a 'box' (usually the tropical
    Pacific)

    Inputs:
    ------
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr, precip) in 'prfilemod'
    :param prareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for PR
    :param prareanamemod: string
        name of areacell variable (areacella, areacello) in 'prareafilemod'
    :param prlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for PR
    :param prlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfilemod'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, prec) in 'prfileobs'
    :param prareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for PR
    :param prareanameobs: string
        name of areacell variable (areacella, areacello) in 'prareafileobs'
    :param prlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for PR
    :param prlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for PR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasPrMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean pr RMSE"
    Units = "mm/day"
    Method = "Spatial root mean square error of " + box + " pr"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasPrMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    pr_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, box, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, box, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(pr_mod.shape[0] / 12))
    yearN_obs = int(round(pr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(pr_mod)
    actualtimebounds_obs = TimeBounds(pr_obs)
    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        pr_mod, Method, keyerror_mod = PreProcessTS(
            pr_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        pr_obs, _, keyerror_obs = PreProcessTS(
            pr_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsHorizontal(pr_mod, pr_obs, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(pr_mod, pr_obs, axis="xy", centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(pr_mod, weights=None, axis="xy", centered=1, biased=1)
            sm_std_obs = Std(pr_obs, weights=None, axis="xy", centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "spatialSTD": sm_std_mod, "description": "mean PR map"}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "spatialSTD": sm_std_obs, "description": "mean PR map"}
                dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                         "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                         "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                         "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error,
                         "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, global_attributes=dict3,
                           var1=pr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                           var2=pr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasPrLatRmse(prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                  prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, box,
                  centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                  metname="", **kwargs):
    """
    The BiasPrLatRmse() function computes the PR meridional (latitude) root mean square error (RMSE) in a 'box'
    (usually 'nino3_LatExt')

    Inputs:
    ------
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr, precip) in 'prfilemod'
    :param prareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for PR
    :param prareanamemod: string
        name of areacell variable (areacella, areacello) in 'prareafilemod'
    :param prlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for PR
    :param prlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfilemod'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, prec) in 'prfileobs'
    :param prareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for PR
    :param prareanameobs: string
        name of areacell variable (areacella, areacello) in 'prareafileobs'
    :param prlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for PR
    :param prlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for PR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasPrLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean pr Meridional RMSE"
    Units = "mm/day"
    Method = "Meridional root mean square error of " + box + " pr"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasPrLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    pr_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, box, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, box, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(pr_mod.shape[0] / 12))
    yearN_obs = int(round(pr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(pr_mod)
    actualtimebounds_obs = TimeBounds(pr_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        pr_mod, Method, keyerror_mod = PreProcessTS(
            pr_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        pr_obs, _, keyerror_obs = PreProcessTS(
            pr_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            pr_mod, keyerror_mod = AverageZonal(pr_mod)
            pr_obs, keyerror_obs = AverageZonal(pr_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(pr_mod, pr_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(pr_mod, pr_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(pr_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(pr_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(pr_mod), "observations": ArrayToList(pr_obs),
                                  "axis": list(pr_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitations", metric, "equatorial_pacific_LatExt2",
                        file_area=prareafilemod, name_area=prareanamemod, file_mask=prlandmaskfilemod,
                        name_mask=prlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitations", metric, "equatorial_pacific_LatExt2",
                        file_area=prareafileobs, name_area=prareanameobs, file_mask=prlandmaskfileobs,
                        name_mask=prlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean PR map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean PR across latitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean PR across latitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=pr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=pr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasPrLonRmse(prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, prfileobs,
                  prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, box, centered_rmse=0,
                  biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="", metname="",
                  **kwargs):
    """
    The BiasPrLonRmse() function computes the PR zonal (longitude) root mean square error (RMSE) in a 'box'
    (usually the Equatorial Pacific)

    Inputs:
    ------
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr, precip) in 'prfilemod'
    :param prareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for PR
    :param prareanamemod: string
        name of areacell variable (areacella, areacello) in 'prareafilemod'
    :param prlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for PR
    :param prlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfilemod'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, prec) in 'prfileobs'
    :param prareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for PR
    :param prareanameobs: string
        name of areacell variable (areacella, areacello) in 'prareafileobs'
    :param prlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for PR
    :param prlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for PR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasPrLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean pr Zonal RMSE"
    Units = "mm/day"
    Method = "Zonal root mean square error of " + box + " pr"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasPrLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    pr_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, box, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, box, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(pr_mod.shape[0] / 12))
    yearN_obs = int(round(pr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(pr_mod)
    actualtimebounds_obs = TimeBounds(pr_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        pr_mod, Method, keyerror_mod = PreProcessTS(
            pr_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        pr_obs, _, keyerror_obs = PreProcessTS(
            pr_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            pr_mod, keyerror_mod = AverageMeridional(pr_mod)
            pr_obs, keyerror_obs = AverageMeridional(pr_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(pr_mod, pr_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(pr_mod, pr_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(pr_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(pr_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(pr_mod), "observations": ArrayToList(pr_obs),
                                  "axis": list(pr_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitations", metric, "equatorial_pacific_LatExt2",
                        file_area=prareafilemod, name_area=prareanamemod, file_mask=prlandmaskfilemod,
                        name_mask=prlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitations", metric, "equatorial_pacific_LatExt2",
                        file_area=prareafileobs, name_area=prareanameobs, file_mask=prlandmaskfileobs,
                        name_mask=prlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean PR map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean PR across longitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean PR across longitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=pr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=pr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLhfMapRmse(lhffilemod, lhfnamemod, lhfareafilemod, lhfareanamemod, lhflandmaskfilemod, lhflandmasknamemod,
                   lhffileobs, lhfnameobs, lhfareafileobs, lhfareanameobs, lhflandmaskfileobs, lhflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLhfMapRmse() function computes the LHF spatial root mean square error (RMSE) in a 'box' (usually the
    tropical Pacific)

    Inputs:
    ------
    :param lhffilemod: string
        path_to/filename of the file (NetCDF) of the modeled LHF
    :param lhfnamemod: string
        name of LHF variable (lhf, hfls) in 'lhffilemod'
    :param lhfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LHF
    :param lhfareanamemod: string
        name of areacell variable (areacella, areacello) in 'lhfareafilemod'
    :param lhflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LHF
    :param lhflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfilemod'
    :param lhffileobs: string
        path_to/filename of the file (NetCDF) of the observed LHF
    :param lhfnameobs: string
        name of LHF variable (lhf, hfls) in 'lhffileobs'
    :param lhfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LHF
    :param lhfareanameobs: string
        name of areacell variable (areacella, areacello) in 'lhfareafileobs'
    :param lhflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LHF
    :param lhflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for LHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadILHF',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLhfMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean lhf RMSE"
    Units = "W/m2"
    Method = "Spatial root mean square error of " + box + " lhf"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLhfMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    lhf_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        lhffilemod, lhfnamemod, "heat flux", metric, box, file_area=lhfareafilemod, name_area=lhfareanamemod,
        file_mask=lhflandmaskfilemod, name_mask=lhflandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    lhf_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        lhffileobs, lhfnameobs, "heat flux", metric, box, file_area=lhfareafileobs, name_area=lhfareanameobs,
        file_mask=lhflandmaskfileobs, name_mask=lhflandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(lhf_mod.shape[0] / 12))
    yearN_obs = int(round(lhf_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(lhf_mod)
    actualtimebounds_obs = TimeBounds(lhf_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        lhf_mod, Method, keyerror_mod = PreProcessTS(
            lhf_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        lhf_obs, _, keyerror_obs = PreProcessTS(
            lhf_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                lhf_mod, lhf_obs, Method = TwoVarRegrid(lhf_mod, lhf_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsHorizontal(lhf_mod, lhf_obs, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(lhf_mod, lhf_obs, axis="xy", centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(lhf_mod, weights=None, axis="xy", centered=1, biased=1)
            sm_std_obs = Std(lhf_obs, weights=None, axis="xy", centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "spatialSTD": sm_std_mod, "description": "mean LHF map"}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "spatialSTD": sm_std_obs, "description": "mean LHF map"}
                dict3 = {"metric_name": Name, "metric_value_" + dataset2: mv,
                         "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2: sm_corr,
                         "CORR_error_" + dataset2: sm_corr_error, "STD_" + dataset2: sm_std,
                         "STD_error_" + dataset2: sm_std_error, "metric_method": Method, "metric_reference": Ref,
                         "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, global_attributes=dict3,
                           var1=lhf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                           var2=lhf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLhfLatRmse(lhffilemod, lhfnamemod, lhfareafilemod, lhfareanamemod, lhflandmaskfilemod, lhflandmasknamemod,
                   lhffileobs, lhfnameobs, lhfareafileobs, lhfareanameobs, lhflandmaskfileobs, lhflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLhfLatRmse() function computes the LHF meridional (latitude) root mean square error (RMSE) in a 'box'
    (usually 'nino3_LatExt')

    Inputs:
    ------
    :param lhffilemod: string
        path_to/filename of the file (NetCDF) of the modeled LHF
    :param lhfnamemod: string
        name of LHF variable (lhf, hfls) in 'lhffilemod'
    :param lhfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LHF
    :param lhfareanamemod: string
        name of areacell variable (areacella, areacello) in 'lhfareafilemod'
    :param lhflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LHF
    :param lhflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfilemod'
    :param lhffileobs: string
        path_to/filename of the file (NetCDF) of the observed LHF
    :param lhfnameobs: string
        name of LHF variable (lhf, hfls) in 'lhffileobs'
    :param lhfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LHF
    :param lhfareanameobs: string
        name of areacell variable (areacella, areacello) in 'lhfareafileobs'
    :param lhflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LHF
    :param lhflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for LHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadILHF',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLhfLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean lhf Meridional RMSE"
    Units = "W/m2"
    Method = "Meridional root mean square error of " + box + " lhf"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLhfLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    lhf_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        lhffilemod, lhfnamemod, "heat flux", metric, box, file_area=lhfareafilemod, name_area=lhfareanamemod,
        file_mask=lhflandmaskfilemod, name_mask=lhflandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    lhf_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        lhffileobs, lhfnameobs, "heat flux", metric, box, file_area=lhfareafileobs, name_area=lhfareanameobs,
        file_mask=lhflandmaskfileobs, name_mask=lhflandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(lhf_mod.shape[0] / 12))
    yearN_obs = int(round(lhf_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(lhf_mod)
    actualtimebounds_obs = TimeBounds(lhf_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        lhf_mod, Method, keyerror_mod = PreProcessTS(
            lhf_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        lhf_obs, _, keyerror_obs = PreProcessTS(
            lhf_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                lhf_mod, lhf_obs, Method = TwoVarRegrid(lhf_mod, lhf_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            lhf_mod, keyerror_mod = AverageZonal(lhf_mod)
            lhf_obs, keyerror_obs = AverageZonal(lhf_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(lhf_mod, lhf_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(lhf_mod, lhf_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(lhf_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(lhf_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(lhf_mod), "observations": ArrayToList(lhf_obs),
                                  "axis": list(lhf_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        lhffilemod, lhfnamemod, "heat flux", metric, "equatorial_pacific_LatExt2",
                        file_area=lhfareafilemod, name_area=lhfareanamemod, file_mask=lhflandmaskfilemod,
                        name_mask=lhflandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        lhffileobs, lhfnameobs, "heat flux", metric, "equatorial_pacific_LatExt2",
                        file_area=lhfareafileobs, name_area=lhfareanameobs, file_mask=lhflandmaskfileobs,
                        name_mask=lhflandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean LHF map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean LHF across latitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean LHF across latitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=lhf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=lhf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLhfLonRmse(lhffilemod, lhfnamemod, lhfareafilemod, lhfareanamemod, lhflandmaskfilemod, lhflandmasknamemod,
                   lhffileobs, lhfnameobs, lhfareafileobs, lhfareanameobs, lhflandmaskfileobs, lhflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLhfLonRmse() function computes the LHF zonal (longitude) root mean square error (RMSE) in a 'box'
    (usually the Equatorial Pacific)

    Inputs:
    ------
    :param lhffilemod: string
        path_to/filename of the file (NetCDF) of the modeled LHF
    :param lhfnamemod: string
        name of LHF variable (lhf, hfls) in 'lhffilemod'
    :param lhfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LHF
    :param lhfareanamemod: string
        name of areacell variable (areacella, areacello) in 'lhfareafilemod'
    :param lhflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LHF
    :param lhflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfilemod'
    :param lhffileobs: string
        path_to/filename of the file (NetCDF) of the observed LHF
    :param lhfnameobs: string
        name of LHF variable (lhf, hfls) in 'lhffileobs'
    :param lhfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LHF
    :param lhfareanameobs: string
        name of areacell variable (areacella, areacello) in 'lhfareafileobs'
    :param lhflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LHF
    :param lhflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for LHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadILHF',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLhfLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean lhf Zonal RMSE"
    Units = "W/m2"
    Method = "Zonal root mean square error of " + box + " lhf"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLhfLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    lhf_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        lhffilemod, lhfnamemod, "heat flux", metric, box, file_area=lhfareafilemod, name_area=lhfareanamemod,
        file_mask=lhflandmaskfilemod, name_mask=lhflandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    lhf_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        lhffileobs, lhfnameobs, "heat flux", metric, box, file_area=lhfareafileobs, name_area=lhfareanameobs,
        file_mask=lhflandmaskfileobs, name_mask=lhflandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(lhf_mod.shape[0] / 12))
    yearN_obs = int(round(lhf_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(lhf_mod)
    actualtimebounds_obs = TimeBounds(lhf_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        lhf_mod, Method, keyerror_mod = PreProcessTS(
            lhf_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        lhf_obs, _, keyerror_obs = PreProcessTS(
            lhf_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                lhf_mod, lhf_obs, Method = TwoVarRegrid(lhf_mod, lhf_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            lhf_mod, keyerror_mod = AverageMeridional(lhf_mod)
            lhf_obs, keyerror_obs = AverageMeridional(lhf_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lhf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in lhf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(lhf_mod.shape), "shape2": "(obs) " + str(lhf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(lhf_mod, lhf_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(lhf_mod, lhf_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(lhf_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(lhf_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(lhf_mod), "observations": ArrayToList(lhf_obs),
                                  "axis": list(lhf_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        lhffilemod, lhfnamemod, "heat flux", metric, "equatorial_pacific_LatExt2",
                        file_area=lhfareafilemod, name_area=lhfareanamemod, file_mask=lhflandmaskfilemod,
                        name_mask=lhflandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        lhffileobs, lhfnameobs, "heat flux", metric, "equatorial_pacific_LatExt2",
                        file_area=lhfareafileobs, name_area=lhfareanameobs, file_mask=lhflandmaskfileobs,
                        name_mask=lhflandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean LHF map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean LHF across longitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean LHF across longitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=lhf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=lhf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLwrMapRmse(lwrfilemod, lwrnamemod, lwrareafilemod, lwrareanamemod, lwrlandmaskfilemod, lwrlandmasknamemod,
                   lwrfileobs, lwrnameobs, lwrareafileobs, lwrareanameobs, lwrlandmaskfileobs, lwrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLwrMapRmse() function computes the LWR spatial root mean square error (RMSE) in a 'box' (usually the
    tropical Pacific)

    Inputs:
    ------
    :param lwrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled LWR
    :param lwrnamemod: string
        name of LWR variable (lwr, rlds - rlus) in 'lwrfilemod'
    :param lwrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LWR
    :param lwrareanamemod: string
        name of areacell variable (areacella, areacello) in 'lwrareafilemod'
    :param lwrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LWR
    :param lwrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfilemod'
    :param lwrfileobs: string
        path_to/filename of the file (NetCDF) of the observed LWR
    :param lwrnameobs: string
        name of LWR variable (lwr, rlds - rlus) in 'lwrfileobs'
    :param lwrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LWR
    :param lwrareanameobs: string
        name of areacell variable (areacella, areacello) in 'lwrareafileobs'
    :param lwrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LWR
    :param lwrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for LWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadILWR',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLwrMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean lwr RMSE"
    Units = "W/m2"
    Method = "Spatial root mean square error of " + box + " lwr"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLwrMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    lwr_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
        lwrfilemod, lwrnamemod, "heat flux", "lwr", metric, box, file_area=lwrareafilemod, name_area=lwrareanamemod,
        file_mask=lwrlandmaskfilemod, name_mask=lwrlandmasknamemod, maskland=True, maskocean=False,
        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    lwr_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
        lwrfileobs, lwrnameobs, "heat flux", "lwr", metric, box, file_area=lwrareafileobs, name_area=lwrareanameobs,
        file_mask=lwrlandmaskfileobs, name_mask=lwrlandmasknameobs, maskland=True, maskocean=False,
        interpreter="project_interpreter_obs", time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(lwr_mod.shape[0] / 12))
    yearN_obs = int(round(lwr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(lwr_mod)
    actualtimebounds_obs = TimeBounds(lwr_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        lwr_mod, Method, keyerror_mod = PreProcessTS(
            lwr_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        lwr_obs, _, keyerror_obs = PreProcessTS(
            lwr_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                lwr_mod, lwr_obs, Method = TwoVarRegrid(lwr_mod, lwr_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsHorizontal(lwr_mod, lwr_obs, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(lwr_mod, lwr_obs, axis="xy", centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(lwr_mod, weights=None, axis="xy", centered=1, biased=1)
            sm_std_obs = Std(lwr_obs, weights=None, axis="xy", centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "spatialSTD": sm_std_mod, "description": "mean LWR map"}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "spatialSTD": sm_std_obs, "description": "mean LWR map"}
                dict3 = {"metric_name": Name, "metric_value_" + dataset2: mv,
                         "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2: sm_corr,
                         "CORR_error_" + dataset2: sm_corr_error, "STD_" + dataset2: sm_std,
                         "STD_error_" + dataset2: sm_std_error, "metric_method": Method, "metric_reference": Ref,
                         "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, global_attributes=dict3,
                           var1=lwr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                           var2=lwr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLwrLatRmse(lwrfilemod, lwrnamemod, lwrareafilemod, lwrareanamemod, lwrlandmaskfilemod, lwrlandmasknamemod,
                   lwrfileobs, lwrnameobs, lwrareafileobs, lwrareanameobs, lwrlandmaskfileobs, lwrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLwrLatRmse() function computes the LWR meridional (latitude) root mean square error (RMSE) in a 'box'
    (usually 'nino3_LatExt')

    Inputs:
    ------
    :param lwrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled LWR
    :param lwrnamemod: string
        name of LWR variable (lwr, rlds - rlus) in 'lwrfilemod'
    :param lwrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LWR
    :param lwrareanamemod: string
        name of areacell variable (areacella, areacello) in 'lwrareafilemod'
    :param lwrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LWR
    :param lwrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfilemod'
    :param lwrfileobs: string
        path_to/filename of the file (NetCDF) of the observed LWR
    :param lwrnameobs: string
        name of LWR variable (lwr, rlds - rlus) in 'lwrfileobs'
    :param lwrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LWR
    :param lwrareanameobs: string
        name of areacell variable (areacella, areacello) in 'lwrareafileobs'
    :param lwrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LWR
    :param lwrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for LWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadILWR',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLwrLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean lwr Meridional RMSE"
    Units = "W/m2"
    Method = "Meridional root mean square error of " + box + " lwr"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLwrLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    lwr_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
        lwrfilemod, lwrnamemod, "heat flux", "lwr", metric, box, file_area=lwrareafilemod, name_area=lwrareanamemod,
        file_mask=lwrlandmaskfilemod, name_mask=lwrlandmasknamemod, maskland=True, maskocean=False,
        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    lwr_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
        lwrfileobs, lwrnameobs, "heat flux", "lwr", metric, box, file_area=lwrareafileobs, name_area=lwrareanameobs,
        file_mask=lwrlandmaskfileobs, name_mask=lwrlandmasknameobs, maskland=True, maskocean=False,
        interpreter="project_interpreter_obs", time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(lwr_mod.shape[0] / 12))
    yearN_obs = int(round(lwr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(lwr_mod)
    actualtimebounds_obs = TimeBounds(lwr_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        lwr_mod, Method, keyerror_mod = PreProcessTS(
            lwr_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        lwr_obs, _, keyerror_obs = PreProcessTS(
            lwr_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                lwr_mod, lwr_obs, Method = TwoVarRegrid(lwr_mod, lwr_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            lwr_mod, keyerror_mod = AverageZonal(lwr_mod)
            lwr_obs, keyerror_obs = AverageZonal(lwr_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(lwr_mod, lwr_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(lwr_mod, lwr_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(lwr_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(lwr_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(lwr_mod), "observations": ArrayToList(lwr_obs),
                                  "axis": list(lwr_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                        lwrfilemod, lwrnamemod, "heat flux", "lwr", metric, "equatorial_pacific_LatExt2",
                        file_area=lwrareafilemod, name_area=lwrareanamemod, file_mask=lwrlandmaskfilemod,
                        name_mask=lwrlandmasknamemod, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"],
                        debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                        lwrfileobs, lwrnameobs, "heat flux", "lwr", metric, "equatorial_pacific_LatExt2",
                        file_area=lwrareafileobs, name_area=lwrareanameobs, file_mask=lwrlandmaskfileobs,
                        name_mask=lwrlandmasknameobs, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_obs"],
                        debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean LWR map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean LWR across latitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean LWR across latitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=lwr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=lwr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasLwrLonRmse(lwrfilemod, lwrnamemod, lwrareafilemod, lwrareanamemod, lwrlandmaskfilemod, lwrlandmasknamemod,
                   lwrfileobs, lwrnameobs, lwrareafileobs, lwrareanameobs, lwrlandmaskfileobs, lwrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasLwrLonRmse() function computes the LWR zonal (longitude) root mean square error (RMSE) in a 'box'
    (usually the Equatorial Pacific)

    Inputs:
    ------
    :param lwrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled LWR
    :param lwrnamemod: string
        name of LWR variable (lwr, rlds - rlus) in 'lwrfilemod'
    :param lwrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for LWR
    :param lwrareanamemod: string
        name of areacell variable (areacella, areacello) in 'lwrareafilemod'
    :param lwrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for LWR
    :param lwrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfilemod'
    :param lwrfileobs: string
        path_to/filename of the file (NetCDF) of the observed LWR
    :param lwrnameobs: string
        name of LWR variable (lwr, rlds - rlus) in 'lwrfileobs'
    :param lwrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for LWR
    :param lwrareanameobs: string
        name of areacell variable (areacella, areacello) in 'lwrareafileobs'
    :param lwrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for LWR
    :param lwrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for LWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadILWR',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasLwrLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed LWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean lwr Zonal RMSE"
    Units = "W/m2"
    Method = "Zonal root mean square error of " + box + " lwr"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasLwrLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    lwr_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
        lwrfilemod, lwrnamemod, "heat flux", "lwr", metric, box, file_area=lwrareafilemod, name_area=lwrareanamemod,
        file_mask=lwrlandmaskfilemod, name_mask=lwrlandmasknamemod, maskland=True, maskocean=False,
        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    lwr_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
        lwrfileobs, lwrnameobs, "heat flux", "lwr", metric, box, file_area=lwrareafileobs, name_area=lwrareanameobs,
        file_mask=lwrlandmaskfileobs, name_mask=lwrlandmasknameobs, maskland=True, maskocean=False,
        interpreter="project_interpreter_obs", time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(lwr_mod.shape[0] / 12))
    yearN_obs = int(round(lwr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(lwr_mod)
    actualtimebounds_obs = TimeBounds(lwr_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        lwr_mod, Method, keyerror_mod = PreProcessTS(
            lwr_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        lwr_obs, _, keyerror_obs = PreProcessTS(
            lwr_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                lwr_mod, lwr_obs, Method = TwoVarRegrid(lwr_mod, lwr_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            lwr_mod, keyerror_mod = AverageMeridional(lwr_mod)
            lwr_obs, keyerror_obs = AverageMeridional(lwr_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in lwr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in lwr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(lwr_mod.shape), "shape2": "(obs) " + str(lwr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(lwr_mod, lwr_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(lwr_mod, lwr_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(lwr_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(lwr_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(lwr_mod), "observations": ArrayToList(lwr_obs),
                                  "axis": list(lwr_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                        lwrfilemod, lwrnamemod, "heat flux", "lwr", metric, "equatorial_pacific_LatExt2",
                        file_area=lwrareafilemod, name_area=lwrareanamemod, file_mask=lwrlandmaskfilemod,
                        name_mask=lwrlandmasknamemod, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"],
                        debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                        lwrfileobs, lwrnameobs, "heat flux", "lwr", metric, "equatorial_pacific_LatExt2",
                        file_area=lwrareafileobs, name_area=lwrareanameobs, file_mask=lwrlandmaskfileobs,
                        name_mask=lwrlandmasknameobs, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_obs"],
                        debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean LWR map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean LWR across longitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean LWR across longitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=lwr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=lwr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasShfMapRmse(shffilemod, shfnamemod, shfareafilemod, shfareanamemod, shflandmaskfilemod, shflandmasknamemod,
                   shffileobs, shfnameobs, shfareafileobs, shfareanameobs, shflandmaskfileobs, shflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasShfMapRmse() function computes the SHF spatial root mean square error (RMSE) in a 'box' (usually the
    tropical Pacific)

    Inputs:
    ------
    :param shffilemod: string
        path_to/filename of the file (NetCDF) of the modeled SHF
    :param shfnamemod: string
        name of SHF variable (shf, hfss) in 'shffilemod'
    :param shfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SHF
    :param shfareanamemod: string
        name of areacell variable (areacella, areacello) in 'shfareafilemod'
    :param shflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SHF
    :param shflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfilemod'
    :param shffileobs: string
        path_to/filename of the file (NetCDF) of the observed SHF
    :param shfnameobs: string
        name of SHF variable (shf, hfss) in 'shffileobs'
    :param shfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SHF
    :param shfareanameobs: string
        name of areacell variable (areacella, areacello) in 'shfareafileobs'
    :param shflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SHF
    :param shflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for SHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISHF',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasShfMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean shf RMSE"
    Units = "W/m2"
    Method = "Spatial root mean square error of " + box + " shf"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasShfMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    shf_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        shffilemod, shfnamemod, "heat flux", metric, box, file_area=shfareafilemod, name_area=shfareanamemod,
        file_mask=shflandmaskfilemod, name_mask=shflandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    shf_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        shffileobs, shfnameobs, "heat flux", metric, box, file_area=shfareafileobs, name_area=shfareanameobs,
        file_mask=shflandmaskfileobs, name_mask=shflandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(shf_mod.shape[0] / 12))
    yearN_obs = int(round(shf_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(shf_mod)
    actualtimebounds_obs = TimeBounds(shf_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        shf_mod, Method, keyerror_mod = PreProcessTS(
            shf_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        shf_obs, _, keyerror_obs = PreProcessTS(
            shf_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                shf_mod, shf_obs, Method = TwoVarRegrid(shf_mod, shf_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsHorizontal(shf_mod, shf_obs, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(shf_mod, shf_obs, axis="xy", centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(shf_mod, weights=None, axis="xy", centered=1, biased=1)
            sm_std_obs = Std(shf_obs, weights=None, axis="xy", centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "spatialSTD": sm_std_mod, "description": "mean SHF map"}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "spatialSTD": sm_std_obs, "description": "mean SHF map"}
                dict3 = {"metric_name": Name, "metric_value_" + dataset2: mv,
                         "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2: sm_corr,
                         "CORR_error_" + dataset2: sm_corr_error, "STD_" + dataset2: sm_std,
                         "STD_error_" + dataset2: sm_std_error, "metric_method": Method, "metric_reference": Ref,
                         "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, global_attributes=dict3,
                           var1=shf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                           var2=shf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasShfLatRmse(shffilemod, shfnamemod, shfareafilemod, shfareanamemod, shflandmaskfilemod, shflandmasknamemod,
                   shffileobs, shfnameobs, shfareafileobs, shfareanameobs, shflandmaskfileobs, shflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasShfLatRmse() function computes the SHF meridional (latitude) root mean square error (RMSE) in a 'box'
    (usually 'nino3_LatExt')

    Inputs:
    ------
    :param shffilemod: string
        path_to/filename of the file (NetCDF) of the modeled SHF
    :param shfnamemod: string
        name of SHF variable (shf, hfss) in 'shffilemod'
    :param shfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SHF
    :param shfareanamemod: string
        name of areacell variable (areacella, areacello) in 'shfareafilemod'
    :param shflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SHF
    :param shflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfilemod'
    :param shffileobs: string
        path_to/filename of the file (NetCDF) of the observed SHF
    :param shfnameobs: string
        name of SHF variable (shf, hfss) in 'shffileobs'
    :param shfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SHF
    :param shfareanameobs: string
        name of areacell variable (areacella, areacello) in 'shfareafileobs'
    :param shflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SHF
    :param shflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for SHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISHF',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasShfLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean shf Meridional RMSE"
    Units = "W/m2"
    Method = "Meridional root mean square error of " + box + " shf"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasShfLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    shf_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        shffilemod, shfnamemod, "heat flux", metric, box, file_area=shfareafilemod, name_area=shfareanamemod,
        file_mask=shflandmaskfilemod, name_mask=shflandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    shf_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        shffileobs, shfnameobs, "heat flux", metric, box, file_area=shfareafileobs, name_area=shfareanameobs,
        file_mask=shflandmaskfileobs, name_mask=shflandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(shf_mod.shape[0] / 12))
    yearN_obs = int(round(shf_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(shf_mod)
    actualtimebounds_obs = TimeBounds(shf_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        shf_mod, Method, keyerror_mod = PreProcessTS(
            shf_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        shf_obs, _, keyerror_obs = PreProcessTS(
            shf_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                shf_mod, shf_obs, Method = TwoVarRegrid(shf_mod, shf_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            shf_mod, keyerror_mod = AverageZonal(shf_mod)
            shf_obs, keyerror_obs = AverageZonal(shf_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(shf_mod, shf_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(shf_mod, shf_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(shf_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(shf_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(shf_mod), "observations": ArrayToList(shf_obs),
                                  "axis": list(shf_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        shffilemod, shfnamemod, "heat flux", metric, "equatorial_pacific_LatExt2",
                        file_area=shfareafilemod, name_area=shfareanamemod, file_mask=shflandmaskfilemod,
                        name_mask=shflandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        shffileobs, shfnameobs, "heat flux", metric, "equatorial_pacific_LatExt2",
                        file_area=shfareafileobs, name_area=shfareanameobs, file_mask=shflandmaskfileobs,
                        name_mask=shflandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean SHF map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean SHF across latitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean SHF across latitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=shf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=shf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasShfLonRmse(shffilemod, shfnamemod, shfareafilemod, shfareanamemod, shflandmaskfilemod, shflandmasknamemod,
                   shffileobs, shfnameobs, shfareafileobs, shfareanameobs, shflandmaskfileobs, shflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasShfLonRmse() function computes the SHF zonal (longitude) root mean square error (RMSE) in a 'box'
    (usually the Equatorial Pacific)

    Inputs:
    ------
    :param shffilemod: string
        path_to/filename of the file (NetCDF) of the modeled SHF
    :param shfnamemod: string
        name of SHF variable (shf, hfss) in 'shffilemod'
    :param shfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SHF
    :param shfareanamemod: string
        name of areacell variable (areacella, areacello) in 'shfareafilemod'
    :param shflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SHF
    :param shflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfilemod'
    :param shffileobs: string
        path_to/filename of the file (NetCDF) of the observed SHF
    :param shfnameobs: string
        name of SHF variable (shf, hfss) in 'shffileobs'
    :param shfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SHF
    :param shfareanameobs: string
        name of areacell variable (areacella, areacello) in 'shfareafileobs'
    :param shflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SHF
    :param shflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SHF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISHF',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasShfLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SHF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean shf Zonal RMSE"
    Units = "W/m2"
    Method = "Zonal root mean square error of " + box + " shf"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasShfLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    shf_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        shffilemod, shfnamemod, "heat flux", metric, box, file_area=shfareafilemod, name_area=shfareanamemod,
        file_mask=shflandmaskfilemod, name_mask=shflandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    shf_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        shffileobs, shfnameobs, "heat flux", metric, box, file_area=shfareafileobs, name_area=shfareanameobs,
        file_mask=shflandmaskfileobs, name_mask=shflandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(shf_mod.shape[0] / 12))
    yearN_obs = int(round(shf_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(shf_mod)
    actualtimebounds_obs = TimeBounds(shf_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        shf_mod, Method, keyerror_mod = PreProcessTS(
            shf_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        shf_obs, _, keyerror_obs = PreProcessTS(
            shf_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                shf_mod, shf_obs, Method = TwoVarRegrid(shf_mod, shf_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            shf_mod, keyerror_mod = AverageMeridional(shf_mod)
            shf_obs, keyerror_obs = AverageMeridional(shf_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in shf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in shf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(shf_mod.shape), "shape2": "(obs) " + str(shf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(shf_mod, shf_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(shf_mod, shf_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(shf_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(shf_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(shf_mod), "observations": ArrayToList(shf_obs),
                                  "axis": list(shf_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        shffilemod, shfnamemod, "heat flux", metric, "equatorial_pacific_LatExt2",
                        file_area=shfareafilemod, name_area=shfareanamemod, file_mask=shflandmaskfilemod,
                        name_mask=shflandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        shffileobs, shfnameobs, "heat flux", metric, "equatorial_pacific_LatExt2",
                        file_area=shfareafileobs, name_area=shfareanameobs, file_mask=shflandmaskfileobs,
                        name_mask=shflandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean SHF map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean SHF across longitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean SHF across longitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=shf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=shf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSshMapRmse(sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                   sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSshMapRmse() function computes the SSH spatial root mean square error (RMSE) in a 'box' (usually the
    tropical Pacific)

    Inputs:
    ------
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SSH
    :param sshareanamemod: string
        name of areacell variable (areacella, areacello) in 'sshareafilemod'
    :param sshlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SSH
    :param sshlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfilemod'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SSH
    :param sshareanameobs: string
        name of areacell variable (areacella, areacello) in 'sshareafileobs'
    :param sshlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SSH
    :param sshlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for SSH
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'AVISO',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSshMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean ssh RMSE"
    Units = "cm"
    Method = "Spatial root mean square error of " + box + " ssh"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSshMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    ssh_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sshfilemod, sshnamemod, "sea surface height", metric, box, file_area=sshareafilemod, name_area=sshareanamemod,
        file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    ssh_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sshfileobs, sshnameobs, "sea surface height", metric, box, file_area=sshareafileobs, name_area=sshareanameobs,
        file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(ssh_mod.shape[0] / 12))
    yearN_obs = int(round(ssh_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(ssh_mod)
    actualtimebounds_obs = TimeBounds(ssh_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        ssh_mod, Method, keyerror_mod = PreProcessTS(
            ssh_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        ssh_obs, _, keyerror_obs = PreProcessTS(
            ssh_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                ssh_mod, ssh_obs, Method = TwoVarRegrid(ssh_mod, ssh_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # change units
            ssh_mod = ssh_mod * 1e2
            ssh_obs = ssh_obs * 1e2

            # Computes the root mean square difference
            mv, keyerror = RmsHorizontal(ssh_mod, ssh_obs, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(ssh_mod, ssh_obs, axis="xy", centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(ssh_mod, weights=None, axis="xy", centered=1, biased=1)
            sm_std_obs = Std(ssh_obs, weights=None, axis="xy", centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "spatialSTD": sm_std_mod, "description": "mean SSH map"}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "spatialSTD": sm_std_obs, "description": "mean SSH map"}
                dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                         "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                         "CORR_" + dataset2: sm_corr, "CORR_error_" + dataset2: sm_corr_error,
                         "STD_" + dataset2: sm_std, "STD_error_" + dataset2: sm_std_error,
                         "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, global_attributes=dict3,
                           var1=ssh_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                           var2=ssh_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSshLatRmse(sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                   sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSshLatRmse() function computes the SSH meridional (latitude) root mean square error (RMSE) in a 'box'
    (usually 'nino3_LatExt')

    Inputs:
    ------
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SSH
    :param sshareanamemod: string
        name of areacell variable (areacella, areacello) in "sshareafilemod'
    :param sshlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SSH
    :param sshlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfilemod'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SSH
    :param sshareanameobs: string
        name of areacell variable (areacella, areacello) in 'sshareafileobs'
    :param sshlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SSH
    :param sshlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for SSH
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'AVISO',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSshLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean ssh Meridional RMSE"
    Units = "cm"
    Method = "Meridional root mean square error of " + box + " ssh"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSshLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    ssh_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sshfilemod, sshnamemod, "sea surface height", metric, box, file_area=sshareafilemod, name_area=sshareanamemod,
        file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    ssh_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sshfileobs, sshnameobs, "sea surface height", metric, box, file_area=sshareafileobs, name_area=sshareanameobs,
        file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(ssh_mod.shape[0] / 12))
    yearN_obs = int(round(ssh_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(ssh_mod)
    actualtimebounds_obs = TimeBounds(ssh_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        ssh_mod, Method, keyerror_mod = PreProcessTS(
            ssh_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        ssh_obs, _, keyerror_obs = PreProcessTS(
            ssh_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                ssh_mod, ssh_obs, Method = TwoVarRegrid(ssh_mod, ssh_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            ssh_mod, keyerror_mod = AverageZonal(ssh_mod)
            ssh_obs, keyerror_obs = AverageZonal(ssh_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                ssh_mod = ssh_mod * 1e2
                ssh_obs = ssh_obs * 1e2
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(ssh_mod, ssh_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(ssh_mod, ssh_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(ssh_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(ssh_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(ssh_mod), "observations": ArrayToList(ssh_obs),
                                  "axis": list(ssh_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific_LatExt2",
                        file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                        name_mask=sshlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific_LatExt2",
                        file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                        name_mask=sshlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # change units
                            map_mod = map_mod * 1e2
                            map_obs = map_obs * 1e2
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean SSH map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean SSH across latitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean SSH across latitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=ssh_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=ssh_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSshLonRmse(sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                   sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSshLonRmse() function computes the SSH zonal (longitude) root mean square error (RMSE) in a 'box'
    (usually the Equatorial Pacific)

    Inputs:
    ------
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SSH
    :param sshareanamemod: string
        name of areacell variable (areacella, areacello) in 'sshareafilemod'
    :param sshlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SSH
    :param sshlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfilemod'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SSH
    :param sshareanameobs: string
        name of areacell variable (areacella, areacello) in 'sshareafileobs'
    :param sshlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SSH
    :param sshlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SSH
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'AVISO',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSshLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean ssh Zonal RMSE"
    Units = "cm"
    Method = "Zonal root mean square error of " + box + " ssh"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSshLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    ssh_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sshfilemod, sshnamemod, "sea surface height", metric, box, file_area=sshareafilemod, name_area=sshareanamemod,
        file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    ssh_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sshfileobs, sshnameobs, "sea surface height", metric, box, file_area=sshareafileobs, name_area=sshareanameobs,
        file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(ssh_mod.shape[0] / 12))
    yearN_obs = int(round(ssh_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(ssh_mod)
    actualtimebounds_obs = TimeBounds(ssh_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        ssh_mod, Method, keyerror_mod = PreProcessTS(
            ssh_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        ssh_obs, _, keyerror_obs = PreProcessTS(
            ssh_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                ssh_mod, ssh_obs, Method = TwoVarRegrid(ssh_mod, ssh_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            ssh_mod, keyerror_mod = AverageMeridional(ssh_mod)
            ssh_obs, keyerror_obs = AverageMeridional(ssh_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                ssh_mod = ssh_mod * 1e2
                ssh_obs = ssh_obs * 1e2
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                sshRmse, keyerror = RmsZonal(ssh_mod, ssh_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                sshRmseErr = None

                # Supplementary metrics
                sm_corr = float(Correlation(ssh_mod, ssh_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(ssh_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(ssh_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(ssh_mod), "observations": ArrayToList(ssh_obs),
                                  "axis": list(ssh_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific_LatExt2",
                        file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                        name_mask=sshlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific_LatExt2",
                        file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                        name_mask=sshlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # change units
                            map_mod = map_mod * 1e2
                            map_obs = map_obs * 1e2
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean SSH map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean SSH across longitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean SSH across longitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=ssh_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=ssh_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSstMapRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSstMapRmse() function computes the SST spatial root mean square error (RMSE) in a 'box' (usually the
    tropical Pacific)

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSstMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean sst RMSE"
    Units = "C"
    Method = "Spatial root mean square error of " + box + " sst"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSstMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        sst_mod, Method, keyerror_mod = PreProcessTS(
            sst_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sst_mod, sst_obs, Method = TwoVarRegrid(sst_mod, sst_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsHorizontal(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(sst_mod, sst_obs, axis="xy", centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(sst_mod, weights=None, axis="xy", centered=1, biased=1)
            sm_std_obs = Std(sst_obs, weights=None, axis="xy", centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "spatialSTD": sm_std_mod, "description": "mean SST map"}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "spatialSTD": sm_std_obs, "description": "mean SST map"}
                dict3 = {"metric_name": Name, "metric_value_" + dataset2: mv,
                         "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2: sm_corr,
                         "CORR_error_" + dataset2: sm_corr_error, "STD_" + dataset2: sm_std,
                         "STD_error_" + dataset2: sm_std_error, "metric_method": Method, "metric_reference": Ref,
                         "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, global_attributes=dict3,
                           var1=sst_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                           var2=sst_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSstLatRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSstLatRmse() function computes the SST meridional (latitude) root mean square error (RMSE) in a 'box'
    (usually 'nino3_LatExt')

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSstLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean sst Meridional RMSE"
    Units = "C"
    Method = "Meridional root mean square error of " + box + " sst"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSstLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        sst_mod, Method, keyerror_mod = PreProcessTS(
            sst_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sst_mod, sst_obs, Method = TwoVarRegrid(sst_mod, sst_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            sst_mod, keyerror_mod = AverageZonal(sst_mod)
            sst_obs, keyerror_obs = AverageZonal(sst_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(sst_mod, sst_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(sst_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(sst_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(sst_mod), "observations": ArrayToList(sst_obs),
                                  "axis": list(sst_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific_LatExt2",
                        file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                        name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific_LatExt2",
                        file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                        name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean SST map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean SST across latitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean SST across latitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=sst_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=sst_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSstLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSstLonRmse() function computes the SST zonal (longitude) root mean square error (RMSE) in a 'box'
    (usually the Equatorial Pacific)

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSstLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean sst Zonal RMSE"
    Units = "C"
    Method = "Zonal root mean square error of " + box + " sst"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSstLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        sst_mod, Method, keyerror_mod = PreProcessTS(
            sst_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sst_mod, sst_obs, Method = TwoVarRegrid(sst_mod, sst_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            sst_mod, keyerror_mod = AverageMeridional(sst_mod)
            sst_obs, keyerror_obs = AverageMeridional(sst_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(sst_mod, sst_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(sst_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(sst_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(sst_mod), "observations": ArrayToList(sst_obs),
                                  "axis": list(sst_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific_LatExt2",
                        file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                        name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific_LatExt2",
                        file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                        name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean SST map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean SST across longitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean SST across longitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=sst_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=sst_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSstSkLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                     sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                     box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                     netcdf_name="", metname="", **kwargs):
    """
    The BiasSstSkLonRmse() function computes the SST zonal (longitude) skewness and then its root mean square error
    (RMSE) in a 'box' (usually the Equatorial Pacific)

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSstSkLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'regridding', 'smoothing',
                    'time_bounds_mod', 'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'sstA Skewness Zonal RMSE'
    if kwargs['normalization']:
        Units = ''
    else:
        Units = 'C'
    Method = 'Zonal root mean square error of ' + box + ' sstA skewness'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = "BiasSstSkLonRmse"
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = \
        Read_data_mask_area(sstfilemod, sstnamemod, 'temperature', metric, box, file_area=sstareafilemod,
                            name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod,
                            maskland=True, maskocean=False, time_bounds=kwargs['time_bounds_mod'], debug=debug,
                            **kwargs)
    sst_obs, obs_areacell, keyerror_obs = \
        Read_data_mask_area(sstfileobs, sstnameobs, 'temperature', metric, box, file_area=sstareafileobs,
                            name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs,
                            maskland=True, maskocean=False, time_bounds=kwargs['time_bounds_obs'], debug=debug,
                            **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        sst_mod, Method, keyerror_mod = PreProcessTS(
            sst_mod, Method, areacell=mod_areacell, average=False, compute_anom=True, region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average=False, compute_anom=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sst_mod, sst_obs, Method = TwoVarRegrid(sst_mod, sst_obs, Method, region=box, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # Meridional average
            sst_mod, keyerror_mod = AverageMeridional(sst_mod)
            sst_obs, keyerror_obs = AverageMeridional(sst_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                # skewness
                sst_mod = SkewnessTemporal(sst_mod)
                sst_obs = SkewnessTemporal(sst_obs)
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after SkewnessTemporal', 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Dive down diagnostic
                dive_down_diag = {'model': ArrayToList(sst_mod), 'observations': ArrayToList(sst_obs),
                                  'axis': list(sst_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        sstfilemod, sstnamemod, 'temperature', metric, 'equatorial_pacific_LatExt2',
                        file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                        name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        sstfileobs, sstnameobs, 'temperature', metric, 'equatorial_pacific_LatExt2',
                        file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                        name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, '', areacell=mod_areacell, average=False, compute_anom=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, '', areacell=obs_areacell, average=False, compute_anom=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in map_mod.getAxisList()]),
                                              'axes2': '(obs) ' + str([ax.id for ax in map_obs.getAxisList()]),
                                              'line1': '(mod) minmax' + str(MinMax(map_mod)),
                                              'line2': '(obs) minmax' + str(MinMax(map_obs)),
                                              'shape1': '(mod) ' + str(map_mod.shape),
                                              'shape2': '(obs) ' + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    '\033[92m', 'after PreProcessTS: netcdf', 15, **dict_debug)
                            # skewness
                            ske_map_mod = SkewnessTemporal(map_mod)
                            ske_map_obs = SkewnessTemporal(map_obs)
                            if debug is True:
                                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in ske_map_mod.getAxisList()]),
                                              'axes2': '(obs) ' + str([ax.id for ax in ske_map_obs.getAxisList()]),
                                              'line1': '(mod) minmax' + str(MinMax(ske_map_mod)),
                                              'line2': '(obs) minmax' + str(MinMax(ske_map_obs)),
                                              'shape1': '(mod) ' + str(ske_map_mod.shape),
                                              'shape2': '(obs) ' + str(ske_map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    '\033[92m', 'after SkewnessTemporal: netcdf', 15, **dict_debug)
                            # Regridding
                            if isinstance(kwargs['regridding'], dict):
                                ske_map_mod, ske_map_obs, _ = TwoVarRegrid(
                                    ske_map_mod, ske_map_obs, '', region='equatorial_pacific_LatExt2',
                                    **kwargs['regridding'])
                                if debug is True:
                                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in ske_map_mod.getAxisList()]),
                                                  'axes2': '(obs) ' + str([ax.id for ax in ske_map_obs.getAxisList()]),
                                                  'line1': '(mod) minmax' + str(MinMax(ske_map_mod)),
                                                  'line2': '(obs) minmax' + str(MinMax(ske_map_obs)),
                                                  'shape1': '(mod) ' + str(ske_map_mod.shape),
                                                  'shape2': '(obs) ' + str(ske_map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        '\033[92m', 'after TwoVarRegrid: netcdf', 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod)}
                            dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs)}
                            dict3 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod)}
                            dict4 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs)}
                            dict5 = {'metric_name': Name, 'metric_value_' + dataset2: mv,
                                     'metric_value_error_' + dataset2: mv_error, 'metric_method': Method,
                                     'metric_reference': Ref, 'frequency': kwargs['frequency']}
                            SaveNetcdf(
                                file_name, var1=sst_mod, var1_attributes=dict1, var1_name='sstSke_lon__' + dataset1,
                                var2=sst_obs, var2_attributes=dict2, var2_name='sstSke_lon__' + dataset2,
                                var3=ske_map_mod, var3_attributes=dict3, var3_name='sstSke_map__' + dataset1,
                                var4=ske_map_obs, var4_attributes=dict4, var4_name='sstSke_map__' + dataset2,
                                global_attributes=dict5)
                            del dict1, dict2, dict3, dict4, dict5
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(mv), 'line2': 'metric value_error: ' + str(mv_error)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        'name': Name, 'value': mv, 'value_error': mv_error, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return metric_output


def BiasSwrMapRmse(swrfilemod, swrnamemod, swrareafilemod, swrareanamemod, swrlandmaskfilemod, swrlandmasknamemod,
                   swrfileobs, swrnameobs, swrareafileobs, swrareanameobs, swrlandmaskfileobs, swrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSwrMapRmse() function computes the SWR spatial root mean square error (RMSE) in a 'box' (usually the
    tropical Pacific)

    Inputs:
    ------
    :param swrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SWR
    :param swrnamemod: string
        name of SWR variable (swr, rsds - rsus) in 'swrfilemod'
    :param swrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SWR
    :param swrareanamemod: string
        name of areacell variable (areacella, areacello) in 'swrareafilemod'
    :param swrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SWR
    :param swrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfilemod'
    :param swrfileobs: string
        path_to/filename of the file (NetCDF) of the observed SWR
    :param swrnameobs: string
        name of SWR variable (swr, rsds - rsus) in 'swrfileobs'
    :param swrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SWR
    :param swrareanameobs: string
        name of areacell variable (areacella, areacello) in 'swrareafileobs'
    :param swrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SWR
    :param swrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for SWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISWR',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSwrMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean swr RMSE"
    Units = "W/m2"
    Method = "Spatial root mean square error of " + box + " swr"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSwrMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    swr_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
        swrfilemod, swrnamemod, "heat flux", "swr", metric, box, file_area=swrareafilemod, name_area=swrareanamemod,
        file_mask=swrlandmaskfilemod, name_mask=swrlandmasknamemod, maskland=True, maskocean=False,
        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    swr_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
        swrfileobs, swrnameobs, "heat flux", "swr", metric, box, file_area=swrareafileobs, name_area=swrareanameobs,
        file_mask=swrlandmaskfileobs, name_mask=swrlandmasknameobs, maskland=True, maskocean=False,
        interpreter="project_interpreter_obs", time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(swr_mod.shape[0] / 12))
    yearN_obs = int(round(swr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(swr_mod)
    actualtimebounds_obs = TimeBounds(swr_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        swr_mod, Method, keyerror_mod = PreProcessTS(
            swr_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        swr_obs, _, keyerror_obs = PreProcessTS(
            swr_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                swr_mod, swr_obs, Method = TwoVarRegrid(swr_mod, swr_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsHorizontal(swr_mod, swr_obs, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(swr_mod, swr_obs, axis="xy", centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(swr_mod, weights=None, axis="xy", centered=1, biased=1)
            sm_std_obs = Std(swr_obs, weights=None, axis="xy", centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "spatialSTD": sm_std_mod, "description": "mean SWR map"}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "spatialSTD": sm_std_obs, "description": "mean SWR map"}
                dict3 = {"metric_name": Name, "metric_value_" + dataset2: mv,
                         "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2: sm_corr,
                         "CORR_error_" + dataset2: sm_corr_error, "STD_" + dataset2: sm_std,
                         "STD_error_" + dataset2: sm_std_error, "metric_method": Method, "metric_reference": Ref,
                         "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, global_attributes=dict3,
                           var1=swr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                           var2=swr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSwrLatRmse(swrfilemod, swrnamemod, swrareafilemod, swrareanamemod, swrlandmaskfilemod, swrlandmasknamemod,
                   swrfileobs, swrnameobs, swrareafileobs, swrareanameobs, swrlandmaskfileobs, swrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSwrLatRmse() function computes the SWR meridional (latitude) root mean square error (RMSE) in a 'box'
    (usually 'nino3_LatExt')

    Inputs:
    ------
    :param swrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SWR
    :param swrnamemod: string
        name of SWR variable (swr, rsds - rsus) in 'swrfilemod'
    :param swrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SWR
    :param swrareanamemod: string
        name of areacell variable (areacella, areacello) in 'swrareafilemod'
    :param swrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SWR
    :param swrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfilemod'
    :param swrfileobs: string
        path_to/filename of the file (NetCDF) of the observed SWR
    :param swrnameobs: string
        name of SWR variable (swr, rsds - rsus) in 'swrfileobs'
    :param swrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SWR
    :param swrareanameobs: string
        name of areacell variable (areacella, areacello) in 'swrareafileobs'
    :param swrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SWR
    :param swrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for SWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISWR',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSwrLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean swr Meridional RMSE"
    Units = "W/m2"
    Method = "Meridional root mean square error of " + box + " swr"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSwrLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    swr_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
        swrfilemod, swrnamemod, "heat flux", "swr", metric, box, file_area=swrareafilemod, name_area=swrareanamemod,
        file_mask=swrlandmaskfilemod, name_mask=swrlandmasknamemod, maskland=True, maskocean=False,
        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    swr_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
        swrfileobs, swrnameobs, "heat flux", "swr", metric, box, file_area=swrareafileobs, name_area=swrareanameobs,
        file_mask=swrlandmaskfileobs, name_mask=swrlandmasknameobs, maskland=True, maskocean=False,
        interpreter="project_interpreter_obs", time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(swr_mod.shape[0] / 12))
    yearN_obs = int(round(swr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(swr_mod)
    actualtimebounds_obs = TimeBounds(swr_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        swr_mod, Method, keyerror_mod = PreProcessTS(
            swr_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        swr_obs, _, keyerror_obs = PreProcessTS(
            swr_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                swr_mod, swr_obs, Method = TwoVarRegrid(swr_mod, swr_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            swr_mod, keyerror_mod = AverageZonal(swr_mod)
            swr_obs, keyerror_obs = AverageZonal(swr_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(swr_mod, swr_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(swr_mod, swr_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(swr_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(swr_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(swr_mod), "observations": ArrayToList(swr_obs),
                                  "axis": list(swr_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                        swrfilemod, swrnamemod, "heat flux", "swr", metric, "equatorial_pacific_LatExt2",
                        file_area=swrareafilemod, name_area=swrareanamemod, file_mask=swrlandmaskfilemod,
                        name_mask=swrlandmasknamemod, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"],
                        debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                        swrfileobs, swrnameobs, "heat flux", "swr", metric, "equatorial_pacific_LatExt2",
                        file_area=swrareafileobs, name_area=swrareanameobs, file_mask=swrlandmaskfileobs,
                        name_mask=swrlandmasknameobs, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_obs"],
                        debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean SWR map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean SWR across latitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean SWR across latitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=swr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=swr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasSwrLonRmse(swrfilemod, swrnamemod, swrareafilemod, swrareanamemod, swrlandmaskfilemod, swrlandmasknamemod,
                   swrfileobs, swrnameobs, swrareafileobs, swrareanameobs, swrlandmaskfileobs, swrlandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasSwrLonRmse() function computes the SWR zonal (longitude) root mean square error (RMSE) in a 'box'
    (usually the Equatorial Pacific)

    Inputs:
    ------
    :param swrfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SWR
    :param swrnamemod: string
        name of SWR variable (swr, rsds - rsus) in 'swrfilemod'
    :param swrareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SWR
    :param swrareanamemod: string
        name of areacell variable (areacella, areacello) in 'swrareafilemod'
    :param swrlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SWR
    :param swrlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfilemod'
    :param swrfileobs: string
        path_to/filename of the file (NetCDF) of the observed SWR
    :param swrnameobs: string
        name of SWR variable (swr, rsds - rsus) in 'swrfileobs'
    :param swrareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SWR
    :param swrareanameobs: string
        name of areacell variable (areacella, areacello) in 'swrareafileobs'
    :param swrlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SWR
    :param swrlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SWR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISWR',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasSwrLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SWR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean swr Zonal RMSE"
    Units = "W/m2"
    Method = "Zonal root mean square error of " + box + " swr"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasSwrLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    swr_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
        swrfilemod, swrnamemod, "heat flux", "swr", metric, box, file_area=swrareafilemod, name_area=swrareanamemod,
        file_mask=swrlandmaskfilemod, name_mask=swrlandmasknamemod, maskland=True, maskocean=False,
        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    swr_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
        swrfileobs, swrnameobs, "heat flux", "swr", metric, box, file_area=swrareafileobs, name_area=swrareanameobs,
        file_mask=swrlandmaskfileobs, name_mask=swrlandmasknameobs, maskland=True, maskocean=False,
        interpreter="project_interpreter_obs", time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(swr_mod.shape[0] / 12))
    yearN_obs = int(round(swr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(swr_mod)
    actualtimebounds_obs = TimeBounds(swr_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        swr_mod, Method, keyerror_mod = PreProcessTS(
            swr_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        swr_obs, _, keyerror_obs = PreProcessTS(
            swr_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                swr_mod, swr_obs, Method = TwoVarRegrid(swr_mod, swr_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            swr_mod, keyerror_mod = AverageMeridional(swr_mod)
            swr_obs, keyerror_obs = AverageMeridional(swr_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in swr_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in swr_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(swr_mod.shape), "shape2": "(obs) " + str(swr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(swr_mod, swr_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(swr_mod, swr_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(swr_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(swr_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(swr_mod), "observations": ArrayToList(swr_obs),
                                  "axis": list(swr_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                        swrfilemod, swrnamemod, "heat flux", "swr", metric, "equatorial_pacific_LatExt2",
                        file_area=swrareafilemod, name_area=swrareanamemod, file_mask=swrlandmaskfilemod,
                        name_mask=swrlandmasknamemod, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"],
                        debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                        swrfileobs, swrnameobs, "heat flux", "swr", metric, "equatorial_pacific_LatExt2",
                        file_area=swrareafileobs, name_area=swrareanameobs, file_mask=swrlandmaskfileobs,
                        name_mask=swrlandmasknameobs, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_obs"],
                        debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean SWR map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean SWR across longitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean SWR across longitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=swr_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=swr_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauxMapRmse(tauxfilemod, tauxnamemod, tauxareafilemod, tauxareanamemod, tauxlandmaskfilemod,
                    tauxlandmasknamemod, tauxfileobs, tauxnameobs, tauxareafileobs, tauxareanameobs,
                    tauxlandmaskfileobs, tauxlandmasknameobs, box, centered_rmse=0, biased_rmse=1, dataset1="",
                    dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The BiasTauxMapRmse() function computes the TAUX spatial root mean square error (RMSE) in a 'box' (usually the
    tropical Pacific)

    Inputs:
    ------
    :param tauxfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUX
    :param tauxnamemod: string
        name of TAUX variable (taux, tauu) in 'tauxfilemod'
    :param tauxareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for TAUX
    :param tauxareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauxareafilemod'
    :param tauxlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for TAUX
    :param tauxlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfilemod'
    :param tauxfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUX
    :param tauxnameobs: string
        name of TAUX variable (taux, tauu) in 'tauxfileobs'
    :param tauxareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for TAUX
    :param tauxareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauxareafileobs'
    :param tauxlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for TAUX
    :param tauxlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for TAUX
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauxMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled TAUX file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed TAUX file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean taux RMSE"
    Units = "1e-3 N/m2"
    Method = "Spatial root mean square error of " + box + " taux"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauxMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    taux_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        tauxfilemod, tauxnamemod, "wind stress", metric, box, file_area=tauxareafilemod, name_area=tauxareanamemod,
        file_mask=tauxlandmaskfilemod, name_mask=tauxlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    taux_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        tauxfileobs, tauxnameobs, "wind stress", metric, box, file_area=tauxareafileobs, name_area=tauxareanameobs,
        file_mask=tauxlandmaskfileobs, name_mask=tauxlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(taux_mod.shape[0] / 12))
    yearN_obs = int(round(taux_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(taux_mod)
    actualtimebounds_obs = TimeBounds(taux_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        taux_mod, Method, keyerror_mod = PreProcessTS(
            taux_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        taux_obs, _, keyerror_obs = PreProcessTS(
            taux_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                              "shape1": "(mod) " + str(taux_mod.shape), "shape2": "(obs) " + str(taux_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                taux_mod, taux_obs, Method = TwoVarRegrid(
                    taux_mod, taux_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(taux_mod.shape), "shape2": "(obs) " + str(taux_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # change units
            taux_mod = taux_mod * 1e3
            taux_obs = taux_obs * 1e3

            # Computes the root mean square difference
            mv, keyerror = RmsHorizontal(taux_mod, taux_obs, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(taux_mod, taux_obs, axis="xy", centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(taux_mod, weights=None, axis="xy", centered=1, biased=1)
            sm_std_obs = Std(taux_obs, weights=None, axis="xy", centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "spatialSTD": sm_std_mod, "description": "mean TAUX map"}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "spatialSTD": sm_std_obs, "description": "mean TAUX map"}
                dict3 = {"metric_name": Name, "metric_value_" + dataset2: mv,
                         "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2: sm_corr,
                         "CORR_error_" + dataset2: sm_corr_error, "STD_" + dataset2: sm_std,
                         "STD_error_" + dataset2: sm_std_error, "metric_method": Method, "metric_reference": Ref,
                         "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, global_attributes=dict3,
                           var1=taux_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                           var2=taux_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauxLatRmse(tauxfilemod, tauxnamemod, tauxareafilemod, tauxareanamemod, tauxlandmaskfilemod,
                    tauxlandmasknamemod, tauxfileobs, tauxnameobs, tauxareafileobs, tauxareanameobs,
                    tauxlandmaskfileobs, tauxlandmasknameobs, box, centered_rmse=0, biased_rmse=1, dataset1="",
                    dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The BiasTauxLatRmse() function computes the TAUX meridional (latitude) root mean square error (RMSE) in a 'box'
    (usually 'equatorial_pacific_LatExt')

    Inputs:
    ------
    :param tauxfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUX
    :param tauxnamemod: string
        name of TAUX variable (taux, tauu) in 'tauxfilemod'
    :param tauxareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for TAUX
    :param tauxareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauxareafilemod'
    :param tauxlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for TAUX
    :param tauxlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfilemod'
    :param tauxfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUX
    :param tauxnameobs: string
        name of TAUX variable (taux, tauu) in 'tauxfileobs'
    :param tauxareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for TAUX
    :param tauxareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauxareafileobs'
    :param tauxlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for TAUX
    :param tauxlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for TAUX
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauxLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled TAUX file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed TAUX file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean taux Meridional RMSE"
    Units = "1e-3 N/m2"
    Method = "Meridional root mean square error of " + box + " taux"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauxLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    taux_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        tauxfilemod, tauxnamemod, "wind stress", metric, box, file_area=tauxareafilemod, name_area=tauxareanamemod,
        file_mask=tauxlandmaskfilemod, name_mask=tauxlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    taux_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        tauxfileobs, tauxnameobs, "wind stress", metric, box, file_area=tauxareafileobs, name_area=tauxareanameobs,
        file_mask=tauxlandmaskfileobs, name_mask=tauxlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(taux_mod.shape[0] / 12))
    yearN_obs = int(round(taux_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(taux_mod)
    actualtimebounds_obs = TimeBounds(taux_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        taux_mod, Method, keyerror_mod = PreProcessTS(
            taux_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        taux_obs, _, keyerror_obs = PreProcessTS(
            taux_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                              "shape1": "(mod) " + str(taux_mod.shape), "shape2": "(obs) " + str(taux_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                taux_mod, taux_obs, Method = TwoVarRegrid(
                    taux_mod, taux_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(taux_mod.shape), "shape2": "(obs) " + str(taux_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            taux_mod, keyerror_mod = AverageZonal(taux_mod)
            taux_obs, keyerror_obs = AverageZonal(taux_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                taux_mod = taux_mod * 1e3
                taux_obs = taux_obs * 1e3
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(taux_mod.shape), "shape2": "(obs) " + str(taux_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(taux_mod, taux_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(taux_mod, taux_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(taux_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(taux_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(taux_mod), "observations": ArrayToList(taux_obs),
                                  "axis": list(taux_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        tauxfilemod, tauxnamemod, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauxareafilemod, name_area=tauxareanamemod, file_mask=tauxlandmaskfilemod,
                        name_mask=tauxlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        tauxfileobs, tauxnameobs, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauxareafileobs, name_area=tauxareanameobs, file_mask=tauxlandmaskfileobs,
                        name_mask=tauxlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # change units
                            map_mod = map_mod * 1e3
                            map_obs = map_obs * 1e3
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean TAUX map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean TAUX across latitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean TAUX across latitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=taux_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=taux_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauxLonRmse(tauxfilemod, tauxnamemod, tauxareafilemod, tauxareanamemod, tauxlandmaskfilemod,
                    tauxlandmasknamemod, tauxfileobs, tauxnameobs, tauxareafileobs, tauxareanameobs,
                    tauxlandmaskfileobs, tauxlandmasknameobs, box, centered_rmse=0, biased_rmse=1, dataset1="",
                    dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The BiasTauxLonRmse() function computes the TAUX zonal (longitude) root mean square error (RMSE) in a 'box'
    (usually the Equatorial Pacific)

    Inputs:
    ------
    :param tauxfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUX
    :param tauxnamemod: string
        name of TAUX variable (taux, tauu) in 'tauxfilemod'
    :param tauxareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for TAUX
    :param tauxareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauxareafilemod"
    :param tauxlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for TAUX
    :param tauxlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfilemod'
    :param tauxfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUX
    :param tauxnameobs: string
        name of TAUX variable (taux, tauu) in 'tauxfileobs'
    :param tauxareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for TAUX
    :param tauxareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauxareafileobs'
    :param tauxlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for TAUX
    :param tauxlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for TAUX
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauxLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled TAUX file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed TAUX file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean taux Zonal RMSE"
    Units = "1e-3 N/m2"
    Method = "Zonal root mean square error of " + box + " taux"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauxLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    taux_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        tauxfilemod, tauxnamemod, "wind stress", metric, box, file_area=tauxareafilemod, name_area=tauxareanamemod,
        file_mask=tauxlandmaskfilemod, name_mask=tauxlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    taux_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        tauxfileobs, tauxnameobs, "wind stress", metric, box, file_area=tauxareafileobs, name_area=tauxareanameobs,
        file_mask=tauxlandmaskfileobs, name_mask=tauxlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(taux_mod.shape[0] / 12))
    yearN_obs = int(round(taux_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(taux_mod)
    actualtimebounds_obs = TimeBounds(taux_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        taux_mod, Method, keyerror_mod = PreProcessTS(
            taux_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        taux_obs, _, keyerror_obs = PreProcessTS(
            taux_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                              "shape1": "(mod) " + str(taux_mod.shape), "shape2": "(obs) " + str(taux_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                taux_mod, taux_obs, Method = TwoVarRegrid(
                    taux_mod, taux_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(taux_mod.shape), "shape2": "(obs) " + str(taux_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            taux_mod, keyerror_mod = AverageMeridional(taux_mod)
            taux_obs, keyerror_obs = AverageMeridional(taux_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                taux_mod = taux_mod * 1e3
                taux_obs = taux_obs * 1e3
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(taux_mod.shape), "shape2": "(obs) " + str(taux_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(taux_mod, taux_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(taux_mod, taux_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(taux_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(taux_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(taux_mod), "observations": ArrayToList(taux_obs),
                                  "axis": list(taux_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        tauxfilemod, tauxnamemod, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauxareafilemod, name_area=tauxareanamemod, file_mask=tauxlandmaskfilemod,
                        name_mask=tauxlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        tauxfileobs, tauxnameobs, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauxareafileobs, name_area=tauxareanameobs, file_mask=tauxlandmaskfileobs,
                        name_mask=tauxlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # change units
                            map_mod = map_mod * 1e3
                            map_obs = map_obs * 1e3
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean TAUX map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean TAUX across longitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean TAUX across longitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=taux_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=taux_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauyMapRmse(tauyfilemod, tauynamemod, tauyareafilemod, tauyareanamemod, tauylandmaskfilemod,
                    tauylandmasknamemod, tauyfileobs, tauynameobs, tauyareafileobs, tauyareanameobs,
                    tauylandmaskfileobs, tauylandmasknameobs, box, centered_rmse=0, biased_rmse=1, dataset1="",
                    dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The BiasTauyMapRmse() function computes the TAUY spatial root mean square error (RMSE) in a 'box' (usually the
    tropical Pacific)

    Inputs:
    ------
    :param tauyfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUY
    :param tauynamemod: string
        name of TAUY variable (tauy, tauu) in 'tauyfilemod'
    :param tauyareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for TAUY
    :param tauyareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauyareafilemod'
    :param tauylandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for TAUY
    :param tauylandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauylandmaskfilemod'
    :param tauyfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUY
    :param tauynameobs: string
        name of TAUY variable (tauy, tauu) in 'tauyfileobs'
    :param tauyareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for TAUY
    :param tauyareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauyareafileobs'
    :param tauylandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for TAUY
    :param tauylandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauylandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for TAUY
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauyMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled TAUY file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed TAUY file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean tauy RMSE"
    Units = "1e-3 N/m2"
    Method = "Spatial root mean square error of " + box + " tauy"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauyMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    tauy_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        tauyfilemod, tauynamemod, "wind stress", metric, box, file_area=tauyareafilemod, name_area=tauyareanamemod,
        file_mask=tauylandmaskfilemod, name_mask=tauylandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tauy_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        tauyfileobs, tauynameobs, "wind stress", metric, box, file_area=tauyareafileobs, name_area=tauyareanameobs,
        file_mask=tauylandmaskfileobs, name_mask=tauylandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(tauy_mod.shape[0] / 12))
    yearN_obs = int(round(tauy_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(tauy_mod)
    actualtimebounds_obs = TimeBounds(tauy_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        tauy_mod, Method, keyerror_mod = PreProcessTS(
            tauy_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        tauy_obs, _, keyerror_obs = PreProcessTS(
            tauy_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauy_mod.shape), "shape2": "(obs) " + str(tauy_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tauy_mod, tauy_obs, Method = TwoVarRegrid(
                    tauy_mod, tauy_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauy_mod.shape), "shape2": "(obs) " + str(tauy_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # change units
            tauy_mod = tauy_mod * 1e3
            tauy_obs = tauy_obs * 1e3

            # Computes the root mean square difference
            mv, keyerror = RmsHorizontal(tauy_mod, tauy_obs, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(tauy_mod, tauy_obs, axis="xy", centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(tauy_mod, weights=None, axis="xy", centered=1, biased=1)
            sm_std_obs = Std(tauy_obs, weights=None, axis="xy", centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "spatialSTD": sm_std_mod, "description": "mean TAUY map"}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "spatialSTD": sm_std_obs, "description": "mean TAUY map"}
                dict3 = {"metric_name": Name, "metric_value_" + dataset2: mv,
                         "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2: sm_corr,
                         "CORR_error_" + dataset2: sm_corr_error, "STD_" + dataset2: sm_std,
                         "STD_error_" + dataset2: sm_std_error, "metric_method": Method, "metric_reference": Ref,
                         "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, global_attributes=dict3,
                           var1=tauy_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                           var2=tauy_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauyLatRmse(tauyfilemod, tauynamemod, tauyareafilemod, tauyareanamemod, tauylandmaskfilemod,
                    tauylandmasknamemod, tauyfileobs, tauynameobs, tauyareafileobs, tauyareanameobs,
                    tauylandmaskfileobs, tauylandmasknameobs, box, centered_rmse=0, biased_rmse=1, dataset1="",
                    dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The BiasTauyLatRmse() function computes the TAUY meridional (latitude) root mean square error (RMSE) in a 'box'
    (usually 'equatorial_pacific_LatExt')

    Inputs:
    ------
    :param tauyfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUY
    :param tauynamemod: string
        name of TAUY variable (tauy, tauu) in 'tauyfilemod'
    :param tauyareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for TAUY
    :param tauyareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauyareafilemod'
    :param tauylandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for TAUY
    :param tauylandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauylandmaskfilemod'
    :param tauyfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUY
    :param tauynameobs: string
        name of TAUY variable (tauy, tauu) in 'tauyfileobs'
    :param tauyareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for TAUY
    :param tauyareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauyareafileobs'
    :param tauylandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for TAUY
    :param tauylandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauylandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for TAUY
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauyLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled TAUY file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed TAUY file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean tauy Meridional RMSE"
    Units = "1e-3 N/m2"
    Method = "Meridional root mean square error of " + box + " tauy"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauyLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    tauy_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        tauyfilemod, tauynamemod, "wind stress", metric, box, file_area=tauyareafilemod, name_area=tauyareanamemod,
        file_mask=tauylandmaskfilemod, name_mask=tauylandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tauy_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        tauyfileobs, tauynameobs, "wind stress", metric, box, file_area=tauyareafileobs, name_area=tauyareanameobs,
        file_mask=tauylandmaskfileobs, name_mask=tauylandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(tauy_mod.shape[0] / 12))
    yearN_obs = int(round(tauy_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(tauy_mod)
    actualtimebounds_obs = TimeBounds(tauy_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        tauy_mod, Method, keyerror_mod = PreProcessTS(
            tauy_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        tauy_obs, _, keyerror_obs = PreProcessTS(
            tauy_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauy_mod.shape), "shape2": "(obs) " + str(tauy_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tauy_mod, tauy_obs, Method = TwoVarRegrid(
                    tauy_mod, tauy_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauy_mod.shape), "shape2": "(obs) " + str(tauy_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            tauy_mod, keyerror_mod = AverageZonal(tauy_mod)
            tauy_obs, keyerror_obs = AverageZonal(tauy_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                tauy_mod = tauy_mod * 1e3
                tauy_obs = tauy_obs * 1e3
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauy_mod.shape), "shape2": "(obs) " + str(tauy_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                tauyRmse, keyerror = RmsMeridional(tauy_mod, tauy_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                tauyRmseErr = None

                # Supplementary metrics
                sm_corr = float(Correlation(tauy_mod, tauy_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(tauy_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(tauy_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(tauy_mod), "observations": ArrayToList(tauy_obs),
                                  "axis": list(tauy_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        tauyfilemod, tauynamemod, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauyareafilemod, name_area=tauyareanamemod, file_mask=tauylandmaskfilemod,
                        name_mask=tauylandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        tauyfileobs, tauynameobs, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauyareafileobs, name_area=tauyareanameobs, file_mask=tauylandmaskfileobs,
                        name_mask=tauylandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # change units
                            map_mod = map_mod * 1e3
                            map_obs = map_obs * 1e3
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean TAUY map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean TAUY across latitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean TAUY across latitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=tauy_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=tauy_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasTauyLonRmse(tauyfilemod, tauynamemod, tauyareafilemod, tauyareanamemod, tauylandmaskfilemod,
                    tauylandmasknamemod, tauyfileobs, tauynameobs, tauyareafileobs, tauyareanameobs,
                    tauylandmaskfileobs, tauylandmasknameobs, box, centered_rmse=0, biased_rmse=1, dataset1="",
                    dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The BiasTauyLonRmse() function computes the TAUY zonal (longitude) root mean square error (RMSE) in a 'box'
    (usually the Equatorial Pacific)

    Inputs:
    ------
    :param tauyfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUY
    :param tauynamemod: string
        name of TAUY variable (tauy, tauu) in 'tauyfilemod'
    :param tauyareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for TAUY
    :param tauyareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauyareafilemod'
    :param tauylandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for TAUY
    :param tauylandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauylandmaskfilemod'
    :param tauyfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUY
    :param tauynameobs: string
        name of TAUY variable (tauy, tauu) in 'tauyfileobs'
    :param tauyareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for TAUY
    :param tauyareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauyareafileobs'
    :param tauylandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for TAUY
    :param tauylandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauylandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for TAUY
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasTauyLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled TAUY file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed TAUY file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean tauy Zonal RMSE"
    Units = "1e-3 N/m2"
    Method = "Zonal root mean square error of " + box + " tauy"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasTauyLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    tauy_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        tauyfilemod, tauynamemod, "wind stress", metric, box, file_area=tauyareafilemod, name_area=tauyareanamemod,
        file_mask=tauylandmaskfilemod, name_mask=tauylandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tauy_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        tauyfileobs, tauynameobs, "wind stress", metric, box, file_area=tauyareafileobs, name_area=tauyareanameobs,
        file_mask=tauylandmaskfileobs, name_mask=tauylandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(tauy_mod.shape[0] / 12))
    yearN_obs = int(round(tauy_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(tauy_mod)
    actualtimebounds_obs = TimeBounds(tauy_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        tauy_mod, Method, keyerror_mod = PreProcessTS(
            tauy_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        tauy_obs, _, keyerror_obs = PreProcessTS(
            tauy_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauy_mod.shape), "shape2": "(obs) " + str(tauy_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tauy_mod, tauy_obs, Method = TwoVarRegrid(
                    tauy_mod, tauy_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauy_mod.shape), "shape2": "(obs) " + str(tauy_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            tauy_mod, keyerror_mod = AverageMeridional(tauy_mod)
            tauy_obs, keyerror_obs = AverageMeridional(tauy_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                tauy_mod = tauy_mod * 1e3
                tauy_obs = tauy_obs * 1e3
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauy_mod.shape), "shape2": "(obs) " + str(tauy_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(tauy_mod, tauy_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(tauy_mod, tauy_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(tauy_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(tauy_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(tauy_mod), "observations": ArrayToList(tauy_obs),
                                  "axis": list(tauy_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        tauyfilemod, tauynamemod, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauyareafilemod, name_area=tauyareanamemod, file_mask=tauylandmaskfilemod,
                        name_mask=tauylandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        tauyfileobs, tauynameobs, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauyareafileobs, name_area=tauyareanameobs, file_mask=tauylandmaskfileobs,
                        name_mask=tauylandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # change units
                            map_mod = map_mod * 1e3
                            map_obs = map_obs * 1e3
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean TAUY map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean TAUY across longitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean TAUY across longitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=tauy_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=tauy_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasThfMapRmse(thffilemod, thfnamemod, thfareafilemod, thfareanamemod, thflandmaskfilemod, thflandmasknamemod,
                   thffileobs, thfnameobs, thfareafileobs, thfareanameobs, thflandmaskfileobs, thflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasThfMapRmse() function computes the THF spatial root mean square error (RMSE) in a 'box' (usually the
    tropical Pacific)

    Inputs:
    ------
    :param thffilemod: string
        path_to/filename of the file (NetCDF) of the modeled THF
    :param thfnamemod: string
        name of THF variable (thf, swr + lwr + lhf + shf, hfls + hfss + rlds - rlus + rsds - rsus) in 'thffilemod'
    :param thfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for THF
    :param thfareanamemod: string
        name of areacell variable (areacella, areacello) in 'thfareafilemod'
    :param thflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for THF
    :param thflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfilemod'
    :param thffileobs: string
        path_to/filename of the file (NetCDF) of the observed THF
    :param thfnameobs: string
        name of THF variable (thf, swr + lwr + lhf + shf, hfls + hfss + rlds - rlus + rsds - rsus) in 'thffileobs'
    :param thfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for THF
    :param thfareanameobs: string
        name of areacell variable (areacella, areacello) in 'thfareafileobs'
    :param thflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for THF
    :param thflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfileobs'
    :param box: string
        name of box ('tropical_pacific') for THF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If you want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadITHF',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasThfMapRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean thf RMSE"
    Units = "W/m2"
    Method = "Spatial root mean square error of " + box + " thf"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasThfMapRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    thf_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
        thffilemod, thfnamemod, "heat flux", "thf", metric, box, file_area=thfareafilemod, name_area=thfareanamemod,
        file_mask=thflandmaskfilemod, name_mask=thflandmasknamemod, maskland=True, maskocean=False,
        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    thf_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
        thffileobs, thfnameobs, "heat flux", "thf", metric, box, file_area=thfareafileobs, name_area=thfareanameobs,
        file_mask=thflandmaskfileobs, name_mask=thflandmasknameobs, maskland=True, maskocean=False,
        interpreter="project_interpreter_obs", time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(thf_mod.shape[0] / 12))
    yearN_obs = int(round(thf_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(thf_mod)
    actualtimebounds_obs = TimeBounds(thf_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        thf_mod, Method, keyerror_mod = PreProcessTS(
            thf_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        thf_obs, _, keyerror_obs = PreProcessTS(
            thf_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                thf_mod, thf_obs, Method = TwoVarRegrid(thf_mod, thf_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsHorizontal(thf_mod, thf_obs, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(thf_mod, thf_obs, axis="xy", centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(thf_mod, weights=None, axis="xy", centered=1, biased=1)
            sm_std_obs = Std(thf_obs, weights=None, axis="xy", centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "spatialSTD": sm_std_mod, "description": "mean THF map"}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "spatialSTD": sm_std_obs, "description": "mean THF map"}
                dict3 = {"metric_name": Name, "metric_value_" + dataset2: mv,
                         "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2: sm_corr,
                         "CORR_error_" + dataset2: sm_corr_error, "STD_" + dataset2: sm_std,
                         "STD_error_" + dataset2: sm_std_error, "metric_method": Method, "metric_reference": Ref,
                         "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, global_attributes=dict3,
                           var1=thf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                           var2=thf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2)
                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasThfLatRmse(thffilemod, thfnamemod, thfareafilemod, thfareanamemod, thflandmaskfilemod, thflandmasknamemod,
                   thffileobs, thfnameobs, thfareafileobs, thfareanameobs, thflandmaskfileobs, thflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasThfLatRmse() function computes the THF meridional (latitude) root mean square error (RMSE) in a 'box'
    (usually 'nino3_LatExt')

    Inputs:
    ------
    :param thffilemod: string
        path_to/filename of the file (NetCDF) of the modeled THF
    :param thfnamemod: string
        name of THF variable (thf, swr + lwr + lhf + shf, hfls + hfss + rlds - rlus + rsds - rsus) in 'thffilemod'
    :param thfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for THF
    :param thfareanamemod: string
        name of areacell variable (areacella, areacello) in 'thfareafilemod'
    :param thflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for THF
    :param thflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfilemod'
    :param thffileobs: string
        path_to/filename of the file (NetCDF) of the observed THF
    :param thfnameobs: string
        name of THF variable (thf, swr + lwr + lhf + shf, hfls + hfss + rlds - rlus + rsds - rsus) in 'thffileobs'
    :param thfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for THF
    :param thfareanameobs: string
        name of areacell variable (areacella, areacello) in 'thfareafileobs'
    :param thflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for THF
    :param thflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfileobs'
    :param box: string
        name of box ('nino3_LatExt') for THF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadITHF',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasThfLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean thf Meridional RMSE"
    Units = "W/m2"
    Method = "Meridional root mean square error of " + box + " thf"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasThfLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    thf_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
        thffilemod, thfnamemod, "heat flux", "thf", metric, box, file_area=thfareafilemod, name_area=thfareanamemod,
        file_mask=thflandmaskfilemod, name_mask=thflandmasknamemod, maskland=True, maskocean=False,
        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    thf_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
        thffileobs, thfnameobs, "heat flux", "thf", metric, box, file_area=thfareafileobs, name_area=thfareanameobs,
        file_mask=thflandmaskfileobs, name_mask=thflandmasknameobs, maskland=True, maskocean=False,
        interpreter="project_interpreter_obs", time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(thf_mod.shape[0] / 12))
    yearN_obs = int(round(thf_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(thf_mod)
    actualtimebounds_obs = TimeBounds(thf_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        thf_mod, Method, keyerror_mod = PreProcessTS(
            thf_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        thf_obs, _, keyerror_obs = PreProcessTS(
            thf_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                thf_mod, thf_obs, Method = TwoVarRegrid(thf_mod, thf_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            thf_mod, keyerror_mod = AverageZonal(thf_mod)
            thf_obs, keyerror_obs = AverageZonal(thf_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(thf_mod, thf_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(thf_mod, thf_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(thf_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(thf_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(thf_mod), "observations": ArrayToList(thf_obs),
                                  "axis": list(thf_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                        thffilemod, thfnamemod, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                        file_area=thfareafilemod, name_area=thfareanamemod, file_mask=thflandmaskfilemod,
                        name_mask=thflandmasknamemod, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"],
                        debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                        thffileobs, thfnameobs, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                        file_area=thfareafileobs, name_area=thfareanameobs, file_mask=thflandmaskfileobs,
                        name_mask=thflandmasknameobs, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_obs"],
                        debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean THF map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean THF across latitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean THF across latitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lat": sm_corr, "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=thf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=thf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def BiasThfLonRmse(thffilemod, thfnamemod, thfareafilemod, thfareanamemod, thflandmaskfilemod, thflandmasknamemod,
                   thffileobs, thfnameobs, thfareafileobs, thfareanameobs, thflandmaskfileobs, thflandmasknameobs, box,
                   centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
                   metname="", **kwargs):
    """
    The BiasThfLonRmse() function computes the THF zonal (longitude) root mean square error (RMSE) in a 'box'
    (usually the Equatorial Pacific)

    Inputs:
    ------
    :param thffilemod: string
        path_to/filename of the file (NetCDF) of the modeled THF
    :param thfnamemod: string
        name of THF variable (thf, swr + lwr + lhf + shf, hfls + hfss + rlds - rlus + rsds - rsus) in 'thffilemod'
    :param thfareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for THF
    :param thfareanamemod: string
        name of areacell variable (areacella, areacello) in 'thfareafilemod'
    :param thflandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for THF
    :param thflandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfilemod'
    :param thffileobs: string
        path_to/filename of the file (NetCDF) of the observed THF
    :param thfnameobs: string
        name of THF variable (thf, swr + lwr + lhf + shf, hfls + hfss + rlds - rlus + rsds - rsus) in 'thffileobs'
    :param thfareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for THF
    :param thfareanameobs: string
        name of areacell variable (areacella, areacello) in 'thfareafileobs'
    :param thflandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for THF
    :param thflandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for THF
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadITHF',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'BiasThfLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed THF file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "time mean thf Zonal RMSE"
    Units = "W/m2"
    Method = "Zonal root mean square error of " + box + " thf"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "BiasThfLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    thf_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
        thffilemod, thfnamemod, "heat flux", "thf", metric, box, file_area=thfareafilemod, name_area=thfareanamemod,
        file_mask=thflandmaskfilemod, name_mask=thflandmasknamemod, maskland=True, maskocean=False,
        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    thf_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
        thffileobs, thfnameobs, "heat flux", "thf", metric, box, file_area=thfareafileobs, name_area=thfareanameobs,
        file_mask=thflandmaskfileobs, name_mask=thflandmasknameobs, maskland=True, maskocean=False,
        interpreter="project_interpreter_obs", time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(thf_mod.shape[0] / 12))
    yearN_obs = int(round(thf_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(thf_mod)
    actualtimebounds_obs = TimeBounds(thf_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        thf_mod, Method, keyerror_mod = PreProcessTS(
            thf_mod, Method, areacell=mod_areacell, average="time", compute_anom=False, region=box, **kwargs)
        thf_obs, _, keyerror_obs = PreProcessTS(
            thf_obs, "", areacell=obs_areacell, average="time", compute_anom=False, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                              "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                thf_mod, thf_obs, Method = TwoVarRegrid(thf_mod, thf_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            thf_mod, keyerror_mod = AverageMeridional(thf_mod)
            thf_obs, keyerror_obs = AverageMeridional(thf_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in thf_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in thf_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(thf_mod.shape), "shape2": "(obs) " + str(thf_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(thf_mod, thf_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(thf_mod, thf_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(thf_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(thf_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(thf_mod), "observations": ArrayToList(thf_obs),
                                  "axis": list(thf_mod.getAxis(0)[:])}

                if netcdf is True:
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area_multifile(
                        thffilemod, thfnamemod, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                        file_area=thfareafilemod, name_area=thfareanamemod, file_mask=thflandmaskfilemod,
                        name_mask=thflandmasknamemod, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_mod"],
                        debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area_multifile(
                        thffileobs, thfnameobs, "heat flux", "thf", metric, "equatorial_pacific_LatExt2",
                        file_area=thfareafileobs, name_area=thfareanameobs, file_mask=thflandmaskfileobs,
                        name_mask=thflandmasknameobs, maskland=True, maskocean=False,
                        interpreter="project_interpreter_mod", time_bounds=kwargs["time_bounds_obs"],
                        debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf", 15, **dict_debug)
                            # Supplementary metrics
                            dict_metric, dict_nc = dict(), dict()
                            dict_metric, dict_nc = fill_dict_axis(
                                map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                biased_rmse=biased_rmse, dict_metric=dict_metric, description="mean THF map")
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod),
                                     "description": "mean THF across longitudes", "arraySTD": sm_std_obs}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs),
                                     "description": "mean THF across longitudes", "arraySTD": sm_std_obs}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=thf_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=thf_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoAmpl(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, dataset="",
             debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoAmpl() function computes the standard deviation of 'sstbox' sstA (usually the standard deviation of nino3
    sstA)

    Author:	Eric Guilyardi : Eric.Guilyardi@locean-ipsl.upmc.fr
    Co-author: Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon Jan  9 11:05:18 CET 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoAmpl_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO SSTA amplitude"
    Units = "C"
    Method = "Standard deviation of " + sstbox + " averaged SSTA"
    Ref = "Using CDAT regression calculation"
    metric = "EnsoAmpl"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, Method, keyerror = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        del sst_areacell
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "time1": "(sst) " + str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Computes the standard deviation
            mv = float(Std(sst))

            # Standard Error of the Standard Deviation (function of npts)
            mv_error = NDJ_err = mv / NUMPYsqrt(2 * (float(len(sst)) - 1))

            # Dive down diagnostic
            dive_down_diag = {"value": None, "axis": None}

            if netcdf is True:
                # additional diagnostic
                # Read file and select the right region
                sst_map, sst_areacell1, keyerror1 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                sst_hov, sst_areacell2, keyerror2 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                    sst_map, _, keyerror1 = PreProcessTS(sst_map, "", areacell=sst_areacell1, compute_anom=True,
                                                         region="equatorial_pacific_LatExt2", **kwargs)
                    sst_hov, _, keyerror2 = PreProcessTS(
                        sst_hov, "", areacell=sst_areacell2, compute_anom=True, region="equatorial_pacific", **kwargs)
                    del sst_areacell1, sst_areacell2
                    keyerror = add_up_errors([keyerror1, keyerror2])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(sst map) " + str([ax.id for ax in sst_map.getAxisList()]),
                                          "axes2": "(sst hov) " + str([ax.id for ax in sst_hov.getAxisList()]),
                                          "shape1": "(sst map) " + str(sst_map.shape),
                                          "shape2": "(sst hov) " + str(sst_hov.shape),
                                          "time1": "(sst map) " + str(TimeBounds(sst_map)),
                                          "time2": "(sst hov) " + str(TimeBounds(sst_hov))}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 10, **dict_debug)
                        # std
                        sst_map = Std(sst_map)
                        sst_hov = Std(sst_hov)
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        sst_map = Regrid(sst_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                        sst_hov = Regrid(sst_hov, None, region="equatorial_pacific", **kwargs["regridding"])
                        # Meridional average
                        sst_hov, keyerror = AverageMeridional(sst_hov)
                        if keyerror is None:
                            # Dive down diagnostic
                            dive_down_diag = {"value": ArrayToList(sst_hov), "axis": list(sst_hov.getAxis(0)[:])}
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            lat = ReferenceRegions("equatorial_pacific")["latitude"]
                            dict1 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "diagnostic_value": mv, "diagnostic_value_error": mv_error,
                                "description": "zonal curve of standard deviation of equatorial_pacific SSTA " +
                                               "(meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) + "])"}
                            dict2 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "map of standard deviation of tropical Pacific SSTA"}
                            dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(file_name, global_attributes=dict3,
                                       var1=sst_hov, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                                       var2=sst_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
                            del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "nyears": yearN,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": Ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSstSkew(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, dataset="",
                debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstSkew() function computes the skewness of 'sstbox' sstA (usually the skewness of nino3 sstA)
    The standard error on the coefficient of skewness is based on Fisher (1930; https://doi.org/10.1098/rspa.1930.0185)

    Author:	Eric Guilyardi : Eric.Guilyardi@locean-ipsl.upmc.fr
    Co-author: Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon Jan  9 11:05:18 CET 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box ('nino3') for SST
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstSkew_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO SSTA skewness"
    Units = "" if kwargs["normalization"] else "C"
    Method = "Skewness of " + sstbox + " averaged SSTA"
    Ref = "Using CDAT regression calculation"
    metric = "EnsoSstSkew"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, Method, keyerror = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        del sst_areacell
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "time1": "(sst) " + str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Computes the skewness
            mv = float(SkewnessTemporal(sst))

            # Standard Error of the skewness (function of nyears)
            npts = float(len(sst))
            mv_error = NUMPYsqrt(6 * npts * (npts - 1) / ((npts - 2) * (npts + 1) * (npts + 3)))

            # Dive down diagnostic
            dive_down_diag = {"value": None, "axis": None}

            if netcdf is True:
                # additional diagnostic
                # Read file and select the right region
                sst_map, sst_areacell1, keyerror1 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                sst_hov, sst_areacell2, keyerror2 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                    sst_map, _, keyerror1 = PreProcessTS(sst_map, "", areacell=sst_areacell1, compute_anom=True,
                                                         region="equatorial_pacific_LatExt2", **kwargs)
                    sst_hov, _, keyerror2 = PreProcessTS(
                        sst_hov, "", areacell=sst_areacell2, compute_anom=True, region="equatorial_pacific", **kwargs)
                    del sst_areacell1, sst_areacell2
                    keyerror = add_up_errors([keyerror1, keyerror2])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(sst map) " + str([ax.id for ax in sst_map.getAxisList()]),
                                          "axes2": "(sst hov) " + str([ax.id for ax in sst_hov.getAxisList()]),
                                          "shape1": "(sst map) " + str(sst_map.shape),
                                          "shape2": "(sst hov) " + str(sst_hov.shape),
                                          "time1": "(sst map) " + str(TimeBounds(sst_map)),
                                          "time2": "(sst hov) " + str(TimeBounds(sst_hov))}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 10, **dict_debug)
                        # std
                        sst_map = SkewnessTemporal(sst_map)
                        sst_hov = SkewnessTemporal(sst_hov)
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        sst_map = Regrid(sst_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                        sst_hov = Regrid(sst_hov, None, region="equatorial_pacific", **kwargs["regridding"])
                        # Meridional average
                        sst_hov, keyerror = AverageMeridional(sst_hov)
                        if keyerror is None:
                            # Dive down diagnostic
                            dive_down_diag = {"value": ArrayToList(sst_hov), "axis": list(sst_hov.getAxis(0)[:])}
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            lat = ReferenceRegions("equatorial_pacific")["latitude"]
                            dict1 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "diagnostic_value": mv, "diagnostic_value_error": mv_error,
                                "description": "zonal curve of skewness of equatorial_pacific SSTA (meridional " +
                                               "averaged [" + str(lat[0]) + " ; " + str(lat[1]) + "])"}
                            dict2 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "map of skewness of tropical Pacific SSTA"}
                            dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(file_name, global_attributes=dict3,
                                       var1=sst_hov, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                                       var2=sst_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
                            del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "nyears": yearN,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": Ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSeasonality(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, dataset="",
                    debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSeasonality() function computes ratio between the November-December-January (NDJ) and March-April-May (MAM)
    average standard deviation of 'sstbox' sstA (usually nino3 sstA)

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box ('nino3') for SST
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSeasonality_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO seasonality"
    Units = "" if kwargs["normalization"] else "C/C"
    Method = "Ratio between NDJ and MAM standard deviation " + sstbox + " sstA"
    Ref = "Using CDAT std dev calculation"
    metric = "EnsoSeasonality"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_ts, Method, keyerror = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=False, region=sstbox, **kwargs)
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_ts.getAxisList()]),
                              "shape1": "(sst) " + str(sst_ts.shape), "time1": "(sst) " + str(TimeBounds(sst_ts))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Seasonal mean
            sst_NDJ = SeasonalMean(sst_ts, "NDJ", compute_anom=True)
            sst_MAM = SeasonalMean(sst_ts, "MAM", compute_anom=True)
            if debug is True:
                dict_debug = {"axes1": "(sst_NDJ) " + str([ax.id for ax in sst_NDJ.getAxisList()]),
                              "axes2": "(sst_NDJ) " + str([ax.id for ax in sst_MAM.getAxisList()]),
                              "shape1": "(sst_NDJ) " + str(sst_NDJ.shape), "shape2": "(sst_NDJ) " + str(sst_MAM.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # Compute std dev and ratio
            sst_NDJ_std = float(Std(sst_NDJ))
            sst_MAM_std = float(Std(sst_MAM))
            mv = float(sst_NDJ_std / sst_MAM_std)

            # Standard Error of the Standard Deviation (function of npts)
            NDJ_err = sst_NDJ_std / NUMPYsqrt(2 * (float(len(sst_NDJ)) - 1))
            MAM_err = sst_MAM_std / NUMPYsqrt(2 * (float(len(sst_MAM)) - 1))
            # The error (dy) on ratio ('y = x/z'): dy = (z*dx + x*dz) / z2
            mv_error = float((sst_MAM_std * NDJ_err + sst_NDJ_std * MAM_err) / NUMPYsquare(sst_MAM_std))

            # Dive down diagnostic
            sst, _, _ = PreProcessTS(
                sst, "", areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
            del sst_areacell
            sstStd_monthly = StdMonthly(sst)
            dive_down_diag = {"value": ArrayToList(sstStd_monthly), "axis": list(sstStd_monthly.getAxis(0)[:])}

            if netcdf is True:
                # additional diagnostic
                # Read file and select the right region
                sst1, sst_areacell1, keyerror1 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                sst2, sst_areacell2, keyerror2 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                    sst1, _, keyerror1 = PreProcessTS(sst1, "", areacell=sst_areacell1, compute_anom=False,
                                                      region="equatorial_pacific_LatExt2", **kwargs)
                    sst3, _, keyerror2 = PreProcessTS(sst2, "", areacell=sst_areacell2, compute_anom=False,
                                                      region="equatorial_pacific", **kwargs)
                    sst2, _, keyerror3 = PreProcessTS(sst2, "", areacell=sst_areacell2, compute_anom=True,
                                                      region="equatorial_pacific", **kwargs)
                    del sst_areacell1, sst_areacell2
                    keyerror = add_up_errors([keyerror, keyerror1, keyerror2, keyerror3])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {
                                "axes1": "(sst1) " + str([ax.id for ax in sst1.getAxisList()]),
                                "axes2": "(sst2) " + str([ax.id for ax in sst2.getAxisList()]),
                                "axes3": "(sst3) " + str([ax.id for ax in sst3.getAxisList()]),
                                "shape1": "(sst1) " + str(sst1.shape), "shape2": "(sst2) " + str(sst2.shape),
                                "shape3": "(sst3) " + str(sst3.shape), "time1": "(sst1) " + str(TimeBounds(sst1)),
                                "time2": "(sst2) " + str(TimeBounds(sst2)), "time3": "(sst3) " + str(TimeBounds(sst3))}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS: netcdf", 10, **dict_debug)
                        # Seasonal mean
                        sst1_NDJ = SeasonalMean(sst1, "NDJ", compute_anom=True)
                        sst1_MAM = SeasonalMean(sst1, "MAM", compute_anom=True)
                        sst3_NDJ = SeasonalMean(sst3, "NDJ", compute_anom=True)
                        sst3_MAM = SeasonalMean(sst3, "MAM", compute_anom=True)
                        if debug is True:
                            dict_debug = {"axes1": "(sst1_NDJ) " + str([ax.id for ax in sst1_NDJ.getAxisList()]),
                                          "axes2": "(sst1_MAM) " + str([ax.id for ax in sst1_MAM.getAxisList()]),
                                          "axes3": "(sst3_NDJ) " + str([ax.id for ax in sst3_NDJ.getAxisList()]),
                                          "axes4": "(sst3_MAM) " + str([ax.id for ax in sst3_MAM.getAxisList()]),
                                          "shape1": "(sst1_NDJ) " + str(sst1_NDJ.shape),
                                          "shape2": "(sst1_MAM) " + str(sst1_MAM.shape),
                                          "shape3": "(sst3_NDJ) " + str(sst3_NDJ.shape),
                                          "shape4": "(sst3_MAM) " + str(sst3_MAM.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean: netcdf", 15, **dict_debug)
                        # Compute std dev
                        sst1_NDJ = Std(sst1_NDJ)
                        sst1_MAM = Std(sst1_MAM)
                        sst2 = StdMonthly(sst2)
                        sst3_NDJ = Std(sst3_NDJ)
                        sst3_MAM = Std(sst3_MAM)
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        sst1_NDJ = Regrid(sst1_NDJ, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                        sst1_MAM = Regrid(sst1_MAM, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                        sst2 = Regrid(sst2, None, region="equatorial_pacific", **kwargs["regridding"])
                        sst3_NDJ = Regrid(sst3_NDJ, None, region="equatorial_pacific", **kwargs["regridding"])
                        sst3_MAM = Regrid(sst3_MAM, None, region="equatorial_pacific", **kwargs["regridding"])
                        # Meridional average
                        sst2, keyerror1 = AverageMeridional(sst2)
                        sst3_NDJ, keyerror2 = AverageMeridional(sst3_NDJ)
                        sst3_MAM, keyerror3 = AverageMeridional(sst3_MAM)
                        keyerror = add_up_errors([keyerror, keyerror1, keyerror2, keyerror3])
                        if keyerror is None:
                            my_units = "" if kwargs["normalization"] is True else "C"
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {
                                "units": my_units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "monthly standard deviation of " + sstbox + " sstA",
                                "diagnostic_value": mv, "diagnostic_value_error": mv_error}
                            dict2 = {
                                "units": my_units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of monthly standard deviation of " +
                                               "equatorial_pacific sstA"}
                            dict3 = {
                                "units": my_units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "zonal curve of standard deviation of equatorial_pacific sstA (during " +
                                               "NDJ)"}
                            dict4 = {
                                "units": my_units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "zonal curve of standard deviation of equatorial_pacific sstA (during " +
                                               "MAM)"}
                            dict5 = {
                                "units": my_units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "map of standard deviation of equatorial_pacific sstA (during NDJ)"}
                            dict6 = {
                                "units": my_units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "map of standard deviation of equatorial_pacific sstA (during MAM)"}
                            dict7 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, global_attributes=dict7,
                                var1=sstStd_monthly, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                                var2=sst2, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                                var3=sst3_NDJ, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                                var4=sst3_MAM, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                                var5=sst1_NDJ, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                                var6=sst1_MAM, var6_attributes=dict6, var6_name=ovar[5] + dataset)
                            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears": yearN, "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoDiversity(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
                  dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoDiversity() function computes a zonal composite of El Nino and La Nina events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > (<) 'threshold' during 'season' are considered as El Nino (La Nina) events
        2.) diversity of the zonal location of the maximum (minimum) SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the maximum (minimum) SSTA for each selected event
            2.3) compute the percentage of EP events (maximum/minimum SSTA eastward of the given threshold)
            2.4) compute the ratio EP events during La Nina divided by EP events during El Nino

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoDiversity_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param treshold_ep_ev: float, optional
        see EnsoToolsLib.percentage_val_eastward
        longitude, in degree east, of the westward boundary of eastern Pacific event
        default value is -140°E (i.e., 140°W)
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, ref, keyerror,
        dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    threshold = event_definition["threshold"]
    normalize = event_definition["normalization"]
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "treshold_ep_ev",
                    "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO Diversity (percentage of eastern Pacific El Nino / La Nina)"
    lat = ReferenceRegions(box)["latitude"]
    lon = ReferenceRegions(box)["longitude"]
    Method = "Nino (Nina) events = " + region_ev + " sstA > (<) " + str(threshold) + " during " + season_ev + \
             ", zonal SSTA " + "(meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) + \
             "]), westward boundary of EP events " + str(kwargs["treshold_ep_ev"]) + "E"
    Units = "%"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoDiversity"
    if metname == "":
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    mv, mv_error, nino_years, nina_years, dive_down_diag = None, None, None, None, {"value": None, "axis": None}
    if keyerror is None:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, _, keyerror = PreProcessTS(
            sst, "", areacell=areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        del areacell
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "time1": "(sst) " + str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # 1.2 SSTA > (<) 'threshold' during 'season' are considered as El Nino (La Nina) events
            # Lists event years
            nino_years = DetectEvents(sst, season_ev, threshold, normalization=normalize, nino=True)
            nina_years = DetectEvents(sst, season_ev, -threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {"nino1": "nbr(" + str(len(nino_years)) + "): " + str(nino_years),
                              "nina1": "nbr(" + str(len(nina_years)) + "): " + str(nina_years)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the minimum SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst, areacell, keyerror = Read_data_mask_area(
                sstfile, sstname, "temperature", metric, box, file_area=sstareafile, name_area=sstareaname,
                file_mask=sstlandmaskfile, name_mask=sstlandmaskfile, maskland=True, maskocean=False, debug=debug,
                **kwargs)
            if keyerror is None:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst, Method, keyerror = PreProcessTS(
                    sst, Method, areacell=areacell, average=False, compute_anom=False, region=box, **kwargs)
                del areacell
                if keyerror is None:
                    if debug is True:
                        dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                                      "shape1": "(sst) " + str(sst.shape), "time1": "(sst) " + str(TimeBounds(sst))}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

                    # Seasonal mean
                    sst = SeasonalMean(sst, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                                      "shape1": "(sst) " + str(sst.shape)}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs["regridding"], dict):
                        known_args = {"newgrid", "missing", "order", "mask", "newgrid_name", "regridder", "regridTool",
                                      "regridMethod"}
                        extra_args = set(kwargs["regridding"]) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst = Regrid(sst, None, region=box, **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                                          "shape1": "(sst) " + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

                    # Meridional average
                    sst, keyerror = AverageMeridional(sst)
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                                          "shape1": "(sst) " + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                        # samples
                        sample_nino = Event_selection(sst, kwargs["frequency"], list_event_years=nino_years)
                        sample_nina = Event_selection(sst, kwargs["frequency"], list_event_years=nina_years)

                        # 2.2 find the zonal position of the maximum/minimum SSTA for each selected event
                        lon_sstmax = FindXYMinMaxInTs(
                            sample_nino, return_val="maxi", smooth=True, axis=0, window=5, method="triangle")
                        lon_sstmin = FindXYMinMaxInTs(
                            sample_nina, return_val="mini", smooth=True, axis=0, window=5, method="triangle")
                        if debug is True:
                            dict_debug = {"line1": "longitude of the maximum SSTA (nino): " + str(lon_sstmax),
                                          "line2": "longitude of the minimum SSTA (nina): " + str(lon_sstmin)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after FindXYMinMaxInTs", 15, **dict_debug)

                        # 2.3 compute the percentage of EP events (maximum/minimum SSTA eastward of the given threshold)
                        ep_event_nino, keyerror_nino = percentage_val_eastward(
                            lon_sstmax, metric, box, threshold=kwargs["treshold_ep_ev"])
                        ep_event_nina, keyerror_nina = percentage_val_eastward(
                            lon_sstmin, metric, box, threshold=kwargs["treshold_ep_ev"])

                        keyerror = add_up_errors([keyerror_nino, keyerror_nina])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"nino1": "percentage of EP event + " + str(ep_event_nino),
                                              "nina1": "percentage of EP event + " + str(ep_event_nina)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

                            # 2.4 compute the ratio EP events during La Nina divided by EP events during El Nino
                            mv = float(ep_event_nina / ep_event_nino)
                            # Standard Error of the Standard Deviation (function of nyears)
                            mv_error = None

                            # Dive down diagnostic
                            dive_down_diag = {"value": ArrayToList(lon_sstmax), "axis": list(lon_sstmax.getAxis(0)[:])}
                            if netcdf is True:
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": "longitude (E)", "number_of_years_used": yearN,
                                         "time_period": str(actualtimebounds), "nino_years": str(nino_years),
                                         "diagnostic_value_" + dataset: mv,
                                         "diagnostic_value_error_" + dataset: mv_error}
                                dict2 = {"units": "longitude (E)", "number_of_years_used": yearN,
                                         "time_period": str(actualtimebounds), "nina_years": str(nina_years),
                                         "diagnostic_value_" + dataset: mv,
                                         "diagnostic_value_error_" + dataset: mv_error}
                                dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                         "frequency": kwargs["frequency"]}
                                SaveNetcdf(file_name, var1=lon_sstmax, var1_attributes=dict1,
                                           var1_name="Nino_lon_pos_maxSSTA__" + dataset, var2=lon_sstmin,
                                           var2_attributes=dict2, var2_name="Nina_lon_pos_minSSTA__" + dataset,
                                           global_attributes=dict3)
                                del dict1, dict2, dict3
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "nyears": yearN,
        "events": nino_years + nina_years, "time_frequency": kwargs["frequency"], "time_period": actualtimebounds,
        "ref": Ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSstDiversity(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box,
                     event_definition, dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstDiversity() function computes a zonal composite of El Nino and La Nina events during the peak of the
    event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > 'threshold' (SSTA < -'threshold') during 'season' are considered as El Nino (La Nina) events
        2.) diversity of the zonal location of the maximum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the maximum SSTA for each selected event
            2.3) compute the spread of the distribution

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstDiversity_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, ref, keyerror,
        dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        enso_method = " ("
        if isinstance(smooth_ev, dict) is True:
            enso_method += "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            if isinstance(length_ev, int) is True:
                enso_method += "; "
        if isinstance(length_ev, int) is True:
            enso_method += "threshold met during at least " + str(length_ev) + " consecutive months"
        enso_method += ")"
    else:
        enso_method = ""
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(box)["latitude"]
    Name = "ENSO Diversity (interquartile range)"
    Units = "long"
    Method = "Nino (Nina) events = " + region_ev + " sstA > " + my_thresh + " (< -" + my_thresh + ") during " + \
             season_ev + enso_method + ", zonal SSTA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) + \
             "]), zonal location of the maximum (minimum) SSTA detected for each event, the diversity is the " + \
             "interquartile range (IQR = Q3 - Q1)"
    Ref = "Using CDAT regridding"
    metric = "EnsoSstDiversity"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    sstmap, sstmap_areacell, keyerror2 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, box, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    mv1, mv1_err, mv2, mv2_err, dive_down_diag = None, None, None, None, {"value": None, "axis": None}
    mv3, mv3_err, mv4, mv4_err, mv5, mv5_err, mv6, mv6_err = None, None, None, None, None, None, None, None
    ln_years, en_years = list(), list()
    keyerror = add_up_errors([keyerror1, keyerror2])
    if keyerror is None:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        enso_sst, _, keyerror1 = PreProcessTS(
            sst, "", areacell=sst_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sstmap, Method, keyerror2 = PreProcessTS(
            sstmap, Method, areacell=sstmap_areacell, average=False, compute_anom=False, region=box, **kwargs)
        kwargs["smoothing"] = smooth
        del sst_areacell, sstmap_areacell
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst ts) " + str([ax.id for ax in enso_sst.getAxisList()]),
                              "axes2": "(sst map) " + str([ax.id for ax in sstmap.getAxisList()]),
                              "shape1": "(sst ts) " + str(enso_sst.shape), "shape2": "(sst map) " + str(sstmap.shape),
                              "time1": "(sst ts) " + str(TimeBounds(enso_sst)),
                              "time2": "(sst map) " + str(TimeBounds(sstmap))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # 1.2 SSTA > 'threshold' (SSTA < -'threshold') during 'season' are considered as El Nino (La Nina) events
            # Lists event years
            ln_years = DetectEvents(enso_sst, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                    compute_season=True, duration=length_ev)
            en_years = DetectEvents(enso_sst, season_ev, thresh_ev, normalization=normalize, nino=True,
                                    compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": "nbr(" + str(len(ln_years)) + "): " + str(ln_years),
                              "nino1": "nbr(" + str(len(en_years)) + "): " + str(en_years)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the maximum SSTA
            # ------------------------------------------------
            # 2.1 zonal SSTA at the peak of the event is computed for each selected event
            # Seasonal mean
            sstmap = SeasonalMean(sstmap, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sstmap.getAxisList()]),
                              "shape1": "(sst) " + str(sstmap.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"newgrid", "missing", "order", "mask", "newgrid_name", "regridder", "regridTool",
                              "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sstmap = Regrid(sstmap, None, region=box, **kwargs["regridding"])
                Method += ", observations and model regridded to " + str(kwargs["regridding"]["newgrid_name"])
                if debug is True:
                    dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sstmap.getAxisList()]),
                                  "shape1": "(sst) " + str(sstmap.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            sstlon, keyerror = AverageMeridional(sstmap)
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sstlon.getAxisList()]),
                                  "shape1": "(sst) " + str(sstlon.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                if len(ln_years) > 0:
                    # samples
                    ln_sample = Event_selection(sstlon, kwargs["frequency"], list_event_years=ln_years)
                    # 2.2 find the zonal position of the minimum SSTA for each selected event
                    ln_lon = FindXYMinMaxInTs(
                        ln_sample, return_val="mini", smooth=True, axis=0, window=19, method="triangle")
                    Method += ", zonal SSTA smoothed using a triangle shaped window of 19 points"
                    if debug is True:
                        dict_debug = {"line1": "longitude of the minimum SSTA: " + str(ln_lon)}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after FindXYMinMaxInTs", 15, **dict_debug)
                else:
                    ln_sample = MyEmpty(sstlon[:5, 0], time=True, time_id="years")
                    ln_lon = MyEmpty(sstlon[:5, 0], time=True, time_id="years")

                if len(en_years) > 0:
                    # samples
                    en_sample = Event_selection(sstlon, kwargs["frequency"], list_event_years=en_years)
                    # 2.2 find the zonal position of the maximum SSTA for each selected event
                    en_lon = FindXYMinMaxInTs(
                        en_sample, return_val="maxi", smooth=True, axis=0, window=19, method="triangle")
                    if debug is True:
                        dict_debug = {"line1": "longitude of the maximum SSTA: " + str(en_lon)}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after FindXYMinMaxInTs", 15, **dict_debug)
                else:
                    en_sample = MyEmpty(sstlon[:5, 0], time=True, time_id="years")
                    en_lon = MyEmpty(sstlon[:5, 0], time=True, time_id="years")

                if len(ln_years) > 0 and len(en_years) > 0:
                    enso_sample = Concatenate(-1 * ln_sample, en_sample, events1=ln_years, events2=en_years)
                    enso_lon = Concatenate(ln_lon, en_lon, events1=ln_years, events2=en_years)
                    # 2.3 compute the spread of the distribution
                    mv1 = statistical_dispersion(enso_lon, method="IQR")
                    mv2 = statistical_dispersion(enso_lon, method="MAD")
                    mv1_err, mv2_err = None, None
                else:
                    if len(ln_years) > 0:
                        enso_sample = -1 * deepcopy(ln_sample)
                        enso_lon = deepcopy(ln_lon)
                    else:
                        enso_sample = deepcopy(en_sample)
                        enso_lon = deepcopy(en_lon)
                if len(ln_years) > 0:
                    mv3 = statistical_dispersion(ln_lon, method="IQR")
                    mv4 = statistical_dispersion(ln_lon, method="MAD")
                    mv3_err, mv4_err = None, None
                if len(en_years) > 0:
                    mv5 = statistical_dispersion(en_lon, method="IQR")
                    mv6 = statistical_dispersion(en_lon, method="MAD")
                    mv5_err, mv6_err = None, None

                # Dive down diagnostic
                dive_down_diag = {"value": ArrayToList(enso_lon), "axis": list(enso_lon.getAxis(0)[:])}

                if netcdf is True:
                    # save
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {
                        "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                        "nina_years": str(ln_years), "nino_years": str(en_years),
                        "description": "zonal location of maximum SSTA (-1 * Nina sstA) during both El Nino and La " +
                                       "Nina; " + season_ev + " SSTA averaged in [" + str(lat[0]) + " ; " +
                                       str(lat[1]) + "]; Nino (Nina) events = " + region_ev + " sstA > " + my_thresh +
                                       " (< -" + my_thresh + ") during " + season_ev + enso_method,
                        "diagnostic_value_" + dataset: mv1, "diagnostic_value_error_" + dataset: mv1_err,
                        "diagnostic_value2_" + dataset: mv2, "diagnostic_value_error2_" + dataset: mv2_err}
                    dict2 = {
                        "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                        "nina_years": str(ln_years),
                        "description": "zonal location of maximum SSTA (-1 * Nina sstA) during La Nina; " + season_ev +
                                       " SSTA averaged in [" + str(lat[0]) + " ; " + str(lat[1]) + "]; Nina events = " +
                                       region_ev + " sstA < -" + my_thresh + ") during " + season_ev + enso_method,
                        "diagnostic_value_" + dataset: mv3, "diagnostic_value_error_" + dataset: mv3_err,
                        "diagnostic_value2_" + dataset: mv4, "diagnostic_value_error2_" + dataset: mv4_err}
                    dict3 = {
                        "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                        "nino_years": str(en_years),
                        "description": "zonal location of maximum SSTA during El Nino; " + season_ev + " SSTA " +
                                       "averaged in [" + str(lat[0]) + " ; " + str(lat[1]) + "]; Nino events = " +
                                       region_ev + " sstA > " + my_thresh + " during " + season_ev + enso_method,
                        "diagnostic_value_" + dataset: mv5, "diagnostic_value_error_" + dataset: mv5_err,
                        "diagnostic_value2_" + dataset: mv6, "diagnostic_value_error2_" + dataset: mv6_err}
                    dict4 = {
                        "units": "C", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                        "nina_years": str(ln_years), "nino_years": str(en_years),
                        "description": "zonal SSTA (-1 * Nina sstA) during both El Nino and La Nina; " + season_ev +
                                       " SSTA averaged in [" + str(lat[0]) + " ; " + str(lat[1]) + "]; Nino (Nina) " +
                                       "events = " + region_ev + " sstA > " + my_thresh + " (< -" + my_thresh +
                                       ") during " + season_ev + enso_method}
                    dict5 = {
                        "units": "C", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                        "nina_years": str(ln_years),
                        "description": "zonal SSTA (-1 * Nina sstA) during La Nina; " + season_ev + " SSTA averaged " +
                                       "in [" + str(lat[0]) + " ; " + str(lat[1]) + "]; Nina events = " + region_ev +
                                       " sstA < -" + my_thresh + ") during " + season_ev + enso_method}
                    dict6 = {
                        "units": "C", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                        "nino_years": str(en_years),
                        "description": "zonal SSTA during El Nino; " + season_ev + " SSTA averaged in [" + str(lat[0]) +
                                       " ; " + str(lat[1]) + "]; Nino " + "events = " + region_ev + " sstA > " +
                                       my_thresh + " during " + season_ev + enso_method}
                    dict7 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                             "frequency": kwargs["frequency"]}
                    SaveNetcdf(
                        file_name, global_attributes=dict7,
                        var1=enso_lon, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                        var2=enso_sample, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                        var3=ln_lon, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                        var4=ln_sample, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                        var5=en_lon, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                        var6=en_sample, var6_attributes=dict6, var6_name=ovar[5] + dataset)
                    del dict1, dict2, dict3, dict4, dict5, dict6, dict7, file_name

    # metric value
    if debug is True:
        dict_debug = {
            "line1": "diagnostic (IQR) value: " + str(mv1), "line2": "diagnostic (IQR) value_error: " + str(mv1_err),
            "line3": "diagnostic (MAD) value: " + str(mv2), "line4": "diagnostic (MAD) value_error: " + str(mv2_err)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv1, "value_error": mv1_err, "value2": mv2, "value_error2": mv2_err, "units": Units,
        "method": Method, "nyears": yearN, "events": ln_years + en_years, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": Ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoDuration(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
                 nbr_years_window, dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoDuration() function computes sea surface temperature anomalies life cycle associated with ENSO in a 'box'
    (usually the nino3.4) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and lagging
    ENSO), the duration is then the period, around the peak, during which the life cycle is above 0.25.

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box (e.g. 'nino3.4') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoDuration_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        enso_method = " ("
        if isinstance(smooth_ev, dict) is True:
            enso_method += "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            if isinstance(length_ev, int) is True:
                enso_method += "; "
        if isinstance(length_ev, int) is True:
            enso_method += "threshold met during at least " + str(length_ev) + " consecutive months"
        enso_method += ")"
    else:
        enso_method = ""
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO Duration based on life cyle SSTA pattern"
    Units = "months"
    Method = "SSTA during " + str(nbr_years_window) + " years (centered on ENSO) regressed onto " + region_ev + \
             " SSTA during " + season_ev + ", the duration is the number of consecutive months during which the " + \
             "regression is above 0.25"
    Ref = "Using CDAT"
    metric = "EnsoDuration"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "axis": None}
    if keyerror is None:
        # ------------------------------------------------
        # 1. box SSTA
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        enso_sst, _, keyerror = PreProcessTS(
            sst, "", areacell=areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        kwargs["smoothing"] = smooth
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": str([ax.id for ax in enso_sst.getAxisList()]), "shape1": str(enso_sst.shape),
                              "time1": str(TimeBounds(enso_sst))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # 1.2 Seasonal mean and anomalies
            enso = SeasonalMean(enso_sst, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {"axes1": str([ax.id for ax in enso.getAxisList()]),
                              "shape1": str(enso.shape), "time1": str(TimeBounds(enso))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 2. temporal SSTA
            # ------------------------------------------------
            # 2.1 SSTA in 'box' are normalized / detrended / smoothed (running average) if applicable
            # Preprocess ts (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
            sst, Method, keyerror = PreProcessTS(
                sst, Method, areacell=areacell, average="horizontal", compute_anom=True, region=region_ev, **kwargs)
            del areacell
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": str([ax.id for ax in sst.getAxisList()]), "shape1": str(sst.shape),
                                  "time1": str(TimeBounds(sst))}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

                # ------------------------------------------------
                # 3. Regression time series
                # ------------------------------------------------
                sstts = LinearRegressionTsAgainstTs(
                    sst, enso, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
                if debug is True:
                    dict_debug = {"axes1": str([ax.id for ax in sstts.getAxisList()]), "shape1": str(sstts.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

                # ------------------------------------------------
                # 4. Duration
                # ------------------------------------------------
                # 4.1 count the number of consecutive month above a threshold
                mv = float(DurationEvent(sstts, 0.25, nino=True, debug=debug))

                # Error on the metric
                mv_error = None

                # Dive down diagnostic
                dive_down_diag = {"value": ArrayToList(sstts), "axis": list(sstts.getAxis(0)[:])}

                if netcdf is True:
                    # Lists event years
                    nina_years = DetectEvents(enso_sst, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                              compute_season=True, duration=length_ev)
                    nino_years = DetectEvents(enso_sst, season_ev, thresh_ev, normalization=normalize, nino=True,
                                              compute_season=True, duration=length_ev)
                    if debug is True:
                        dict_debug = {"nina1": "nbr(" + str(len(nina_years)) + "): " + str(nina_years),
                                      "nino1": "nbr(" + str(len(nino_years)) + "): " + str(nino_years)}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                    # composites
                    lth = 0.9 * thresh_ev
                    if normalize is True:
                        lth = lth * float(Std(enso))
                    if len(nina_years) > 0:
                        sample = Event_selection(
                            sst, kwargs["frequency"], nbr_years_window=nbr_years_window, list_event_years=nina_years)
                        # count the number of consecutive month bellow a threshold
                        nina_dur_all = DurationAllEvent(sample, -lth, nino=False, debug=debug)
                        nina_dur_mean = float(nina_dur_all.mean())
                        nina_dur_err = float(Std(nina_dur_all) / NUMPYsqrt(len(nina_dur_all)))
                    else:
                        nina_dur_all = MyEmpty(sst[:5], time=True, time_id="years")
                        nina_dur_mean = None
                        nina_dur_err = None
                    if len(nino_years) > 0:
                        sample = Event_selection(
                            sst, kwargs["frequency"], nbr_years_window=nbr_years_window, list_event_years=nino_years)
                        # count the number of consecutive month above a threshold
                        nino_dur_all = DurationAllEvent(sample, lth, nino=True, debug=debug)
                        nino_dur_mean = float(nino_dur_all.mean())
                        nino_dur_err = float(Std(nino_dur_all) / NUMPYsqrt(len(nino_dur_all)))
                    else:
                        nino_dur_all = MyEmpty(sst[:5], time=True, time_id="years")
                        nino_dur_mean = None
                        nino_dur_err = None
                    if debug is True:
                        dict_debug = {"nina1": "mean(" + str(nina_dur_mean) + "): " + str(nina_dur_all),
                                      "nino1": "mean(" + str(nino_dur_mean) + "): " + str(nino_dur_all)}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after DurationAllEvent", 15, **dict_debug)
                    # save
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    my_thresh = "std" if normalize is True else "C"
                    dict1 = {
                        "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                        "description": "time series of SSTA during " + str(nbr_years_window) + " years (centered on " +
                                       "ENSO) regressed onto " + region_ev + " SSTA during " + season_ev,
                        "diagnostic_value": mv, "diagnostic_value_error": mv_error}
                    dict2 = {
                        "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                        "nina_years": str(nina_years),
                        "description": "duration of La Nina events; Nina events = " + region_ev + " SSTA < -" +
                                       my_thresh + " during " + season_ev + enso_method + ", duration is the number " +
                                       "of consecutive months during which SSTA < -" + my_thresh,
                        "diagnostic_value": nina_dur_mean, "diagnostic_value_error": nina_dur_err}
                    dict3 = {
                        "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                        "nino_years": str(nino_years),
                        "description": "duration of El Nino events; Nino events = " + region_ev + " SSTA > " +
                                       my_thresh + " during " + season_ev + enso_method + ", duration is the number " +
                                       "of consecutive months during which SSTA > " + my_thresh,
                        "diagnostic_value": nino_dur_mean, "diagnostic_value_error": nino_dur_err}
                    dict4 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                             "frequency": kwargs["frequency"]}
                    SaveNetcdf(
                        file_name, global_attributes=dict4,
                        var1=sstts, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                        var2=nina_dur_all, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                        var3=nino_dur_all, var3_attributes=dict3, var3_name=ovar[2] + dataset)
                    del dict1, dict2, dict3, dict4
    # metric value
    if debug is True:
        dict_debug = {
            "line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "nyears": yearN,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": Ref, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def EnsodSstOce(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox,
                thffile, thfname, thfareafile, thfareaname, thflandmaskfile, thflandmaskname, thfbox,
                event_definition, dataset="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsodSstOce() function computes an estimation of the SST change caused by an anomalous ocean circulation
    (usually in nino3)
    For this, the (THF) total heat flux is integrated from June to December (representing SST change driven by heat
    fluxes) and subtracted to the SST change during this period. dSSToce = dSST - dSSTthf
    The total heat flux is the sum of four term:
         - net surface shortwave radiation,
         - net surface longwave radiation,
         - latent heat flux,
         - sensible heat flux

    The total heat flux is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into thffile and thfname of this
    function a list() of the four needed files and variable names (CMIP: rsds-rsus, rlds-rlus, hfls, hfss)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Jul 18 2019

    Based on:
    Bayr, T., C. Wengel, M. Latif, D. Dommenget, J. Lübbecke, W. Park (2018) Error compensation of ENSO atmospheric
    feedbacks in climate models and its influence on simulated ENSO dynamics. Clim. Dyn., doi:10.1007/s00382-018-4575-7

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param thffile: string
        path_to/filename of the file (NetCDF) of THF
    :param thfname: string
        name of THF variable (thf, netflux, thflx, swr + lwr + lhf + shf) (may be a list of variables) in 'thffile'
    :param thfareafile: string
        path_to/filename of the file (NetCDF) of the areacell for THF
    :param thfareaname: string
        name of areacell variable (areacella, areacello) in 'thfareafile'
    :param thflandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for THF
    :param thflandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfile'
    :param thfbox: string
        name of box (nino3') for THF
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsodSstOce_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        enso_method = " ("
        if isinstance(smooth_ev, dict) is True:
            enso_method += "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            if isinstance(length_ev, int) is True:
                enso_method += "; "
        if isinstance(length_ev, int) is True:
            enso_method += "threshold met during at least " + str(length_ev) + " consecutive months"
        enso_method += ")"
    else:
        enso_method = ""
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "SST change caused by an anomalous ocean circulation (dSSToce)"
    Units = "C/C"
    Method = "Nino (Nina) events = " + region_ev + " sstA > " + my_thresh + " (< -" + my_thresh + ") during " + \
             season_ev + enso_method + ", dSSToce = dSST - dSSTthf during ENSO events (relative difference between " + \
             sstbox + " SST change and heat flux-driven " + thfbox + " SST change from Jul to Dec)"
    Ref = "Using CDAT"
    metric = "EnsodSstOce"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    enso, enso_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    sst, sst_areacell, keyerror2 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    thf, thf_areacell, keyerror3 = Read_data_mask_area_multifile(
        thffile, thfname, "heat flux", "thf", metric, thfbox, file_area=thfareafile, name_area=thfareaname,
        file_mask=thflandmaskfile, name_mask=thflandmaskname, maskland=True, maskocean=False, debug=debug,
        interpreter="project_interpreter_var2", **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst, thf, keyerror4 = CheckTime(sst, thf, metric_name=metric, debug=debug, **kwargs)
    sst, enso, _ = CheckTime(sst, enso, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)
    mv, mv_error, dive_down_diag = None, None, {"value": None, "value2": None, "value3": None, "axis": None}
    nina_years, nino_years = list(), list()
    keyerror = add_up_errors([keyerror1, keyerror2, keyerror3, keyerror4])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, averages horizontally)
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        enso_sst, _, keyerror1 = PreProcessTS(
            enso, "", areacell=enso_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        kwargs["smoothing"] = smooth
        sst, Method, keyerror2 = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        thf, _, keyerror3 = PreProcessTS(
            thf, "", areacell=thf_areacell, average="horizontal", compute_anom=True, region=thfbox, **kwargs)
        del enso_areacell, sst_areacell, thf_areacell
        keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "axes2": "(thf) " + str([ax.id for ax in thf.getAxisList()]),
                              "axes3": "(enso) " + str([ax.id for ax in enso_sst.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "shape2": "(thf) " + str(thf.shape),
                              "shape3": "(thf) " + str(enso_sst.shape), "time1": "(sst) " + str(TimeBounds(sst)),
                              "time2": "(thf) " + str(TimeBounds(thf)), "time3": "(enso) " + str(TimeBounds(enso_sst))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Lists event years
            nina_years = DetectEvents(enso_sst, season_ev, -thresh_ev, normalization=normalize, nino=False,
                                      compute_season=True, duration=length_ev)
            nino_years = DetectEvents(enso_sst, season_ev, thresh_ev, normalization=normalize, nino=True,
                                      compute_season=True, duration=length_ev)
            if debug is True:
                dict_debug = {"nina1": str(nina_years), "nino1": str(nino_years)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

            # SST change
            dSST, dSSTthf, dSSToce = SlabOcean(
                sst, thf, "JUN", "DEC", nina_years + nino_years, frequency=kwargs["frequency"], debug=debug)

            # Mean SST change caused by an anomalous ocean circulation during ENSO events
            mv = float(dSSToce[-1])
            mv_error = None

            # Dive down diagnostic
            dive_down_diag = {"value": ArrayToList(dSST), "value2": ArrayToList(dSSTthf),
                              "value3": ArrayToList(dSSToce), "axis": list(dSST.getAxis(0)[:])}

            if netcdf is True:
                sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                thf_map, thf_map_areacell, keyerror2 = Read_data_mask_area_multifile(
                    thffile, thfname, "heat flux", "thf", metric, "equatorial_pacific", file_area=thfareafile,
                    name_area=thfareaname, file_mask=thflandmaskfile, name_mask=thflandmaskname, maskland=True,
                    maskocean=False, debug=debug, interpreter="project_interpreter_var2", **kwargs)
                # Checks if the same time period is used for both variables
                sst_map, thf_map, keyerror3 = CheckTime(sst_map, thf_map, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
                    sst_map, _, keyerror1 = PreProcessTS(
                        sst_map, "", areacell=sst_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    thf_map, _, keyerror2 = PreProcessTS(
                        thf_map, "", areacell=thf_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del thf_map_areacell, sst_map_areacell
                    keyerror = add_up_errors([keyerror1, keyerror2])
                    if keyerror is None:
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        thf_map = Regrid(thf_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                          "axes2": "(thf) " + str([ax.id for ax in thf_map.getAxisList()]),
                                          "shape1": "(sst) " + str(sst_map.shape),
                                          "shape2": "(thf) " + str(thf_map.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid", 15, **dict_debug)
                        # Meridional average
                        sst_map, keyerror1 = AverageMeridional(sst_map)
                        thf_map, keyerror2 = AverageMeridional(thf_map)
                        keyerror = add_up_errors([keyerror1, keyerror2])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(thf) " + str([ax.id for ax in thf_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(thf) " + str(thf_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Zonal smoothing
                            sst_map, _ = Smoothing(sst_map, "", axis=1, window=51, method="square")
                            thf_map, _ = Smoothing(thf_map, "", axis=1, window=51, method="square")
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(thf) " + str([ax.id for ax in thf_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(thf) " + str(thf_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing", 15, **dict_debug)
                            # SST change
                            lth = deepcopy(thresh_ev)
                            if normalize is True:
                                enso = SeasonalMean(enso_sst, season_ev, compute_anom=True)
                                lth = lth * float(Std(enso))
                            hovdSST, hovdSSTthf, hovdSSToce = SlabOcean(
                                sst_map, thf_map, "JUN", "DEC", nina_years + nino_years, frequency=kwargs["frequency"],
                                tmin=lth, debug=debug)
                            curdSSTthf, curdSSToce = hovdSSTthf[-1], hovdSSToce[-1]
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(zonal curdSSTthf) " + str([ax.id for ax in curdSSTthf.getAxisList()]),
                                    "axes2": "(hovtx hovdSST) " + str([ax.id for ax in hovdSST.getAxisList()]),
                                    "shape1": "(zonal curdSSTthf) " + str(curdSSTthf.shape),
                                    "shape2": "(hovtx hovdSST) " + str(hovdSST.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after SlabOcean", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "evolution of " + sstbox + " SST change from Jul to Dec prior ENSO " +
                                               "events",
                                "diagnostic_value": mv, "diagnostic_value_error": mv_error,
                                "nina_years": str(nina_years), "nino_years": str(nino_years)}
                            dict2 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "evolution of heat flux-driven " + sstbox + " SST change from Jul to " +
                                               "Dec prior ENSO events",
                                "diagnostic_value": mv, "diagnostic_value_error": mv_error,
                                "nina_years": str(nina_years), "nino_years": str(nino_years)}
                            dict3 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "evolution of " + sstbox + " SST change caused by an anomalous ocean " +
                                               "circulation from Jul to Dec prior ENSO events",
                                "diagnostic_value": mv, "diagnostic_value_error": mv_error,
                                "nina_years": str(nina_years), "nino_years": str(nino_years)}
                            dict4 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "zonal slice during Dec of heat flux-driven " + sstbox + " SST change " +
                                               "from Jul to Dec prior ENSO events",
                                "nina_years": str(nina_years), "nino_years": str(nino_years)}
                            dict5 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "zonal slice during Dec of " + sstbox + " SST change caused by an " +
                                               "anomalous ocean circulation from Jul to Dec prior ENSO events",
                                "nina_years": str(nina_years), "nino_years": str(nino_years)}
                            dict6 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of " + sstbox + " SST change from" +
                                               " Jul to Dec prior ENSO events",
                                "nina_years": str(nina_years), "nino_years": str(nino_years)}
                            dict7 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of heat flux-driven " + sstbox +
                                               " SST change from Jul to Dec prior ENSO events",
                                "nina_years": str(nina_years), "nino_years": str(nino_years)}
                            dict8 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of " + sstbox + " SST change " +
                                               "caused by an anomalous ocean circulation from Jul to Dec prior ENSO " +
                                               "events",
                                "nina_years": str(nina_years), "nino_years": str(nino_years)}
                            dict9 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, global_attributes=dict9,
                                var1=dSST, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                                var2=dSSTthf, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                                var3=dSSToce, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                                var4=curdSSTthf, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                                var5=curdSSToce, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                                var6=hovdSST, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                                var7=hovdSSTthf, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                                var8=hovdSSToce, var8_attributes=dict8, var8_name=ovar[7] + dataset)
                            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "nyears": yearN,
        "events": sorted(nina_years + nino_years), "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": Ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoPrLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox,
                  prbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False,
                  netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrLonRmse() function computes precipitation anomalies pattern associated with ENSO in a 'prbox'
    (usually the equatorial_pacific).
    It is the regression of 'prbox' averaged PRA (precipitation anomalies) time series onto 'region_ev' averaged
    SSTA (sea surface temperature anomalies) (usually the regression of equatorial_pacific PRA onto nino3.4 SSTA).

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param prbox: string
        name of box (e.g. 'equatorial_pacific') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(prbox)["latitude"]
    Name = "ENSO zonal PRA pattern"
    Units = "" if kwargs["normalization"] else "mm/day/C"
    Method = "zonal curve of " + prbox + " precipitation anomalies (meridional averaged [" + str(lat[0]) + " ; " + \
             str(lat[1]) + "]) regressed onto " + region_ev + " averaged SSTA during " + season_ev + enso_method3
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoPrLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, pr_mod, keyerror_mod3 = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, pr_obs, keyerror_obs3 = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2, keyerror_mod3, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(sst_mod, "", areacell=sst_mod_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(sst_obs, "", areacell=sst_obs_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        # 1.2 PR in 'prbox' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        kwargs["smoothing"] = smooth
        del sst_mod_areacell, sst_obs_areacell, pr_mod_areacell, pr_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                    "axes3": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                    "shape3": "(mod pr) " + str(pr_mod.shape), "shape4": "(obs pr) " + str(pr_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                    "time3": "(mod pr) " + str(TimeBounds(pr_mod)), "time4": "(obs pr) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            # 2.1 horizontally averaged SST
            enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
            # 2.2 map of SST
            pr_mod = SeasonalMean(pr_mod, season_ev, compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                    "shape3": "(mod pr) " + str(pr_mod.shape), "shape4": "(obs pr) " + str(pr_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(enso_obs)),
                    "time3": "(mod pr) " + str(TimeBounds(pr_mod)), "time4": "(obs pr) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(
                    pr_mod, pr_obs, Method, region=prbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(mod pr) " + str(pr_mod.shape), "shape2": "(obs pr) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            pr_lon_mod, keyerror_mod = AverageMeridional(pr_mod)
            pr_lon_obs, keyerror_obs = AverageMeridional(pr_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(mod pr) " + str(pr_mod.shape), "shape2": "(obs pr) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

                # regression
                pr_slope_mod = LinearRegressionTsAgainstMap(pr_lon_mod, enso_mod, return_stderr=False)
                pr_slope_obs = LinearRegressionTsAgainstMap(pr_lon_obs, enso_obs, return_stderr=False)
                if debug is True:
                    dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_slope_mod.getAxisList()]),
                                  "axes2": "(obs pr) " + str([ax.id for ax in pr_slope_obs.getAxisList()]),
                                  "shape1": "(mod pr) " + str(pr_slope_mod.shape),
                                  "shape2": "(obs pr) " + str(pr_slope_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(pr_slope_mod, pr_slope_obs, centered=centered_rmse, biased=biased_rmse)
                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(pr_slope_mod, pr_slope_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(pr_slope_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(pr_slope_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(pr_slope_mod), "observations": ArrayToList(pr_slope_obs),
                                  "axis": list(pr_slope_mod.getAxis(0)[:])}

                if netcdf is True:
                    # Read file and select the right region
                    pr_mod, pr_mod_areacell, keyerror_mod = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitations", metric, "equatorial_pacific_LatExt2",
                        file_area=prareafilemod, name_area=prareanamemod, file_mask=prlandmaskfilemod,
                        name_mask=prlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    pr_obs, pr_obs_areacell, keyerror_obs = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitations", metric, "equatorial_pacific_LatExt2",
                        file_area=prareafileobs, name_area=prareanameobs, file_mask=prlandmaskfileobs,
                        name_mask=prlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    # Checks if the same time period is used for both variables
                    sst_mod, pr_mod, keyerror_mod2 = CheckTime(
                        sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, pr_obs, keyerror_obs2 = CheckTime(
                        sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # Preprocess PR (computes anomalies, normalizes, detrends, smoothes, averages,...)
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        pr_mod, _, keyerror_mod = PreProcessTS(
                            pr_mod, "", areacell=pr_mod_areacell, compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        pr_obs, _, keyerror_obs = PreProcessTS(
                            pr_obs, "", areacell=pr_obs_areacell, compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        kwargs["smoothing"] = smooth
                        del pr_mod_areacell, pr_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                              "axes2": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                              "shape1": "(mod pr) " + str(pr_mod.shape),
                                              "shape2": "(obs pr) " + str(pr_obs.shape),
                                              "time1": "(mod pr) " + str(TimeBounds(pr_mod)),
                                              "time2": "(obs pr) " + str(TimeBounds(pr_obs))}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # Seasonal mean and anomalies
                            pr_mod = SeasonalMean(pr_mod, season_ev, compute_anom=True)
                            pr_obs = SeasonalMean(pr_obs, season_ev, compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                              "axes2": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                              "shape1": "(mod pr) " + str(pr_mod.shape),
                                              "shape2": "(obs pr) " + str(pr_obs.shape),
                                              "time1": "(mod pr) " + str(TimeBounds(pr_mod)),
                                              "time2": "(obs pr) " + str(TimeBounds(pr_obs))}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                pr_mod, pr_obs, _ = TwoVarRegrid(
                                    pr_mod, pr_obs, "", region="equatorial_pacific_LatExt2",
                                    **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                                  "axes2": "(obs pr) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                                  "shape1": "(mod pr) " + str(pr_mod.shape),
                                                  "shape2": "(obs pr) " + str(pr_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                            # Linear regression
                            pr_map_slope_mod = LinearRegressionTsAgainstMap(pr_mod, enso_mod, return_stderr=False)
                            pr_map_slope_obs = LinearRegressionTsAgainstMap(pr_obs, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod pr) " + str([ax.id for ax in pr_map_slope_mod.getAxisList()]),
                                    "axes2": "(obs pr) " + str([ax.id for ax in pr_map_slope_obs.getAxisList()]),
                                    "shape1": "(mod pr) " + str(pr_map_slope_mod.shape),
                                    "shape2": "(obs pr) " + str(pr_map_slope_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # Lists event years
                            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # composites
                            if len(ln_years_mod) > 0:
                                ln_pr_lon_mod = Composite(pr_lon_mod, ln_years_mod, kwargs["frequency"])
                                ln_pr_map_mod = Composite(pr_mod, ln_years_mod, kwargs["frequency"])
                            else:
                                ln_pr_lon_mod = MyEmpty(pr_lon_mod[0], time=False)
                                ln_pr_map_mod = MyEmpty(pr_mod[0], time=False)
                            if len(en_years_mod) > 0:
                                en_pr_lon_mod = Composite(pr_lon_mod, en_years_mod, kwargs["frequency"])
                                en_pr_map_mod = Composite(pr_mod, en_years_mod, kwargs["frequency"])
                            else:
                                en_pr_lon_mod = MyEmpty(pr_lon_mod[0], time=False)
                                en_pr_map_mod = MyEmpty(pr_mod[0], time=False)
                            if len(ln_years_obs) > 0:
                                ln_pr_lon_obs = Composite(pr_lon_obs, ln_years_obs, kwargs["frequency"])
                                ln_pr_map_obs = Composite(pr_obs, ln_years_obs, kwargs["frequency"])
                            else:
                                ln_pr_lon_obs = MyEmpty(pr_lon_obs[0], time=False)
                                ln_pr_map_obs = MyEmpty(pr_obs[0], time=False)
                            if len(en_years_obs) > 0:
                                en_pr_lon_obs = Composite(pr_lon_obs, en_years_obs, kwargs["frequency"])
                                en_pr_map_obs = Composite(pr_obs, en_years_obs, kwargs["frequency"])
                            else:
                                en_pr_lon_obs = MyEmpty(pr_lon_obs[0], time=False)
                                en_pr_map_obs = MyEmpty(pr_obs[0], time=False)
                            # Supplementary metrics
                            my_units = "" if kwargs["normalization"] is True else "mm/day"
                            dict_metric, dict_nc = dict(), dict()
                            nbr = 3
                            my_ev = ["nina", "nino", None, "nina", "nino"]
                            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
                            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
                            my_de = [
                                "zonal curve of " + prbox + " PRA (meridional averaged [" + str(lat[0]) + " ; " +
                                str(lat[1]) + "]) of La Nina events composite during " + season_ev + "; Nina = " +
                                region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "zonal curve of " + prbox + " PRA (meridional averaged [" + str(lat[0]) + " ; " +
                                str(lat[1]) + "]) of El Nino events composite during " + season_ev + "; Nino = " +
                                region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                                "map of tropical Pacific PRA regressed onto " + region_ev + " averaged SSTA during " +
                                season_ev + enso_method3,
                                "map of tropical Pacific PRA of La Nina events composite during " + season_ev + "; " +
                                "Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "map of tropical Pacific PRA of El Nino events composite during " + season_ev +
                                "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                            my_un = [my_units, my_units, Units, my_units, my_units]
                            my_mo = [ln_pr_lon_mod, en_pr_lon_mod, pr_map_slope_mod, ln_pr_map_mod, en_pr_map_mod]
                            my_ob = [ln_pr_lon_obs, en_pr_lon_obs, pr_map_slope_obs, ln_pr_map_obs, en_pr_map_obs]
                            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
                            my_ax = ["0", "0", "xy", "xy", "xy"]
                            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                                dict_metric, dict_nc = fill_dict_axis(
                                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn,
                                    events1=ev1, events2=ev2, description=des)
                                nbr += 2
                            # save
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "arraySTD_" + dataset1: sm_std_mod,
                                     "description": Method.split(", ")[0]}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "arraySTD_" + dataset2: sm_std_obs,
                                     "description": Method.split(", ")[0]}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=pr_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=pr_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, \
                                my_eo, my_mo, my_ob, my_ty, my_un, my_units, nbr
    # metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "ref": Ref,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "time_frequency": kwargs["frequency"], "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSshLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                   sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs,
                   sstbox, sshbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                   debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSshLonRmse() function computes sea surface height anomalies pattern associated with ENSO in a 'sshbox'
    (usually the equatorial_pacific).
    It is the regression of 'sshbox' averaged SSHA (sea surface height anomalies) time series onto 'region_ev' averaged
    SSTA (sea surface temperature anomalies) (usually the regression of equatorial_pacific PRA onto nino3.4 SSTA).

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (zos) in 'sshfilemod'
    :param sshareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SSH areacell
    :param sshareanamemod: string, optional
        name of areacell for the SSH variable (areacella, areacello,...) in 'sshareafilemod'
    :param sshlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SSH landmask
    :param sshlandmasknamemod: string, optional
        name of landmask for the SSH variable (sftlf,...) in 'sshlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SSH areacell
    :param sshareanameobs: string, optional
        name of areacell for the SSH variable (areacella, areacello,...) in 'sshareafileobs'
    :param sshlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SSH landmask
    :param sshlandmasknameobs: string, optional
        name of landmask for the SSH variable (sftlf,...) in 'sshlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param sshbox: string
        name of box (e.g. 'equatorial_pacific') for SSH
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSshLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(sshbox)["latitude"]
    Name = "ENSO zonal SSHA pattern"
    Units = "" if kwargs["normalization"] else "cm/C"
    Method = "zonal curve of " + sshbox + " sea surface height anomalies (meridional averaged [" + str(lat[0]) + " ;" + \
             " " + str(lat[1]) + "]) regressed onto " + region_ev + " averaged SSTA during " + season_ev + enso_method3
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoSshLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    ssh_mod, ssh_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sshfilemod, sshnamemod, "sea surface height", metric, sshbox, file_area=sshareafilemod,
        name_area=sshareanamemod, file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False,
        maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    ssh_obs, ssh_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sshfileobs, sshnameobs, "sea surface height", metric, sshbox, file_area=sshareafileobs,
        name_area=sshareanameobs, file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False,
        maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, ssh_mod, keyerror_mod3 = CheckTime(sst_mod, ssh_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, ssh_obs, keyerror_obs3 = CheckTime(sst_obs, ssh_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2, keyerror_mod3, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(sst_mod, "", areacell=sst_mod_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(sst_obs, "", areacell=sst_obs_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        # 1.2 SSH in 'sshbox' are normalized / detrended / smoothed (running average) if applicable
        ssh_mod, Method, keyerror_mod2 = PreProcessTS(
            ssh_mod, Method, areacell=ssh_mod_areacell, compute_anom=False, region=sshbox, **kwargs)
        ssh_obs, _, keyerror_obs2 = PreProcessTS(
            ssh_obs, "", areacell=ssh_obs_areacell, compute_anom=False, region=sshbox, **kwargs)
        kwargs["smoothing"] = smooth
        del sst_mod_areacell, sst_obs_areacell, ssh_mod_areacell, ssh_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                    "axes3": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                    "axes4": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                    "shape3": "(mod ssh) " + str(ssh_mod.shape), "shape4": "(obs ssh) " + str(ssh_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                    "time3": "(mod ssh) " + str(TimeBounds(ssh_mod)), "time4": "(obs ssh) " + str(TimeBounds(ssh_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            # 2.1 horizontally averaged SST
            enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
            # 2.2 map of SST
            ssh_mod = SeasonalMean(ssh_mod, season_ev, compute_anom=True)
            ssh_obs = SeasonalMean(ssh_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                    "axes4": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                    "shape3": "(mod ssh) " + str(ssh_mod.shape), "shape4": "(obs ssh) " + str(ssh_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(enso_obs)),
                    "time3": "(mod ssh) " + str(TimeBounds(ssh_mod)), "time4": "(obs ssh) " + str(TimeBounds(ssh_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                ssh_mod, ssh_obs, Method = TwoVarRegrid(
                    ssh_mod, ssh_obs, Method, region=sshbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                        "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                        "shape1": "(mod ssh) " + str(ssh_mod.shape), "shape2": "(obs ssh) " + str(ssh_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # change units
            ssh_mod = ssh_mod * 1e2
            ssh_obs = ssh_obs * 1e2

            # Meridional average
            ssh_lon_mod, keyerror_mod = AverageMeridional(ssh_mod)
            ssh_lon_obs, keyerror_obs = AverageMeridional(ssh_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {
                        "axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                        "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                        "shape1": "(mod ssh) " + str(ssh_mod.shape), "shape2": "(obs ssh) " + str(ssh_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

                # regression
                ssh_slope_mod = LinearRegressionTsAgainstMap(ssh_lon_mod, enso_mod, return_stderr=False)
                ssh_slope_obs = LinearRegressionTsAgainstMap(ssh_lon_obs, enso_obs, return_stderr=False)
                if debug is True:
                    dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_slope_mod.getAxisList()]),
                                  "axes2": "(obs ssh) " + str([ax.id for ax in ssh_slope_obs.getAxisList()]),
                                  "shape1": "(mod ssh) " + str(ssh_slope_mod.shape),
                                  "shape2": "(obs ssh) " + str(ssh_slope_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(ssh_slope_mod, ssh_slope_obs, centered=centered_rmse, biased=biased_rmse)
                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(ssh_slope_mod, ssh_slope_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(ssh_slope_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(ssh_slope_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(ssh_slope_mod), "observations": ArrayToList(ssh_slope_obs),
                                  "axis": list(ssh_slope_mod.getAxis(0)[:])}

                if netcdf is True:
                    # Read file and select the right region
                    ssh_mod, ssh_mod_areacell, keyerror_mod = Read_data_mask_area(
                        sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific_LatExt2",
                        file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                        name_mask=sshlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    ssh_obs, ssh_obs_areacell, keyerror_obs = Read_data_mask_area(
                        sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific_LatExt2",
                        file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                        name_mask=sshlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    # Checks if the same time period is used for both variables
                    sst_mod, ssh_mod, keyerror_mod2 = CheckTime(
                        sst_mod, ssh_mod, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, ssh_obs, keyerror_obs2 = CheckTime(
                        sst_obs, ssh_obs, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # Preprocess SSH (computes anomalies, normalizes, detrends, smoothes, averages,...)
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        ssh_mod, _, keyerror_mod = PreProcessTS(
                            ssh_mod, "", areacell=ssh_mod_areacell, compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        ssh_obs, _, keyerror_obs = PreProcessTS(
                            ssh_obs, "", areacell=ssh_obs_areacell, compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        kwargs["smoothing"] = smooth
                        del ssh_mod_areacell, ssh_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                              "shape1": "(mod ssh) " + str(ssh_mod.shape),
                                              "shape2": "(obs ssh) " + str(ssh_obs.shape),
                                              "time1": "(mod ssh) " + str(TimeBounds(ssh_mod)),
                                              "time2": "(obs ssh) " + str(TimeBounds(ssh_obs))}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # Seasonal mean and anomalies
                            ssh_mod = SeasonalMean(ssh_mod, season_ev, compute_anom=True)
                            ssh_obs = SeasonalMean(ssh_obs, season_ev, compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                              "shape1": "(mod ssh) " + str(ssh_mod.shape),
                                              "shape2": "(obs ssh) " + str(ssh_obs.shape),
                                              "time1": "(mod ssh) " + str(TimeBounds(ssh_mod)),
                                              "time2": "(obs ssh) " + str(TimeBounds(ssh_obs))}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                ssh_mod, ssh_obs, _ = TwoVarRegrid(
                                    ssh_mod, ssh_obs, "", region="equatorial_pacific_LatExt2",
                                    **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                                  "axes2": "(obs ssh) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                                  "shape1": "(mod ssh) " + str(ssh_mod.shape),
                                                  "shape2": "(obs ssh) " + str(ssh_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                            # change units
                            ssh_mod = ssh_mod * 1e2
                            ssh_obs = ssh_obs * 1e2
                            # Linear regression
                            ssh_map_slope_mod = LinearRegressionTsAgainstMap(ssh_mod, enso_mod, return_stderr=False)
                            ssh_map_slope_obs = LinearRegressionTsAgainstMap(ssh_obs, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod ssh) " + str([ax.id for ax in ssh_map_slope_mod.getAxisList()]),
                                    "axes2": "(obs ssh) " + str([ax.id for ax in ssh_map_slope_obs.getAxisList()]),
                                    "shape1": "(mod ssh) " + str(ssh_map_slope_mod.shape),
                                    "shape2": "(obs ssh) " + str(ssh_map_slope_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # Lists event years
                            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # composites
                            if len(ln_years_mod) > 0:
                                ln_ssh_lon_mod = Composite(ssh_lon_mod, ln_years_mod, kwargs["frequency"])
                                ln_ssh_map_mod = Composite(ssh_mod, ln_years_mod, kwargs["frequency"])
                            else:
                                ln_ssh_lon_mod = MyEmpty(ssh_lon_mod[0], time=False)
                                ln_ssh_map_mod = MyEmpty(ssh_mod[0], time=False)
                            if len(en_years_mod) > 0:
                                en_ssh_lon_mod = Composite(ssh_lon_mod, en_years_mod, kwargs["frequency"])
                                en_ssh_map_mod = Composite(ssh_mod, en_years_mod, kwargs["frequency"])
                            else:
                                en_ssh_lon_mod = MyEmpty(ssh_lon_mod[0], time=False)
                                en_ssh_map_mod = MyEmpty(ssh_mod[0], time=False)
                            if len(ln_years_obs) > 0:
                                ln_ssh_lon_obs = Composite(ssh_lon_obs, ln_years_obs, kwargs["frequency"])
                                ln_ssh_map_obs = Composite(ssh_obs, ln_years_obs, kwargs["frequency"])
                            else:
                                ln_ssh_lon_obs = MyEmpty(ssh_lon_obs[0], time=False)
                                ln_ssh_map_obs = MyEmpty(ssh_obs[0], time=False)
                            if len(en_years_obs) > 0:
                                en_ssh_lon_obs = Composite(ssh_lon_obs, en_years_obs, kwargs["frequency"])
                                en_ssh_map_obs = Composite(ssh_obs, en_years_obs, kwargs["frequency"])
                            else:
                                en_ssh_lon_obs = MyEmpty(ssh_lon_obs[0], time=False)
                                en_ssh_map_obs = MyEmpty(ssh_obs[0], time=False)
                            # Supplementary metrics
                            my_units = "" if kwargs["normalization"] is True else "cm"
                            dict_metric, dict_nc = dict(), dict()
                            nbr = 3
                            my_ev = ["nina", "nino", None, "nina", "nino"]
                            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
                            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
                            my_de = [
                                "zonal curve of " + sshbox + " SSHA (meridional averaged [" + str(lat[0]) + " ; " +
                                str(lat[1]) + "]) of La Nina events composite during " + season_ev + "; Nina = " +
                                region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "zonal curve of " + sshbox + " SSHA (meridional averaged [" + str(lat[0]) + " ; " +
                                str(lat[1]) + "]) of El Nino events composite during " + season_ev + "; Nino = " +
                                region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                                "map of tropical Pacific SSHA regressed onto " + region_ev + " averaged SSTA during " +
                                season_ev + enso_method3,
                                "map of tropical Pacific SSHA of La Nina events composite during " + season_ev + "; " +
                                "Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "map of tropical Pacific SSHA of El Nino events composite during " + season_ev +
                                "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                            my_un = [my_units, my_units, Units, my_units, my_units]
                            my_mo = [ln_ssh_lon_mod, en_ssh_lon_mod, ssh_map_slope_mod, ln_ssh_map_mod,
                                     en_ssh_map_mod]
                            my_ob = [ln_ssh_lon_obs, en_ssh_lon_obs, ssh_map_slope_obs, ln_ssh_map_obs,
                                     en_ssh_map_obs]
                            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
                            my_ax = ["0", "0", "xy", "xy", "xy"]
                            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                                dict_metric, dict_nc = fill_dict_axis(
                                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn,
                                    events1=ev1, events2=ev2, description=des)
                                nbr += 2
                            # save
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "arraySTD_" + dataset1: sm_std_mod,
                                     "description": Method.split(", ")[0]}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "arraySTD_" + dataset2: sm_std_obs,
                                     "description": Method.split(", ")[0]}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=ssh_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=ssh_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, \
                                my_eo, my_mo, my_ob, my_ty, my_un, my_units, nbr
    # metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "ref": Ref,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "time_frequency": kwargs["frequency"], "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSstLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                   box, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False,
                   netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstLonRmse() function computes sea surface temperature anomalies pattern associated with ENSO in a 'box'
    (usually the equatorial_pacific)
    It is the regression of meridionally averaged 'box' SSTA (sea surface temperature anomalies) onto 'region_ev'
    averaged SSTA during boreal winter (usually the regression of equatorial_pacific SSTA onto nino3.4 SSTA).

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param box: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(box)["latitude"]
    Name = "ENSO Zonal SSTA pattern"
    Units = "" if kwargs["normalization"] else "C/C"
    Method = "zonal curve of " + box + " SSTA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) + \
             "]) regressed onto " + region_ev + " averaged SSTA during " + season_ev + enso_method3
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoSstLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    sst_map_mod, sst_map_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_map_obs, sst_map_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, sst_map_mod, keyerror_mod3 = CheckTime(sst_mod, sst_map_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, sst_map_obs, keyerror_obs3 = CheckTime(sst_obs, sst_map_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2, keyerror_mod3, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        # 1.2 SST in 'box' are normalized / detrended / smoothed (running average) if applicable
        sst_map_mod, Method, keyerror_mod2 = PreProcessTS(
            sst_map_mod, Method, areacell=sst_map_mod_areacell, compute_anom=False, region=box, **kwargs)
        sst_map_obs, _, keyerror_obs2 = PreProcessTS(
            sst_map_obs, "", areacell=sst_map_obs_areacell, compute_anom=False, region=box, **kwargs)
        kwargs["smoothing"] = smooth
        del mod_areacell, obs_areacell, sst_map_mod_areacell, sst_map_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(mod ts) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                    "axes2": "(obs ts) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                    "axes3": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                    "axes4": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                    "shape1": "(mod ts) " + str(sst_box_mod.shape), "shape2": "(obs ts) " + str(sst_box_obs.shape),
                    "shape3": "(mod map) " + str(sst_map_mod.shape), "shape4": "(obs map) " + str(sst_map_obs.shape),
                    "time1": "(mod ts) " + str(TimeBounds(sst_box_mod)),
                    "time2": "(obs ts) " + str(TimeBounds(sst_box_obs)),
                    "time3": "(mod map) " + str(TimeBounds(sst_map_mod)),
                    "time4": "(obs map) " + str(TimeBounds(sst_map_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            # 2.1 horizontally averaged SST
            enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
            # 2.2 map of SST
            sst_map_mod = SeasonalMean(sst_map_mod, season_ev, compute_anom=True)
            sst_map_obs = SeasonalMean(sst_map_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod ts) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(obs ts) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                    "axes4": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                    "shape1": "(mod ts) " + str(enso_mod.shape), "shape2": "(obs ts) " + str(enso_obs.shape),
                    "shape3": "(mod map) " + str(sst_map_mod.shape), "shape4": "(obs map) " + str(sst_map_obs.shape),
                    "time1": "(mod ts) " + str(TimeBounds(enso_mod)), "time2": "(obs ts) " + str(TimeBounds(enso_obs)),
                    "time3": "(mod map) " + str(TimeBounds(sst_map_mod)),
                    "time4": "(obs map) " + str(TimeBounds(sst_map_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sst_map_mod, sst_map_obs, Method = TwoVarRegrid(
                    sst_map_mod, sst_map_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                        "axes2": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                        "shape1": "(mod map) " + str(sst_map_mod.shape),
                        "shape2": "(obs map) " + str(sst_map_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            sst_lon_mod, keyerror_mod = AverageMeridional(sst_map_mod)
            sst_lon_obs, keyerror_obs = AverageMeridional(sst_map_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {
                        "axes1": "(mod lon) " + str([ax.id for ax in sst_lon_mod.getAxisList()]),
                        "axes2": "(obs lon) " + str([ax.id for ax in sst_lon_obs.getAxisList()]),
                        "shape1": "(mod lon) " + str(sst_lon_mod.shape),
                        "shape2": "(obs lon) " + str(sst_lon_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # regression
                sst_slope_mod = LinearRegressionTsAgainstMap(sst_lon_mod, enso_mod, return_stderr=False)
                sst_slope_obs = LinearRegressionTsAgainstMap(sst_lon_obs, enso_obs, return_stderr=False)
                if debug is True:
                    dict_debug = {"axes1": "(mod lon) " + str([ax.id for ax in sst_slope_mod.getAxisList()]),
                                  "axes2": "(obs lon) " + str([ax.id for ax in sst_slope_obs.getAxisList()]),
                                  "shape1": "(mod lon) " + str(sst_slope_mod.shape),
                                  "shape2": "(obs lon) " + str(sst_slope_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(sst_slope_mod, sst_slope_obs, centered=centered_rmse, biased=biased_rmse)
                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(sst_slope_mod, sst_slope_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(sst_slope_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(sst_slope_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(sst_slope_mod), "observations": ArrayToList(sst_slope_obs),
                                  "axis": list(sst_slope_mod.getAxis(0)[:])}

                if netcdf is True:
                    # Read file and select the right region
                    sst_map_mod, sst_map_mod_areacell, keyerror_mod = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific_LatExt2",
                        file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                        name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    sst_map_obs, sst_map_obs_areacell, keyerror_obs = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific_LatExt2",
                        file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                        name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess SST (computes anomalies, normalizes, detrends, smoothes, averages,...)
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        sst_map_mod, _, keyerror_mod = PreProcessTS(
                            sst_map_mod, "", areacell=sst_map_mod_areacell, compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        sst_map_obs, _, keyerror_obs = PreProcessTS(
                            sst_map_obs, "", areacell=sst_map_obs_areacell, compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        kwargs["smoothing"] = smooth
                        del sst_map_mod_areacell, sst_map_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                                              "axes2": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                                              "shape1": "(mod map) " + str(sst_map_mod.shape),
                                              "shape2": "(obs map) " + str(sst_map_obs.shape),
                                              "time1": "(mod map) " + str(TimeBounds(sst_map_mod)),
                                              "time2": "(obs map) " + str(TimeBounds(sst_map_obs))}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # Seasonal mean and anomalies
                            sst_map_mod = SeasonalMean(sst_map_mod, season_ev, compute_anom=True)
                            sst_map_obs = SeasonalMean(sst_map_obs, season_ev, compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                                              "axes2": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                                              "shape1": "(mod map) " + str(sst_map_mod.shape),
                                              "shape2": "(obs map) " + str(sst_map_obs.shape),
                                              "time1": "(mod map) " + str(TimeBounds(sst_map_mod)),
                                              "time2": "(obs map) " + str(TimeBounds(sst_map_obs))}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                sst_map_mod, sst_map_obs, _ = TwoVarRegrid(
                                    sst_map_mod, sst_map_obs, "", region="equatorial_pacific_LatExt2",
                                    **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(mod map) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                                        "axes2": "(obs map) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                                        "shape1": "(mod map) " + str(sst_map_mod.shape),
                                        "shape2": "(obs map) " + str(sst_map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                            # Linear regression
                            sst_map_slope_mod = LinearRegressionTsAgainstMap(sst_map_mod, enso_mod, return_stderr=False)
                            sst_map_slope_obs = LinearRegressionTsAgainstMap(sst_map_obs, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod map) " + str([ax.id for ax in sst_map_slope_mod.getAxisList()]),
                                    "axes2": "(obs map) " + str([ax.id for ax in sst_map_slope_obs.getAxisList()]),
                                    "shape1": "(mod map) " + str(sst_map_slope_mod.shape),
                                    "shape2": "(obs map) " + str(sst_map_slope_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # Lists event years
                            nina_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize,
                                                          nino=False, compute_season=True, duration=length_ev)
                            nino_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize,
                                                          nino=True, compute_season=True, duration=length_ev)
                            nina_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize,
                                                          nino=False, compute_season=True, duration=length_ev)
                            nino_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize,
                                                          nino=True, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nina1": "(mod) nbr(" + str(len(nina_years_mod)) + "): " + str(nina_years_mod),
                                    "nina2": "(obs) nbr(" + str(len(nina_years_obs)) + "): " + str(nina_years_obs),
                                    "nino1": "(mod) nbr(" + str(len(nino_years_mod)) + "): " + str(nino_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(nino_years_obs)) + "): " + str(nino_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # composites
                            if len(nina_years_mod) > 0:
                                nina_sst_lon_mod = Composite(sst_lon_mod, nina_years_mod, kwargs["frequency"])
                                nina_sst_map_mod = Composite(sst_map_mod, nina_years_mod, kwargs["frequency"])
                            else:
                                nina_sst_lon_mod = MyEmpty(sst_lon_mod[0], time=False)
                                nina_sst_map_mod = MyEmpty(sst_map_mod[0], time=False)
                            if len(nino_years_mod) > 0:
                                nino_sst_lon_mod = Composite(sst_lon_mod, nino_years_mod, kwargs["frequency"])
                                nino_sst_map_mod = Composite(sst_map_mod, nino_years_mod, kwargs["frequency"])
                            else:
                                nino_sst_lon_mod = MyEmpty(sst_lon_mod[0], time=False)
                                nino_sst_map_mod = MyEmpty(sst_map_mod[0], time=False)
                            if len(nina_years_obs) > 0:
                                nina_sst_lon_obs = Composite(sst_lon_obs, nina_years_obs, kwargs["frequency"])
                                nina_sst_map_obs = Composite(sst_map_obs, nina_years_obs, kwargs["frequency"])
                            else:
                                nina_sst_lon_obs = MyEmpty(sst_lon_obs[0], time=False)
                                nina_sst_map_obs = MyEmpty(sst_map_obs[0], time=False)
                            if len(nino_years_obs) > 0:
                                nino_sst_lon_obs = Composite(sst_lon_obs, nino_years_obs, kwargs["frequency"])
                                nino_sst_map_obs = Composite(sst_map_obs, nino_years_obs, kwargs["frequency"])
                            else:
                                nino_sst_lon_obs = MyEmpty(sst_lon_obs[0], time=False)
                                nino_sst_map_obs = MyEmpty(sst_map_obs[0], time=False)
                            # Supplementary metrics
                            my_units = "" if kwargs["normalization"] is True else "C"
                            dict_metric, dict_nc = dict(), dict()
                            nbr = 3
                            my_ev = ["nina", "nino", None, "nina", "nino"]
                            my_em = [nina_years_mod, nino_years_mod, None, nina_years_mod, nino_years_mod]
                            my_eo = [nina_years_obs, nino_years_obs, None, nina_years_obs, nino_years_obs]
                            my_de = [
                                "zonal curve of " + box + " SSTA (meridional averaged [" + str(lat[0]) + " ; " +
                                str(lat[1]) + "]) of La Nina events composite during " + season_ev + "; Nina = " +
                                region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "zonal curve of " + box + " SSTA (meridional averaged [" + str(lat[0]) + " ; " +
                                str(lat[1]) + "]) of El Nino events composite during " + season_ev + "; Nino = " +
                                region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                                "map of tropical Pacific SSTA regressed onto " + region_ev + " averaged SSTA during " +
                                season_ev + enso_method3,
                                "map of tropical Pacific SSTA of La Nina events composite during " + season_ev + "; " +
                                "Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "map of tropical Pacific SSTA of El Nino events composite during " + season_ev +
                                "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                            my_un = [my_units, my_units, Units, my_units, my_units]
                            my_mo = [nina_sst_lon_mod, nino_sst_lon_mod, sst_map_slope_mod, nina_sst_map_mod,
                                     nino_sst_map_mod]
                            my_ob = [nina_sst_lon_obs, nino_sst_lon_obs, sst_map_slope_obs, nina_sst_map_obs,
                                     nino_sst_map_obs]
                            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
                            my_ax = ["0", "0", "xy", "xy", "xy"]
                            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                                dict_metric, dict_nc = fill_dict_axis(
                                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn,
                                    events1=ev1, events2=ev2, description=des)
                                nbr += 2
                            # save
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "arraySTD_" + dataset1: sm_std_mod,
                                     "description": Method.split(", ")[0]}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "arraySTD_" + dataset2: sm_std_obs,
                                     "description": Method.split(", ")[0]}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=sst_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=sst_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, \
                                my_eo, my_mo, my_ob, my_ty, my_un, my_units, nbr
    # metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoTauxLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                    tauxfilemod, tauxnamemod, tauxareafilemod, tauxareanamemod, tauxlandmaskfilemod,
                    tauxlandmasknamemod, sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs,
                    sstlandmasknameobs, tauxfileobs, tauxnameobs, tauxareafileobs, tauxareanameobs, tauxlandmaskfileobs,
                    tauxlandmasknameobs, sstbox, tauxbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="",
                    dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoTauxLonRmse() function computes zonal wind stress anomalies pattern associated with ENSO in a 'tauxbox'
    (usually the equatorial_pacific).
    It is the regression of 'tauxbox' averaged TAUXA (zonal wind stress anomalies) time series onto 'region_ev' averaged
    SSTA (sea surface temperature anomalies) (usually the regression of equatorial_pacific PRA onto nino3.4 SSTA).

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param tauxfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUX
    :param tauxnamemod: string
        name of TAUX variable (tauu, tauuo) in 'tauxfilemod'
    :param tauxareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled TAUX areacell
    :param tauxareanamemod: string, optional
        name of areacell for the TAUX variable (areacella, areacello,...) in 'tauxareafilemod'
    :param tauxlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled TAUX landmask
    :param tauxlandmasknamemod: string, optional
        name of landmask for the TAUX variable (sftlf,...) in 'tauxlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param tauxfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUX
    :param tauxnameobs: string
        name of TAUX variable (tauu, taux) in 'tauxfileobs'
    :param tauxareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed TAUX areacell
    :param tauxareanameobs: string, optional
        name of areacell for the TAUX variable (areacella, areacello,...) in 'tauxareafileobs'
    :param tauxlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed TAUX landmask
    :param tauxlandmasknameobs: string, optional
        name of landmask for the TAUX variable (sftlf,...) in 'tauxlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param tauxbox: string
        name of box (e.g. 'equatorial_pacific') for TAUX
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoTauxLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(tauxbox)["latitude"]
    Name = "ENSO zonal TAUXA pattern"
    Units = "" if kwargs["normalization"] else "1e-3 N/m2/C"
    Method = "zonal curve of " + tauxbox + " zonal wind stress anomalies (meridional averaged [" + str(lat[0]) + " ;" + \
             " " + str(lat[1]) + "]) regressed onto " + region_ev + " averaged SSTA during " + season_ev + enso_method3
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoTauxLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    taux_mod, taux_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        tauxfilemod, tauxnamemod, "wind stress", metric, tauxbox, file_area=tauxareafilemod, name_area=tauxareanamemod,
        file_mask=tauxlandmaskfilemod, name_mask=tauxlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    taux_obs, taux_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        tauxfileobs, tauxnameobs, "wind stress", metric, tauxbox, file_area=tauxareafileobs, name_area=tauxareanameobs,
        file_mask=tauxlandmaskfileobs, name_mask=tauxlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, taux_mod, keyerror_mod3 = CheckTime(sst_mod, taux_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, taux_obs, keyerror_obs3 = CheckTime(sst_obs, taux_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2, keyerror_mod3, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(sst_mod, "", areacell=sst_mod_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(sst_obs, "", areacell=sst_obs_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        # 1.2 TAUX in 'tauxbox' are normalized / detrended / smoothed (running average) if applicable
        taux_mod, Method, keyerror_mod2 = PreProcessTS(
            taux_mod, Method, areacell=taux_mod_areacell, compute_anom=False, region=tauxbox, **kwargs)
        taux_obs, _, keyerror_obs2 = PreProcessTS(
            taux_obs, "", areacell=taux_obs_areacell, compute_anom=False, region=tauxbox, **kwargs)
        kwargs["smoothing"] = smooth
        del sst_mod_areacell, sst_obs_areacell, taux_mod_areacell, taux_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                    "axes3": "(mod taux) " + str([ax.id for ax in taux_mod.getAxisList()]),
                    "axes4": "(obs taux) " + str([ax.id for ax in taux_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                    "shape3": "(mod taux) " + str(taux_mod.shape), "shape4": "(obs taux) " + str(taux_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                    "time3": "(mod taux) " + str(TimeBounds(taux_mod)),
                    "time4": "(obs taux) " + str(TimeBounds(taux_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            # 2.1 horizontally averaged SST
            enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
            # 2.2 map of SST
            taux_mod = SeasonalMean(taux_mod, season_ev, compute_anom=True)
            taux_obs = SeasonalMean(taux_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(mod taux) " + str([ax.id for ax in taux_mod.getAxisList()]),
                    "axes4": "(obs taux) " + str([ax.id for ax in taux_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                    "shape3": "(mod taux) " + str(taux_mod.shape), "shape4": "(obs taux) " + str(taux_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(enso_obs)),
                    "time3": "(mod taux) " + str(TimeBounds(taux_mod)),
                    "time4": "(obs taux) " + str(TimeBounds(taux_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                taux_mod, taux_obs, Method = TwoVarRegrid(
                    taux_mod, taux_obs, Method, region=tauxbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(mod taux) " + str([ax.id for ax in taux_mod.getAxisList()]),
                        "axes2": "(obs taux) " + str([ax.id for ax in taux_obs.getAxisList()]),
                        "shape1": "(mod taux) " + str(taux_mod.shape), "shape2": "(obs taux) " + str(taux_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # change units
            taux_mod = taux_mod * 1e3
            taux_obs = taux_obs * 1e3

            # Meridional average
            taux_lon_mod, keyerror_mod = AverageMeridional(taux_mod)
            taux_lon_obs, keyerror_obs = AverageMeridional(taux_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {
                        "axes1": "(mod taux) " + str([ax.id for ax in taux_mod.getAxisList()]),
                        "axes2": "(obs taux) " + str([ax.id for ax in taux_obs.getAxisList()]),
                        "shape1": "(mod taux) " + str(taux_mod.shape), "shape2": "(obs taux) " + str(taux_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

                # regression
                taux_slope_mod = LinearRegressionTsAgainstMap(taux_lon_mod, enso_mod, return_stderr=False)
                taux_slope_obs = LinearRegressionTsAgainstMap(taux_lon_obs, enso_obs, return_stderr=False)
                if debug is True:
                    dict_debug = {"axes1": "(mod taux) " + str([ax.id for ax in taux_slope_mod.getAxisList()]),
                                  "axes2": "(obs taux) " + str([ax.id for ax in taux_slope_obs.getAxisList()]),
                                  "shape1": "(mod taux) " + str(taux_slope_mod.shape),
                                  "shape2": "(obs taux) " + str(taux_slope_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(taux_slope_mod, taux_slope_obs, centered=centered_rmse, biased=biased_rmse)
                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(taux_slope_mod, taux_slope_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(taux_slope_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(taux_slope_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(taux_slope_mod), "observations": ArrayToList(taux_slope_obs),
                                  "axis": list(taux_slope_mod.getAxis(0)[:])}

                if netcdf is True:
                    # Read file and select the right region
                    taux_mod, taux_mod_areacell, keyerror_mod = Read_data_mask_area(
                        tauxfilemod, tauxnamemod, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauxareafilemod, name_area=tauxareanamemod, file_mask=tauxlandmaskfilemod,
                        name_mask=tauxlandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    taux_obs, taux_obs_areacell, keyerror_obs = Read_data_mask_area(
                        tauxfileobs, tauxnameobs, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauxareafileobs, name_area=tauxareanameobs, file_mask=tauxlandmaskfileobs,
                        name_mask=tauxlandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    # Checks if the same time period is used for both variables
                    sst_mod, taux_mod, keyerror_mod2 = CheckTime(
                        sst_mod, taux_mod, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, taux_obs, keyerror_obs2 = CheckTime(
                        sst_obs, taux_obs, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # Preprocess TAUX (computes anomalies, normalizes, detrends, smoothes, averages,...)
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        taux_mod, _, keyerror_mod = PreProcessTS(
                            taux_mod, "", areacell=taux_mod_areacell, compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        taux_obs, _, keyerror_obs = PreProcessTS(
                            taux_obs, "", areacell=taux_obs_areacell, compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        kwargs["smoothing"] = smooth
                        del taux_mod_areacell, taux_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod taux) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                              "axes2": "(obs taux) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                              "shape1": "(mod taux) " + str(taux_mod.shape),
                                              "shape2": "(obs taux) " + str(taux_obs.shape),
                                              "time1": "(mod taux) " + str(TimeBounds(taux_mod)),
                                              "time2": "(obs taux) " + str(TimeBounds(taux_obs))}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # Seasonal mean and anomalies
                            taux_mod = SeasonalMean(taux_mod, season_ev, compute_anom=True)
                            taux_obs = SeasonalMean(taux_obs, season_ev, compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(mod taux) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                              "axes2": "(obs taux) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                              "shape1": "(mod taux) " + str(taux_mod.shape),
                                              "shape2": "(obs taux) " + str(taux_obs.shape),
                                              "time1": "(mod taux) " + str(TimeBounds(taux_mod)),
                                              "time2": "(obs taux) " + str(TimeBounds(taux_obs))}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                taux_mod, taux_obs, _ = TwoVarRegrid(
                                    taux_mod, taux_obs, "", region="equatorial_pacific_LatExt2",
                                    **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(mod taux) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                        "axes2": "(obs taux) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                        "shape1": "(mod taux) " + str(taux_mod.shape),
                                        "shape2": "(obs taux) " + str(taux_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                            # change units
                            taux_mod = taux_mod * 1e3
                            taux_obs = taux_obs * 1e3
                            # Linear regression
                            taux_map_slope_mod = LinearRegressionTsAgainstMap(taux_mod, enso_mod, return_stderr=False)
                            taux_map_slope_obs = LinearRegressionTsAgainstMap(taux_obs, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod taux) " + str([ax.id for ax in taux_map_slope_mod.getAxisList()]),
                                    "axes2": "(obs taux) " + str([ax.id for ax in taux_map_slope_obs.getAxisList()]),
                                    "shape1": "(mod taux) " + str(taux_map_slope_mod.shape),
                                    "shape2": "(obs taux) " + str(taux_map_slope_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # Lists event years
                            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # composites
                            if len(ln_years_mod) > 0:
                                ln_taux_lon_mod = Composite(taux_lon_mod, ln_years_mod, kwargs["frequency"])
                                ln_taux_map_mod = Composite(taux_mod, ln_years_mod, kwargs["frequency"])
                            else:
                                ln_taux_lon_mod = MyEmpty(taux_lon_mod[0], time=False)
                                ln_taux_map_mod = MyEmpty(taux_mod[0], time=False)
                            if len(en_years_mod) > 0:
                                en_taux_lon_mod = Composite(taux_lon_mod, en_years_mod, kwargs["frequency"])
                                en_taux_map_mod = Composite(taux_mod, en_years_mod, kwargs["frequency"])
                            else:
                                en_taux_lon_mod = MyEmpty(taux_lon_mod[0], time=False)
                                en_taux_map_mod = MyEmpty(taux_mod[0], time=False)
                            if len(ln_years_obs) > 0:
                                ln_taux_lon_obs = Composite(taux_lon_obs, ln_years_obs, kwargs["frequency"])
                                ln_taux_map_obs = Composite(taux_obs, ln_years_obs, kwargs["frequency"])
                            else:
                                ln_taux_lon_obs = MyEmpty(taux_lon_obs[0], time=False)
                                ln_taux_map_obs = MyEmpty(taux_obs[0], time=False)
                            if len(en_years_obs) > 0:
                                en_taux_lon_obs = Composite(taux_lon_obs, en_years_obs, kwargs["frequency"])
                                en_taux_map_obs = Composite(taux_obs, en_years_obs, kwargs["frequency"])
                            else:
                                en_taux_lon_obs = MyEmpty(taux_lon_obs[0], time=False)
                                en_taux_map_obs = MyEmpty(taux_obs[0], time=False)
                            # Supplementary metrics
                            my_units = "" if kwargs["normalization"] is True else "1e-3 N/m2"
                            dict_metric, dict_nc = dict(), dict()
                            nbr = 3
                            my_ev = ["nina", "nino", None, "nina", "nino"]
                            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
                            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
                            my_de = [
                                "zonal curve of " + tauxbox + " TAUXA (meridional averaged [" + str(lat[0]) + " ; " +
                                str(lat[1]) + "]) of La Nina events composite during " + season_ev + "; Nina = " +
                                region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "zonal curve of " + tauxbox + " TAUXA (meridional averaged [" + str(lat[0]) + " ; " +
                                str(lat[1]) + "]) of El Nino events composite during " + season_ev + "; Nino = " +
                                region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                                "map of tropical Pacific TAUXA regressed onto " + region_ev + " averaged SSTA during " +
                                season_ev + enso_method3,
                                "map of tropical Pacific TAUXA of La Nina events composite during " + season_ev +
                                "; Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev +
                                enso_method,
                                "map of tropical Pacific TAUXA of El Nino events composite during " + season_ev +
                                "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                            my_un = [my_units, my_units, Units, my_units, my_units]
                            my_mo = [ln_taux_lon_mod, en_taux_lon_mod, taux_map_slope_mod, ln_taux_map_mod,
                                     en_taux_map_mod]
                            my_ob = [ln_taux_lon_obs, en_taux_lon_obs, taux_map_slope_obs, ln_taux_map_obs,
                                     en_taux_map_obs]
                            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
                            my_ax = ["0", "0", "xy", "xy", "xy"]
                            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                                dict_metric, dict_nc = fill_dict_axis(
                                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn,
                                    events1=ev1, events2=ev2, description=des)
                                nbr += 2
                            # save
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "arraySTD_" + dataset1: sm_std_mod,
                                     "description": Method.split(", ")[0]}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "arraySTD_" + dataset2: sm_std_obs,
                                     "description": Method.split(", ")[0]}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=taux_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=taux_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, \
                                my_eo, my_mo, my_ob, my_ty, my_un, my_units, nbr
    # metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "ref": Ref,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "time_frequency": kwargs["frequency"], "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoTauyLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                    tauyfilemod, tauynamemod, tauyareafilemod, tauyareanamemod, tauylandmaskfilemod,
                    tauylandmasknamemod, sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs,
                    sstlandmasknameobs, tauyfileobs, tauynameobs, tauyareafileobs, tauyareanameobs, tauylandmaskfileobs,
                    tauylandmasknameobs, sstbox, tauybox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="",
                    dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoTauyLonRmse() function computes meridional wind stress anomalies pattern associated with ENSO in a 'tauybox'
    (usually the equatorial_pacific).
    It is the regression of 'tauybox' averaged TAUYA (meridional wind stress anomalies) time series onto 'region_ev'
    averaged SSTA (sea surface temperature anomalies) (usually the regression of equatorial_pacific PRA onto nino3.4
    SSTA).

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param tauyfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUY
    :param tauynamemod: string
        name of TAUY variable (tauv, tauvo) in 'tauyfilemod'
    :param tauyareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled TAUY areacell
    :param tauyareanamemod: string, optional
        name of areacell for the TAUY variable (areacella, areacello,...) in 'tauyareafilemod'
    :param tauylandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled TAUY landmask
    :param tauylandmasknamemod: string, optional
        name of landmask for the TAUY variable (sftlf,...) in 'tauylandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param tauyfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUY
    :param tauynameobs: string
        name of TAUY variable (tauv, tauy) in 'tauyfileobs'
    :param tauyareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed TAUY areacell
    :param tauyareanameobs: string, optional
        name of areacell for the TAUY variable (areacella, areacello,...) in 'tauyareafileobs'
    :param tauylandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed TAUY landmask
    :param tauylandmasknameobs: string, optional
        name of landmask for the TAUY variable (sftlf,...) in 'tauylandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'equatorial_pacific') for SST
    :param tauybox: string
        name of box (e.g. 'equatorial_pacific') for TAUY
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoTauyLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    lat = ReferenceRegions(tauybox)["latitude"]
    Name = "ENSO zonal TAUYA pattern"
    Units = "" if kwargs["normalization"] else "1e-3 N/m2/C"
    Method = "zonal curve of " + tauybox + " meridional wind stress anomalies (meridional averaged [" + str(lat[0]) + \
             " ; " + str(lat[1]) + "]) regressed onto " + region_ev + " averaged SSTA during " + season_ev + \
             enso_method3
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoTauyLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    tauy_mod, tauy_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        tauyfilemod, tauynamemod, "wind stress", metric, tauybox, file_area=tauyareafilemod, name_area=tauyareanamemod,
        file_mask=tauylandmaskfilemod, name_mask=tauylandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tauy_obs, tauy_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        tauyfileobs, tauynameobs, "wind stress", metric, tauybox, file_area=tauyareafileobs, name_area=tauyareanameobs,
        file_mask=tauylandmaskfileobs, name_mask=tauylandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tauy_mod, keyerror_mod3 = CheckTime(sst_mod, tauy_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tauy_obs, keyerror_obs3 = CheckTime(sst_obs, tauy_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2, keyerror_mod3, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(sst_mod, "", areacell=sst_mod_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(sst_obs, "", areacell=sst_obs_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        # 1.2 TAUY in 'tauybox' are normalized / detrended / smoothed (running average) if applicable
        tauy_mod, Method, keyerror_mod2 = PreProcessTS(
            tauy_mod, Method, areacell=tauy_mod_areacell, compute_anom=False, region=tauybox, **kwargs)
        tauy_obs, _, keyerror_obs2 = PreProcessTS(
            tauy_obs, "", areacell=tauy_obs_areacell, compute_anom=False, region=tauybox, **kwargs)
        kwargs["smoothing"] = smooth
        del sst_mod_areacell, sst_obs_areacell, tauy_mod_areacell, tauy_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_obs1, keyerror_mod2, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                    "axes3": "(mod tauy) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                    "axes4": "(obs tauy) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                    "shape3": "(mod tauy) " + str(tauy_mod.shape), "shape4": "(obs tauy) " + str(tauy_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                    "time3": "(mod tauy) " + str(TimeBounds(tauy_mod)),
                    "time4": "(obs tauy) " + str(TimeBounds(tauy_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            # 2.1 horizontally averaged SST
            enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
            # 2.2 map of SST
            tauy_mod = SeasonalMean(tauy_mod, season_ev, compute_anom=True)
            tauy_obs = SeasonalMean(tauy_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(mod tauy) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                    "axes4": "(obs tauy) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                    "shape3": "(mod tauy) " + str(tauy_mod.shape), "shape4": "(obs tauy) " + str(tauy_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(enso_obs)),
                    "time3": "(mod tauy) " + str(TimeBounds(tauy_mod)),
                    "time4": "(obs tauy) " + str(TimeBounds(tauy_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tauy_mod, tauy_obs, Method = TwoVarRegrid(
                    tauy_mod, tauy_obs, Method, region=tauybox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(mod tauy) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                        "axes2": "(obs tauy) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                        "shape1": "(mod tauy) " + str(tauy_mod.shape), "shape2": "(obs tauy) " + str(tauy_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # change units
            tauy_mod = tauy_mod * 1e3
            tauy_obs = tauy_obs * 1e3

            # Meridional average
            tauy_lon_mod, keyerror_mod = AverageMeridional(tauy_mod)
            tauy_lon_obs, keyerror_obs = AverageMeridional(tauy_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {
                        "axes1": "(mod tauy) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                        "axes2": "(obs tauy) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                        "shape1": "(mod tauy) " + str(tauy_mod.shape), "shape2": "(obs tauy) " + str(tauy_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

                # regression
                tauy_slope_mod = LinearRegressionTsAgainstMap(tauy_lon_mod, enso_mod, return_stderr=False)
                tauy_slope_obs = LinearRegressionTsAgainstMap(tauy_lon_obs, enso_obs, return_stderr=False)
                if debug is True:
                    dict_debug = {"axes1": "(mod tauy) " + str([ax.id for ax in tauy_slope_mod.getAxisList()]),
                                  "axes2": "(obs tauy) " + str([ax.id for ax in tauy_slope_obs.getAxisList()]),
                                  "shape1": "(mod tauy) " + str(tauy_slope_mod.shape),
                                  "shape2": "(obs tauy) " + str(tauy_slope_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(tauy_slope_mod, tauy_slope_obs, centered=centered_rmse, biased=biased_rmse)
                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(tauy_slope_mod, tauy_slope_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(tauy_slope_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(tauy_slope_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(tauy_slope_mod), "observations": ArrayToList(tauy_slope_obs),
                                  "axis": list(tauy_slope_mod.getAxis(0)[:])}

                if netcdf is True:
                    # Read file and select the right region
                    tauy_mod, tauy_mod_areacell, keyerror_mod = Read_data_mask_area(
                        tauyfilemod, tauynamemod, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauyareafilemod, name_area=tauyareanamemod, file_mask=tauylandmaskfilemod,
                        name_mask=tauylandmasknamemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    tauy_obs, tauy_obs_areacell, keyerror_obs = Read_data_mask_area(
                        tauyfileobs, tauynameobs, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauyareafileobs, name_area=tauyareanameobs, file_mask=tauylandmaskfileobs,
                        name_mask=tauylandmasknameobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    # Checks if the same time period is used for both variables
                    sst_mod, tauy_mod, keyerror_mod2 = CheckTime(
                        sst_mod, tauy_mod, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, tauy_obs, keyerror_obs2 = CheckTime(
                        sst_obs, tauy_obs, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # Preprocess TAUY (computes anomalies, normalizes, detrends, smoothes, averages,...)
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        tauy_mod, _, keyerror_mod = PreProcessTS(
                            tauy_mod, "", areacell=tauy_mod_areacell, compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        tauy_obs, _, keyerror_obs = PreProcessTS(
                            tauy_obs, "", areacell=tauy_obs_areacell, compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        kwargs["smoothing"] = smooth
                        del tauy_mod_areacell, tauy_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod tauy) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                              "axes2": "(obs tauy) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                              "shape1": "(mod tauy) " + str(tauy_mod.shape),
                                              "shape2": "(obs tauy) " + str(tauy_obs.shape),
                                              "time1": "(mod tauy) " + str(TimeBounds(tauy_mod)),
                                              "time2": "(obs tauy) " + str(TimeBounds(tauy_obs))}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # Seasonal mean and anomalies
                            tauy_mod = SeasonalMean(tauy_mod, season_ev, compute_anom=True)
                            tauy_obs = SeasonalMean(tauy_obs, season_ev, compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(mod tauy) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                              "axes2": "(obs tauy) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                              "shape1": "(mod tauy) " + str(tauy_mod.shape),
                                              "shape2": "(obs tauy) " + str(tauy_obs.shape),
                                              "time1": "(mod tauy) " + str(TimeBounds(tauy_mod)),
                                              "time2": "(obs tauy) " + str(TimeBounds(tauy_obs))}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                tauy_mod, tauy_obs, _ = TwoVarRegrid(
                                    tauy_mod, tauy_obs, "", region="equatorial_pacific_LatExt2",
                                    **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(mod tauy) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                        "axes2": "(obs tauy) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                        "shape1": "(mod tauy) " + str(tauy_mod.shape),
                                        "shape2": "(obs tauy) " + str(tauy_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                            # change units
                            tauy_mod = tauy_mod * 1e3
                            tauy_obs = tauy_obs * 1e3
                            # Linear regression
                            tauy_map_slope_mod = LinearRegressionTsAgainstMap(tauy_mod, enso_mod, return_stderr=False)
                            tauy_map_slope_obs = LinearRegressionTsAgainstMap(tauy_obs, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod tauy) " + str([ax.id for ax in tauy_map_slope_mod.getAxisList()]),
                                    "axes2": "(obs tauy) " + str([ax.id for ax in tauy_map_slope_obs.getAxisList()]),
                                    "shape1": "(mod tauy) " + str(tauy_map_slope_mod.shape),
                                    "shape2": "(obs tauy) " + str(tauy_map_slope_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # Lists event years
                            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # composites
                            if len(ln_years_mod) > 0:
                                ln_tauy_lon_mod = Composite(tauy_lon_mod, ln_years_mod, kwargs["frequency"])
                                ln_tauy_map_mod = Composite(tauy_mod, ln_years_mod, kwargs["frequency"])
                            else:
                                ln_tauy_lon_mod = MyEmpty(tauy_lon_mod[0], time=False)
                                ln_tauy_map_mod = MyEmpty(tauy_mod[0], time=False)
                            if len(en_years_mod) > 0:
                                en_tauy_lon_mod = Composite(tauy_lon_mod, en_years_mod, kwargs["frequency"])
                                en_tauy_map_mod = Composite(tauy_mod, en_years_mod, kwargs["frequency"])
                            else:
                                en_tauy_lon_mod = MyEmpty(tauy_lon_mod[0], time=False)
                                en_tauy_map_mod = MyEmpty(tauy_mod[0], time=False)
                            if len(ln_years_obs) > 0:
                                ln_tauy_lon_obs = Composite(tauy_lon_obs, ln_years_obs, kwargs["frequency"])
                                ln_tauy_map_obs = Composite(tauy_obs, ln_years_obs, kwargs["frequency"])
                            else:
                                ln_tauy_lon_obs = MyEmpty(tauy_lon_obs[0], time=False)
                                ln_tauy_map_obs = MyEmpty(tauy_obs[0], time=False)
                            if len(en_years_obs) > 0:
                                en_tauy_lon_obs = Composite(tauy_lon_obs, en_years_obs, kwargs["frequency"])
                                en_tauy_map_obs = Composite(tauy_obs, en_years_obs, kwargs["frequency"])
                            else:
                                en_tauy_lon_obs = MyEmpty(tauy_lon_obs[0], time=False)
                                en_tauy_map_obs = MyEmpty(tauy_obs[0], time=False)
                            # Supplementary metrics
                            my_units = "" if kwargs["normalization"] is True else "1e-3 N/m2"
                            dict_metric, dict_nc = dict(), dict()
                            nbr = 3
                            my_ev = ["nina", "nino", None, "nina", "nino"]
                            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
                            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
                            my_de = [
                                "zonal curve of " + tauybox + " TAUYA (meridional averaged [" + str(lat[0]) + " ; " +
                                str(lat[1]) + "]) of La Nina events composite during " + season_ev + "; Nina = " +
                                region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "zonal curve of " + tauybox + " TAUYA (meridional averaged [" + str(lat[0]) + " ; " +
                                str(lat[1]) + "]) of El Nino events composite during " + season_ev + "; Nino = " +
                                region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                                "map of tropical Pacific TAUYA regressed onto " + region_ev + " averaged SSTA during " +
                                season_ev + enso_method3,
                                "map of tropical Pacific TAUYA of La Nina events composite during " + season_ev +
                                "; Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev +
                                enso_method,
                                "map of tropical Pacific TAUYA of El Nino events composite during " + season_ev +
                                "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                            my_un = [my_units, my_units, Units, my_units, my_units]
                            my_mo = [ln_tauy_lon_mod, en_tauy_lon_mod, tauy_map_slope_mod, ln_tauy_map_mod,
                                     en_tauy_map_mod]
                            my_ob = [ln_tauy_lon_obs, en_tauy_lon_obs, tauy_map_slope_obs, ln_tauy_map_obs,
                                     en_tauy_map_obs]
                            my_ty = ["lon_nina", "lon_nino", "map", "map_nina", "map_nino"]
                            my_ax = ["0", "0", "xy", "xy", "xy"]
                            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                                dict_metric, dict_nc = fill_dict_axis(
                                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn,
                                    events1=ev1, events2=ev2, description=des)
                                nbr += 2
                            # save
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "arraySTD_" + dataset1: sm_std_mod,
                                     "description": Method.split(", ")[0]}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "arraySTD_" + dataset2: sm_std_obs,
                                     "description": Method.split(", ")[0]}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "frequency": kwargs["frequency"],
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_lon": sm_corr, "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=tauy_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=tauy_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, \
                                my_eo, my_mo, my_ob, my_ty, my_un, my_units, nbr
    # metric value
    if debug is True:
        dict_debug = {"line1": "diagnostic value: " + str(mv), "line2": "diagnostic value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "ref": Ref,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "time_frequency": kwargs["frequency"], "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoPrTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                 prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                 sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                 prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox,
                 prbox, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                 debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrTsRmse() function computes precipitation anomalies life cycle associated with ENSO in a 'prbox'
    (usually the nino3) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and lagging
    ENSO).
    It is the regression of 'prbox' averaged PRA (precipitation anomalies) time series onto 'region_ev' averaged SSTA
    (sea surface temperature anomalies) (usually the regression of nino3 PRA onto nino3.4 SSTA).

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'nino3.4') for SST
    :param prbox: string
        name of box (e.g. 'nino3') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO life cyle PRA"
    Units = "" if kwargs["normalization"] else "mm/day/C"
    Method = "temporal curve of " + prbox + " averaged precipitation anomalies during " + str(nbr_years_window) + \
             " years (centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " + season_ev + \
             enso_method3
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoPrTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, pr_mod, keyerror_mod3 = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, pr_obs, keyerror_obs3 = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(sst_mod, "", areacell=sst_mod_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(sst_obs, "", areacell=sst_obs_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        kwargs["smoothing"] = smooth
        # 1.2 PR averaged in 'prbox' are normalized / detrended / smoothed (running average) if applicable
        pr_ts_mod, Method, keyerror_mod2 = PreProcessTS(pr_mod, Method, areacell=pr_mod_areacell, average="horizontal",
                                                        compute_anom=True, region=prbox, **kwargs)
        pr_ts_obs, _, keyerror_obs2 = PreProcessTS(pr_obs, "", areacell=pr_obs_areacell, average="horizontal",
                                                   compute_anom=True, region=prbox, **kwargs)
        del pr_mod_areacell, pr_obs_areacell, sst_mod_areacell, sst_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                    "axes3": "(mod pr) " + str([ax.id for ax in pr_ts_mod.getAxisList()]),
                    "axes4": "(obs pr) " + str([ax.id for ax in pr_ts_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                    "shape3": "(mod pr) " + str(pr_ts_mod.shape), "shape4": "(obs pr) " + str(pr_ts_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                    "time3": "(mod pr) " + str(TimeBounds(pr_ts_mod)),
                    "time4": "(obs pr) " + str(TimeBounds(pr_ts_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(enso_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression time series
            # ------------------------------------------------
            pr_slope_mod = LinearRegressionTsAgainstTs(
                pr_ts_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
            pr_slope_obs = LinearRegressionTsAgainstTs(
                pr_ts_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_slope_mod.getAxisList()]),
                              "axes2": "(obs pr) " + str([ax.id for ax in pr_slope_obs.getAxisList()]),
                              "shape1": "(mod pr) " + str(pr_slope_mod.shape),
                              "shape2": "(obs pr) " + str(pr_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsAxis(pr_slope_mod, pr_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(pr_slope_mod, pr_slope_obs, axis=0, centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(pr_slope_mod, weights=None, axis=0, centered=1, biased=1)
            sm_std_obs = Std(pr_slope_obs, weights=None, axis=0, centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(pr_slope_mod), "observations": ArrayToList(pr_slope_obs),
                              "axis": list(pr_slope_mod.getAxis(0)[:])}

            if netcdf is True:
                # Read file and select the right region
                pr_map_mod, pr_map_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                    prfilemod, prnamemod, "precipitations", metric, "equatorial_pacific", file_area=prareafilemod,
                    name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True,
                    maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                pr_map_obs, pr_map_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                    prfileobs, prnameobs, "precipitations", metric, "equatorial_pacific", file_area=prareafileobs,
                    name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True,
                    maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                # Checks if the same time period is used for both variables
                sst_mod, pr_map_mod, keyerror_mod2 = CheckTime(
                    sst_mod, pr_map_mod, metric_name=metric, debug=debug, **kwargs)
                sst_obs, pr_map_obs, keyerror_obs2 = CheckTime(
                    sst_obs, pr_map_obs, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                if keyerror is None:
                    # Preprocess PR (computes anomalies, normalizes, detrends, smoothes, averages,...)
                    pr_map_mod, _, keyerror_mod = PreProcessTS(pr_map_mod, "", areacell=pr_map_mod_areacell,
                                                               compute_anom=True, region="equatorial_pacific", **kwargs)
                    pr_map_obs, _, keyerror_obs = PreProcessTS(pr_map_obs, "", areacell=pr_map_obs_areacell,
                                                               compute_anom=True, region="equatorial_pacific", **kwargs)
                    del pr_map_mod_areacell, pr_map_obs_areacell
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_map_mod.getAxisList()]),
                                          "axes2": "(obs pr) " + str([ax.id for ax in pr_map_obs.getAxisList()]),
                                          "shape1": "(mod pr) " + str(pr_map_mod.shape),
                                          "shape2": "(obs pr) " + str(pr_map_obs.shape),
                                          "time1": "(mod pr) " + str(TimeBounds(pr_map_mod)),
                                          "time2": "(obs pr) " + str(TimeBounds(pr_map_obs))}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                        # Regridding
                        if isinstance(kwargs["regridding"], dict):
                            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name",
                                          "regridder", "regridTool", "regridMethod"}
                            extra_args = set(kwargs["regridding"]) - known_args
                            if extra_args:
                                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                            pr_map_mod, pr_map_obs, _ = TwoVarRegrid(
                                pr_map_mod, pr_map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                            if debug is True:
                                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_map_mod.getAxisList()]),
                                              "axes2": "(obs pr) " + str([ax.id for ax in pr_map_obs.getAxisList()]),
                                              "shape1": "(mod pr) " + str(pr_map_mod.shape),
                                              "shape2": "(obs pr) " + str(pr_map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                        # Meridional average
                        pr_hov_mod, keyerror_mod = AverageMeridional(pr_map_mod)
                        pr_hov_obs, keyerror_obs = AverageMeridional(pr_map_obs)
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod pr) " + str([ax.id for ax in pr_hov_mod.getAxisList()]),
                                              "axes2": "(obs pr) " + str([ax.id for ax in pr_hov_obs.getAxisList()]),
                                              "shape1": "(mod pr) " + str(pr_hov_mod.shape),
                                              "shape2": "(obs pr) " + str(pr_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Linear regression
                            pr_hov_slope_mod = LinearRegressionTsAgainstTs(
                                pr_hov_mod, enso_mod, nbr_years_window, return_stderr=False,
                                frequency=kwargs["frequency"], debug=debug)
                            pr_hov_slope_obs = LinearRegressionTsAgainstTs(
                                pr_hov_obs, enso_obs, nbr_years_window, return_stderr=False,
                                frequency=kwargs["frequency"], debug=debug)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod pr) " + str([ax.id for ax in pr_hov_slope_mod.getAxisList()]),
                                    "axes2": "(obs pr) " + str([ax.id for ax in pr_hov_slope_obs.getAxisList()]),
                                    "shape1": "(mod pr) " + str(pr_hov_slope_mod.shape),
                                    "shape2": "(obs pr) " + str(pr_hov_slope_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)
                            # Lists event years
                            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # composites
                            if len(ln_years_mod) > 0:
                                ln_pr_ts_mod = Composite(
                                    pr_ts_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                ln_pr_hov_mod = Composite(
                                    pr_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                ln_pr_ts_mod = MyEmpty(pr_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                                ln_pr_hov_mod = MyEmpty(pr_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
                            if len(en_years_mod) > 0:
                                en_pr_ts_mod = Composite(
                                    pr_ts_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                en_pr_hov_mod = Composite(
                                    pr_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                en_pr_ts_mod = MyEmpty(pr_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                                en_pr_hov_mod = MyEmpty(pr_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
                            if len(ln_years_obs) > 0:
                                ln_pr_ts_obs = Composite(
                                    pr_ts_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                ln_pr_hov_obs = Composite(
                                    pr_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                ln_pr_ts_obs = MyEmpty(pr_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                                ln_pr_hov_obs = MyEmpty(pr_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
                            if len(en_years_obs) > 0:
                                en_pr_ts_obs = Composite(
                                    pr_ts_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                en_pr_hov_obs = Composite(
                                    pr_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                en_pr_ts_obs = MyEmpty(pr_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                                en_pr_hov_obs = MyEmpty(pr_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
                            # Supplementary metrics
                            my_units = "" if kwargs["normalization"] is True else "mm/day"
                            dict_metric, dict_nc = dict(), dict()
                            nbr = 3
                            my_ev = ["nina", "nino", None, "nina", "nino"]
                            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
                            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
                            my_de = [
                                "temporal curve of " + prbox + " averaged PRA during " + str(nbr_years_window) +
                                " years (centered on ENSO) of La Nina events composite during " + season_ev + "; Nina" +
                                " = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "temporal curve of " + prbox + " averaged PRA during " + str(nbr_years_window) +
                                " years (centered on ENSO) of El Nino events composite during " + season_ev +
                                "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                                "hovmoeller (time - longitude) of PRA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " +
                                season_ev + enso_method3,
                                "hovmoeller (time - longitude) of PRA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " +
                                region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "hovmoeller (time - longitude) of PRA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " +
                                region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                            my_un = [my_units, my_units, Units, my_units, my_units]
                            my_mo = [ln_pr_ts_mod, en_pr_ts_mod, pr_hov_slope_mod, ln_pr_hov_mod, en_pr_hov_mod]
                            my_ob = [ln_pr_ts_obs, en_pr_ts_obs, pr_hov_slope_obs, ln_pr_hov_obs, en_pr_hov_obs]
                            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
                            my_ax = ["0", "0", "01", "01", "01"]
                            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                                dict_metric, dict_nc = fill_dict_axis(
                                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn,
                                    events1=ev1, events2=ev2, description=des)
                                nbr += 2
                            # save
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "arraySTD_" + dataset1: sm_std_mod,
                                     "description": Method.split(", ")[0]}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "arraySTD_" + dataset1: sm_std_obs,
                                     "description": Method.split(", ")[0]}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                                "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error,
                                "frequency": kwargs["frequency"]}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=pr_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=pr_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, \
                                my_eo, my_mo, my_ob, my_ty, my_un, my_units, nbr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "ref": Ref,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "time_frequency": kwargs["frequency"], "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSshTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs,
                  sstbox, sshbox, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="",
                  dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSshTsRmse() function computes sea surface height anomalies life cycle associated with ENSO in a 'sshbox'
    (usually the nino3) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and lagging
    ENSO).
    It is the regression of 'sshbox' averaged SSHA (sea surface height anomalies) time series onto 'region_ev' averaged
    SSTA (sea surface temperature anomalies) (usually the regression of nino3 SSHA onto nino3.4 SSTA).

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (zos) in 'sshfilemod'
    :param sshareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SSH areacell
    :param sshareanamemod: string, optional
        name of areacell for the SSH variable (areacella, areacello,...) in 'sshareafilemod'
    :param sshlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SSH landmask
    :param sshlandmasknamemod: string, optional
        name of landmask for the SSH variable (sftlf,...) in 'sshlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SSH areacell
    :param sshareanameobs: string, optional
        name of areacell for the SSH variable (areacella, areacello,...) in 'sshareafileobs'
    :param sshlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SSH landmask
    :param sshlandmasknameobs: string, optional
        name of landmask for the SSH variable (sftlf,...) in 'sshlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'nino3.4') for SST
    :param sshbox: string
        name of box (e.g. 'nino3') for SSH
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSshTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO life cyle SSHA"
    Units = "" if kwargs["normalization"] else "cm/C"
    Method = "temporal curve of " + sshbox + " averaged sea surface height anomalies during " + str(nbr_years_window) + \
             " years (centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " + season_ev + \
             enso_method3
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoSshTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    ssh_mod, ssh_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sshfilemod, sshnamemod, "sea surface height", metric, sshbox, file_area=sshareafilemod,
        name_area=sshareanamemod,
        file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    ssh_obs, ssh_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sshfileobs, sshnameobs, "sea surface height", metric, sshbox, file_area=sshareafileobs,
        name_area=sshareanameobs,
        file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, ssh_mod, keyerror_mod3 = CheckTime(sst_mod, ssh_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, ssh_obs, keyerror_obs3 = CheckTime(sst_obs, ssh_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(sst_mod, "", areacell=sst_mod_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(sst_obs, "", areacell=sst_obs_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        kwargs["smoothing"] = smooth
        # 1.2 SSH averaged in 'sshbox' are normalized / detrended / smoothed (running average) if applicable
        ssh_ts_mod, Method, keyerror_mod2 = PreProcessTS(
            ssh_mod, Method, areacell=ssh_mod_areacell, average="horizontal", compute_anom=True, region=sshbox,
            **kwargs)
        ssh_ts_obs, _, keyerror_obs2 = PreProcessTS(
            ssh_obs, "", areacell=ssh_obs_areacell, average="horizontal", compute_anom=True, region=sshbox, **kwargs)
        # change units
        ssh_ts_mod = ssh_ts_mod * 1e2
        ssh_ts_obs = ssh_ts_obs * 1e2
        del ssh_mod_areacell, ssh_obs_areacell, sst_mod_areacell, sst_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                    "axes3": "(mod ssh) " + str([ax.id for ax in ssh_ts_mod.getAxisList()]),
                    "axes4": "(obs ssh) " + str([ax.id for ax in ssh_ts_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                    "shape3": "(mod ssh) " + str(ssh_ts_mod.shape), "shape4": "(obs ssh) " + str(ssh_ts_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                    "time3": "(mod ssh) " + str(TimeBounds(ssh_ts_mod)),
                    "time4": "(obs ssh) " + str(TimeBounds(ssh_ts_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(enso_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression time series
            # ------------------------------------------------
            ssh_slope_mod = LinearRegressionTsAgainstTs(
                ssh_ts_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            ssh_slope_obs = LinearRegressionTsAgainstTs(
                ssh_ts_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_slope_mod.getAxisList()]),
                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_slope_obs.getAxisList()]),
                              "shape1": "(mod ssh) " + str(ssh_slope_mod.shape),
                              "shape2": "(obs ssh) " + str(ssh_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsAxis(ssh_slope_mod, ssh_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(ssh_slope_mod, ssh_slope_obs, axis=0, centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(ssh_slope_mod, weights=None, axis=0, centered=1, biased=1)
            sm_std_obs = Std(ssh_slope_obs, weights=None, axis=0, centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(ssh_slope_mod), "observations": ArrayToList(ssh_slope_obs),
                              "axis": list(ssh_slope_mod.getAxis(0)[:])}

            if netcdf is True:
                # Read file and select the right region
                ssh_map_mod, ssh_map_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                    sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific",
                    file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                    name_mask=sshlandmasknamemod, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"],
                    debug=debug, **kwargs)
                ssh_map_obs, ssh_map_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                    sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific",
                    file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                    name_mask=sshlandmasknameobs, maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"],
                    debug=debug, **kwargs)
                # Checks if the same time period is used for both variables
                sst_mod, ssh_map_mod, keyerror_mod2 = CheckTime(
                    sst_mod, ssh_map_mod, metric_name=metric, debug=debug, **kwargs)
                sst_obs, ssh_map_obs, keyerror_obs2 = CheckTime(
                    sst_obs, ssh_map_obs, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                if keyerror is None:
                    # Preprocess SSH (computes anomalies, normalizes, detrends, smoothes, averages,...)
                    ssh_map_mod, _, keyerror_mod = PreProcessTS(
                        ssh_map_mod, "", areacell=ssh_map_mod_areacell, compute_anom=True, region="equatorial_pacific",
                        **kwargs)
                    ssh_map_obs, _, keyerror_obs = PreProcessTS(
                        ssh_map_obs, "", areacell=ssh_map_obs_areacell, compute_anom=True, region="equatorial_pacific",
                        **kwargs)
                    del ssh_map_mod_areacell, ssh_map_obs_areacell
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_map_mod.getAxisList()]),
                                          "axes2": "(obs ssh) " + str([ax.id for ax in ssh_map_obs.getAxisList()]),
                                          "shape1": "(mod ssh) " + str(ssh_map_mod.shape),
                                          "shape2": "(obs ssh) " + str(ssh_map_obs.shape),
                                          "time1": "(mod ssh) " + str(TimeBounds(ssh_map_mod)),
                                          "time2": "(obs ssh) " + str(TimeBounds(ssh_map_obs))}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                        # Regridding
                        if isinstance(kwargs["regridding"], dict):
                            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name",
                                          "regridder", "regridTool", "regridMethod"}
                            extra_args = set(kwargs["regridding"]) - known_args
                            if extra_args:
                                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                            ssh_map_mod, ssh_map_obs, _ = TwoVarRegrid(
                                ssh_map_mod, ssh_map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                            if debug is True:
                                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_map_mod.getAxisList()]),
                                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_map_obs.getAxisList()]),
                                              "shape1": "(mod ssh) " + str(ssh_map_mod.shape),
                                              "shape2": "(obs ssh) " + str(ssh_map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                        # change units
                        ssh_map_mod = ssh_map_mod * 1e2
                        ssh_map_obs = ssh_map_obs * 1e2
                        # Meridional average
                        ssh_hov_mod, keyerror_mod = AverageMeridional(ssh_map_mod)
                        ssh_hov_obs, keyerror_obs = AverageMeridional(ssh_map_obs)
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod ssh) " + str([ax.id for ax in ssh_hov_mod.getAxisList()]),
                                              "axes2": "(obs ssh) " + str([ax.id for ax in ssh_hov_obs.getAxisList()]),
                                              "shape1": "(mod ssh) " + str(ssh_hov_mod.shape),
                                              "shape2": "(obs ssh) " + str(ssh_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Linear regression
                            ssh_hov_slope_mod = LinearRegressionTsAgainstTs(
                                ssh_hov_mod, enso_mod, nbr_years_window, return_stderr=False,
                                frequency=kwargs["frequency"], debug=debug)
                            ssh_hov_slope_obs = LinearRegressionTsAgainstTs(
                                ssh_hov_obs, enso_obs, nbr_years_window, return_stderr=False,
                                frequency=kwargs["frequency"], debug=debug)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod ssh) " + str([ax.id for ax in ssh_hov_slope_mod.getAxisList()]),
                                    "axes2": "(obs ssh) " + str([ax.id for ax in ssh_hov_slope_obs.getAxisList()]),
                                    "shape1": "(mod ssh) " + str(ssh_hov_slope_mod.shape),
                                    "shape2": "(obs ssh) " + str(ssh_hov_slope_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)
                            # Lists event years
                            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # composites
                            if len(ln_years_mod) > 0:
                                ln_ssh_ts_mod = Composite(
                                    ssh_ts_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                ln_ssh_hov_mod = Composite(
                                    ssh_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                ln_ssh_ts_mod = MyEmpty(ssh_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                                ln_ssh_hov_mod = MyEmpty(
                                    ssh_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
                            if len(en_years_mod) > 0:
                                en_ssh_ts_mod = Composite(
                                    ssh_ts_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                en_ssh_hov_mod = Composite(
                                    ssh_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                en_ssh_ts_mod = MyEmpty(ssh_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                                en_ssh_hov_mod = MyEmpty(
                                    ssh_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
                            if len(ln_years_obs) > 0:
                                ln_ssh_ts_obs = Composite(
                                    ssh_ts_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                ln_ssh_hov_obs = Composite(
                                    ssh_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                ln_ssh_ts_obs = MyEmpty(ssh_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                                ln_ssh_hov_obs = MyEmpty(
                                    ssh_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
                            if len(en_years_obs) > 0:
                                en_ssh_ts_obs = Composite(
                                    ssh_ts_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                en_ssh_hov_obs = Composite(
                                    ssh_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                en_ssh_ts_obs = MyEmpty(ssh_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                                en_ssh_hov_obs = MyEmpty(
                                    ssh_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
                            # Supplementary metrics
                            my_units = "" if kwargs["normalization"] is True else "cm"
                            dict_metric, dict_nc = dict(), dict()
                            nbr = 3
                            my_ev = ["nina", "nino", None, "nina", "nino"]
                            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
                            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
                            my_de = [
                                "temporal curve of " + sshbox + " averaged SSHA during " + str(nbr_years_window) +
                                " years (centered on ENSO) of La Nina events composite during " + season_ev + "; Nina" +
                                " = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "temporal curve of " + sshbox + " averaged SSHA during " + str(nbr_years_window) +
                                " years (centered on ENSO) of El Nino events composite during " + season_ev +
                                "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                                "hovmoeller (time - longitude) of SSHA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " +
                                season_ev + enso_method3,
                                "hovmoeller (time - longitude) of SSHA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " +
                                region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "hovmoeller (time - longitude) of SSHA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " +
                                region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                            my_un = [my_units, my_units, Units, my_units, my_units]
                            my_mo = [ln_ssh_ts_mod, en_ssh_ts_mod, ssh_hov_slope_mod, ln_ssh_hov_mod, en_ssh_hov_mod]
                            my_ob = [ln_ssh_ts_obs, en_ssh_ts_obs, ssh_hov_slope_obs, ln_ssh_hov_obs, en_ssh_hov_obs]
                            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
                            my_ax = ["0", "0", "01", "01", "01"]
                            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                                dict_metric, dict_nc = fill_dict_axis(
                                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn,
                                    events1=ev1, events2=ev2, description=des)
                                nbr += 2
                            # save
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "arraySTD_" + dataset1: sm_std_mod,
                                     "description": Method.split(", ")[0]}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "arraySTD_" + dataset1: sm_std_obs,
                                     "description": Method.split(", ")[0]}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                                "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error,
                                "frequency": kwargs["frequency"]}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=ssh_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=ssh_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, \
                                my_eo, my_mo, my_ob, my_ty, my_un, my_units, nbr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "ref": Ref,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "time_frequency": kwargs["frequency"], "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoSstTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  box, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                  debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstTsRmse() function computes SSTA (sea surface temperature anomalies) life cycle associated with ENSO in a
    'region_ev' (usually the nino3.4) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading
    and lagging ENSO).
    It is the regression of 'region_ev' averaged SSTA (sea surface temperature anomalies) time series onto 'region_ev'
    averaged SSTA (usually the regression of nino3.4 SSTA onto nino3.4 SSTA).

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param box: string
        name of box (e.g. 'nino3.4') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO life cyle SSTA"
    Units = "" if kwargs["normalization"] else "C/C"
    Method = "temporal curve of " + region_ev + " averaged SSTA during " + str(nbr_years_window) + " years (centered " + \
             "on ENSO) regressed onto " + region_ev + " averaged SSTA during " + season_ev + enso_method3
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoSstTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(sst_mod, "", areacell=mod_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(sst_obs, "", areacell=obs_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        kwargs["smoothing"] = smooth
        # 1.2 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_ts_mod, Method, keyerror_mod2 = PreProcessTS(sst_mod, Method, areacell=mod_areacell, average="horizontal",
                                                         compute_anom=True, region=region_ev, **kwargs)
        sst_ts_obs, _, keyerror_obs2 = PreProcessTS(sst_obs, "", areacell=obs_areacell, average="horizontal",
                                                    compute_anom=True, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(box mod) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                    "axes2": "(box obs) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                    "axes3": "(ts mod) " + str([ax.id for ax in sst_ts_mod.getAxisList()]),
                    "axes4": "(ts obs) " + str([ax.id for ax in sst_ts_obs.getAxisList()]),
                    "shape1": "(box mod) " + str(sst_box_mod.shape), "shape2": "(box obs) " + str(sst_box_obs.shape),
                    "shape3": "(ts mod) " + str(sst_ts_mod.shape), "shape4": "(ts obs) " + str(sst_ts_obs.shape),
                    "time1": "(box mod) " + str(TimeBounds(sst_box_mod)),
                    "time2": "(box obs) " + str(TimeBounds(sst_box_obs)),
                    "time3": "(ts mod) " + str(TimeBounds(sst_ts_mod)),
                    "time4": "(ts obs) " + str(TimeBounds(sst_ts_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(box mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(box obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "shape1": "(box mod) " + str(enso_mod.shape), "shape2": "(box obs) " + str(enso_obs.shape),
                    "time1": "(box mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(box obs) " + str(TimeBounds(enso_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression time series
            # ------------------------------------------------
            sst_slope_mod = LinearRegressionTsAgainstTs(
                sst_ts_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
            sst_slope_obs = LinearRegressionTsAgainstTs(
                sst_ts_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"], debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(ts mod) " + str([ax.id for ax in sst_slope_mod.getAxisList()]),
                              "axes2": "(ts obs) " + str([ax.id for ax in sst_slope_obs.getAxisList()]),
                              "shape1": "(ts mod) " + str(sst_slope_mod.shape),
                              "shape2": "(ts obs) " + str(sst_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsAxis(sst_slope_mod, sst_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(sst_slope_mod, sst_slope_obs, axis=0, centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(sst_slope_mod, weights=None, axis=0, centered=1, biased=1)
            sm_std_obs = Std(sst_slope_obs, weights=None, axis=0, centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(sst_slope_mod), "observations": ArrayToList(sst_slope_obs),
                              "axis": list(sst_slope_mod.getAxis(0)[:])}

            if netcdf is True:
                # Read file and select the right region
                sst_map_mod, sst_map_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                    sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific", file_area=sstareafilemod,
                    name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True,
                    maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                sst_map_obs, sst_map_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                    sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific", file_area=sstareafileobs,
                    name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True,
                    maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                # Checks if the same time period is used for both variables
                sst_mod, sst_map_mod, keyerror_mod2 = CheckTime(
                    sst_mod, sst_map_mod, metric_name=metric, debug=debug, **kwargs)
                sst_obs, sst_map_obs, keyerror_obs2 = CheckTime(
                    sst_obs, sst_map_obs, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                if keyerror is None:
                    # Preprocess SST (computes anomalies, normalizes, detrends, smoothes, averages,...)
                    sst_map_mod, _, keyerror_mod = PreProcessTS(
                        sst_map_mod, "", areacell=sst_map_mod_areacell, compute_anom=True, region="equatorial_pacific",
                        **kwargs)
                    sst_map_obs, _, keyerror_obs = PreProcessTS(
                        sst_map_obs, "", areacell=sst_map_obs_areacell, compute_anom=True, region="equatorial_pacific",
                        **kwargs)
                    del sst_map_mod_areacell, sst_map_obs_areacell
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(hov mod) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                                          "axes2": "(hov obs) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                                          "shape1": "(hov mod) " + str(sst_map_mod.shape),
                                          "shape2": "(hov obs) " + str(sst_map_obs.shape),
                                          "time1": "(hov mod) " + str(TimeBounds(sst_map_mod)),
                                          "time2": "(hov obs) " + str(TimeBounds(sst_map_obs))}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                        # Regridding
                        if isinstance(kwargs["regridding"], dict):
                            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name",
                                          "regridder", "regridTool", "regridMethod"}
                            extra_args = set(kwargs["regridding"]) - known_args
                            if extra_args:
                                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                            sst_map_mod, sst_map_obs, _ = TwoVarRegrid(
                                sst_map_mod, sst_map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                            if debug is True:
                                dict_debug = {"axes1": "(hov mod) " + str([ax.id for ax in sst_map_mod.getAxisList()]),
                                              "axes2": "(hov obs) " + str([ax.id for ax in sst_map_obs.getAxisList()]),
                                              "shape1": "(hov mod) " + str(sst_map_mod.shape),
                                              "shape2": "(hov obs) " + str(sst_map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                        # Meridional average
                        sst_hov_mod, keyerror_mod = AverageMeridional(sst_map_mod)
                        sst_hov_obs, keyerror_obs = AverageMeridional(sst_map_obs)
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(hov mod) " + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                              "axes2": "(hov obs) " + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                              "shape1": "(hov mod) " + str(sst_hov_mod.shape),
                                              "shape2": "(hov obs) " + str(sst_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Linear regression
                            sst_hov_slope_mod = LinearRegressionTsAgainstTs(
                                sst_hov_mod, enso_mod, nbr_years_window, return_stderr=False,
                                frequency=kwargs["frequency"], debug=debug)
                            sst_hov_slope_obs = LinearRegressionTsAgainstTs(
                                sst_hov_obs, enso_obs, nbr_years_window, return_stderr=False,
                                frequency=kwargs["frequency"], debug=debug)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(hov mod) " + str([ax.id for ax in sst_hov_slope_mod.getAxisList()]),
                                    "axes2": "(hov obs) " + str([ax.id for ax in sst_hov_slope_obs.getAxisList()]),
                                    "shape1": "(hov mod) " + str(sst_hov_slope_mod.shape),
                                    "shape2": "(hov obs) " + str(sst_hov_slope_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)
                            # Lists event years
                            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # composites
                            if len(ln_years_mod) > 0:
                                ln_sst_ts_mod = Composite(
                                    sst_ts_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                ln_sst_hov_mod = Composite(
                                    sst_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                ln_sst_ts_mod = MyEmpty(sst_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                                ln_sst_hov_mod = MyEmpty(
                                    sst_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
                            if len(en_years_mod) > 0:
                                en_sst_ts_mod = Composite(
                                    sst_ts_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                en_sst_hov_mod = Composite(
                                    sst_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                en_sst_ts_mod = MyEmpty(sst_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                                en_sst_hov_mod = MyEmpty(
                                    sst_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
                            if len(ln_years_obs) > 0:
                                ln_sst_ts_obs = Composite(
                                    sst_ts_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                ln_sst_hov_obs = Composite(
                                    sst_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                ln_sst_ts_obs = MyEmpty(sst_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                                ln_sst_hov_obs = MyEmpty(
                                    sst_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
                            if len(en_years_obs) > 0:
                                en_sst_ts_obs = Composite(
                                    sst_ts_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                en_sst_hov_obs = Composite(
                                    sst_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                en_sst_ts_obs = MyEmpty(sst_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                                en_sst_hov_obs = MyEmpty(
                                    sst_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
                            # Supplementary metrics
                            my_units = "" if kwargs["normalization"] is True else "C"
                            dict_metric, dict_nc = dict(), dict()
                            nbr = 3
                            my_ev = ["nina", "nino", None, "nina", "nino"]
                            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
                            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
                            my_de = [
                                "temporal curve of " + region_ev + " averaged SSTA during " + str(nbr_years_window) +
                                " years (centered on ENSO) of La Nina events composite during " + season_ev + "; Nina" +
                                " = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "temporal curve of " + region_ev + " averaged SSTA during " + str(nbr_years_window) +
                                " years (centered on ENSO) of El Nino events composite during " + season_ev +
                                "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                                "hovmoeller (time - longitude) of SSTA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " +
                                season_ev + enso_method3,
                                "hovmoeller (time - longitude) of SSTA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " +
                                region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "hovmoeller (time - longitude) of SSTA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " +
                                region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                            my_un = [my_units, my_units, Units, my_units, my_units]
                            my_mo = [ln_sst_ts_mod, en_sst_ts_mod, sst_hov_slope_mod, ln_sst_hov_mod, en_sst_hov_mod]
                            my_ob = [ln_sst_ts_obs, en_sst_ts_obs, sst_hov_slope_obs, ln_sst_hov_obs, en_sst_hov_obs]
                            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
                            my_ax = ["0", "0", "01", "01", "01"]
                            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                                dict_metric, dict_nc = fill_dict_axis(
                                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn,
                                    events1=ev1, events2=ev2, description=des)
                                nbr += 2
                            # save
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "arraySTD_" + dataset1: sm_std_mod,
                                     "description": Method.split(", ")[0]}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "arraySTD_" + dataset1: sm_std_obs,
                                     "description": Method.split(", ")[0]}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                                "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error,
                                "frequency": kwargs["frequency"]}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=sst_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=sst_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, \
                                my_eo, my_mo, my_ob, my_ty, my_un, my_units, nbr
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "ref": Ref,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "time_frequency": kwargs["frequency"], "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoTauxTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   tauxfilemod, tauxnamemod, tauxareafilemod, tauxareanamemod, tauxlandmaskfilemod, tauxlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                   tauxfileobs, tauxnameobs, tauxareafileobs, tauxareanameobs, tauxlandmaskfileobs, tauxlandmasknameobs,
                   sstbox, tauxbox, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="",
                   dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoTauxTsRmse() function computes zonal wind stress anomalies life cycle associated with ENSO in a 'tauxbox'
    (usually the nino4) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and lagging
    ENSO).
    It is the regression of 'tauxbox' averaged TAUXA (zonal wind stress anomalies) time series onto 'region_ev' averaged
    SSTA (sea surface temperature anomalies) (usually the regression of nino4 TAUXA onto nino3.4 SSTA).

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param tauxfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUX
    :param tauxnamemod: string
        name of TAUX variable (tauu, tauuo) in 'tauxfilemod'
    :param tauxareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled TAUX areacell
    :param tauxareanamemod: string, optional
        name of areacell for the TAUX variable (areacella, areacello,...) in 'tauxareafilemod'
    :param tauxlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled TAUX landmask
    :param tauxlandmasknamemod: string, optional
        name of landmask for the TAUX variable (sftlf,...) in 'tauxlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param tauxfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUX
    :param tauxnameobs: string
        name of TAUX variable (tauu, taux) in 'tauxfileobs'
    :param tauxareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed TAUX areacell
    :param tauxareanameobs: string, optional
        name of areacell for the TAUX variable (areacella, areacello,...) in 'tauxareafileobs'
    :param tauxlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed TAUX landmask
    :param tauxlandmasknameobs: string, optional
        name of landmask for the TAUX variable (sftlf,...) in 'tauxlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'nino3.4') for SST
    :param tauxbox: string
        name of box (e.g. 'nino3') for TAUX
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoTauxTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO life cyle TAUXA"
    Units = "" if kwargs["normalization"] else "1e-3 N/m2/C"
    Method = "temporal curve of " + tauxbox + " averaged zonal wind stress anomalies during " + str(nbr_years_window) + \
             " years (centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " + season_ev + \
             enso_method3
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoTauxTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    taux_mod, taux_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        tauxfilemod, tauxnamemod, "wind stress", metric, tauxbox, file_area=tauxareafilemod, name_area=tauxareanamemod,
        file_mask=tauxlandmaskfilemod, name_mask=tauxlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    taux_obs, taux_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        tauxfileobs, tauxnameobs, "wind stress", metric, tauxbox, file_area=tauxareafileobs, name_area=tauxareanameobs,
        file_mask=tauxlandmaskfileobs, name_mask=tauxlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, taux_mod, keyerror_mod3 = CheckTime(sst_mod, taux_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, taux_obs, keyerror_obs3 = CheckTime(sst_obs, taux_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(sst_mod, "", areacell=sst_mod_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(sst_obs, "", areacell=sst_obs_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        kwargs["smoothing"] = smooth
        # 1.2 TAUX averaged in 'tauxbox' are normalized / detrended / smoothed (running average) if applicable
        taux_ts_mod, Method, keyerror_mod2 = PreProcessTS(
            taux_mod, Method, areacell=taux_mod_areacell, average="horizontal", compute_anom=True, region=tauxbox,
            **kwargs)
        taux_ts_obs, _, keyerror_obs2 = PreProcessTS(taux_obs, "", areacell=taux_obs_areacell, average="horizontal",
                                                     compute_anom=True, region=tauxbox, **kwargs)
        # change units
        taux_ts_mod = taux_ts_mod * 1e3
        taux_ts_obs = taux_ts_obs * 1e3
        del taux_mod_areacell, taux_obs_areacell, sst_mod_areacell, sst_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                    "axes3": "(mod taux) " + str([ax.id for ax in taux_ts_mod.getAxisList()]),
                    "axes4": "(obs taux) " + str([ax.id for ax in taux_ts_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                    "shape3": "(mod taux) " + str(taux_ts_mod.shape), "shape4": "(obs taux) " + str(taux_ts_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                    "time3": "(mod taux) " + str(TimeBounds(taux_ts_mod)),
                    "time4": "(obs taux) " + str(TimeBounds(taux_ts_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(enso_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression time series
            # ------------------------------------------------
            taux_slope_mod = LinearRegressionTsAgainstTs(
                taux_ts_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            taux_slope_obs = LinearRegressionTsAgainstTs(
                taux_ts_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(mod taux) " + str([ax.id for ax in taux_slope_mod.getAxisList()]),
                              "axes2": "(obs taux) " + str([ax.id for ax in taux_slope_obs.getAxisList()]),
                              "shape1": "(mod taux) " + str(taux_slope_mod.shape),
                              "shape2": "(obs taux) " + str(taux_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsAxis(taux_slope_mod, taux_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(taux_slope_mod, taux_slope_obs, axis=0, centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(taux_slope_mod, weights=None, axis=0, centered=1, biased=1)
            sm_std_obs = Std(taux_slope_obs, weights=None, axis=0, centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(taux_slope_mod), "observations": ArrayToList(taux_slope_obs),
                              "axis": list(taux_slope_mod.getAxis(0)[:])}

            if netcdf is True:
                # Read file and select the right region
                taux_map_mod, taux_map_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                    tauxfilemod, tauxnamemod, "wind stress", metric, "equatorial_pacific", file_area=tauxareafilemod,
                    name_area=tauxareanamemod, file_mask=tauxlandmaskfilemod, name_mask=tauxlandmasknamemod,
                    maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                taux_map_obs, taux_map_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                    tauxfileobs, tauxnameobs, "wind stress", metric, "equatorial_pacific", file_area=tauxareafileobs,
                    name_area=tauxareanameobs, file_mask=tauxlandmaskfileobs, name_mask=tauxlandmasknameobs,
                    maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                # Checks if the same time period is used for both variables
                sst_mod, taux_map_mod, keyerror_mod2 = CheckTime(
                    sst_mod, taux_map_mod, metric_name=metric, debug=debug, **kwargs)
                sst_obs, taux_map_obs, keyerror_obs2 = CheckTime(
                    sst_obs, taux_map_obs, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                if keyerror is None:
                    # Preprocess TAUX (computes anomalies, normalizes, detrends, smoothes, averages,...)
                    taux_map_mod, _, keyerror_mod = PreProcessTS(
                        taux_map_mod, "", areacell=taux_map_mod_areacell, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    taux_map_obs, _, keyerror_obs = PreProcessTS(
                        taux_map_obs, "", areacell=taux_map_obs_areacell, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del taux_map_mod_areacell, taux_map_obs_areacell
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(mod taux) " + str([ax.id for ax in taux_map_mod.getAxisList()]),
                                          "axes2": "(obs taux) " + str([ax.id for ax in taux_map_obs.getAxisList()]),
                                          "shape1": "(mod taux) " + str(taux_map_mod.shape),
                                          "shape2": "(obs taux) " + str(taux_map_obs.shape),
                                          "time1": "(mod taux) " + str(TimeBounds(taux_map_mod)),
                                          "time2": "(obs taux) " + str(TimeBounds(taux_map_obs))}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                        # Regridding
                        if isinstance(kwargs["regridding"], dict):
                            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name",
                                          "regridder", "regridTool", "regridMethod"}
                            extra_args = set(kwargs["regridding"]) - known_args
                            if extra_args:
                                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                            taux_map_mod, taux_map_obs, _ = TwoVarRegrid(
                                taux_map_mod, taux_map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod taux) " + str([ax.id for ax in taux_map_mod.getAxisList()]),
                                    "axes2": "(obs taux) " + str([ax.id for ax in taux_map_obs.getAxisList()]),
                                    "shape1": "(mod taux) " + str(taux_map_mod.shape),
                                    "shape2": "(obs taux) " + str(taux_map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                        # change units
                        taux_map_mod = taux_map_mod * 1e3
                        taux_map_obs = taux_map_obs * 1e3
                        # Meridional average
                        taux_hov_mod, keyerror_mod = AverageMeridional(taux_map_mod)
                        taux_hov_obs, keyerror_obs = AverageMeridional(taux_map_obs)
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod taux) " + str([ax.id for ax in taux_hov_mod.getAxisList()]),
                                    "axes2": "(obs taux) " + str([ax.id for ax in taux_hov_obs.getAxisList()]),
                                    "shape1": "(mod taux) " + str(taux_hov_mod.shape),
                                    "shape2": "(obs taux) " + str(taux_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Linear regression
                            taux_hov_slope_mod = LinearRegressionTsAgainstTs(
                                taux_hov_mod, enso_mod, nbr_years_window, return_stderr=False,
                                frequency=kwargs["frequency"], debug=debug)
                            taux_hov_slope_obs = LinearRegressionTsAgainstTs(
                                taux_hov_obs, enso_obs, nbr_years_window, return_stderr=False,
                                frequency=kwargs["frequency"], debug=debug)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod taux) " + str([ax.id for ax in taux_hov_slope_mod.getAxisList()]),
                                    "axes2": "(obs taux) " + str([ax.id for ax in taux_hov_slope_obs.getAxisList()]),
                                    "shape1": "(mod taux) " + str(taux_hov_slope_mod.shape),
                                    "shape2": "(obs taux) " + str(taux_hov_slope_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)
                            # Lists event years
                            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # composites
                            if len(ln_years_mod) > 0:
                                ln_taux_ts_mod = Composite(
                                    taux_ts_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                ln_taux_hov_mod = Composite(
                                    taux_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                ln_taux_ts_mod = MyEmpty(
                                    taux_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                                ln_taux_hov_mod = MyEmpty(
                                    taux_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
                            if len(en_years_mod) > 0:
                                en_taux_ts_mod = Composite(
                                    taux_ts_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                en_taux_hov_mod = Composite(
                                    taux_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                en_taux_ts_mod = MyEmpty(
                                    taux_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                                en_taux_hov_mod = MyEmpty(
                                    taux_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
                            if len(ln_years_obs) > 0:
                                ln_taux_ts_obs = Composite(
                                    taux_ts_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                ln_taux_hov_obs = Composite(
                                    taux_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                ln_taux_ts_obs = MyEmpty(
                                    taux_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                                ln_taux_hov_obs = MyEmpty(
                                    taux_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
                            if len(en_years_obs) > 0:
                                en_taux_ts_obs = Composite(
                                    taux_ts_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                en_taux_hov_obs = Composite(
                                    taux_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                en_taux_ts_obs = MyEmpty(
                                    taux_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                                en_taux_hov_obs = MyEmpty(
                                    taux_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
                            # Supplementary metrics
                            my_units = "" if kwargs["normalization"] is True else "1e-3 N/m2"
                            dict_metric, dict_nc = dict(), dict()
                            nbr = 3
                            my_ev = ["nina", "nino", None, "nina", "nino"]
                            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
                            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
                            my_de = [
                                "temporal curve of " + tauxbox + " averaged TAUXA during " + str(nbr_years_window) +
                                " years (centered on ENSO) of La Nina events composite during " + season_ev + "; Nina" +
                                " = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "temporal curve of " + tauxbox + " averaged TAUXA during " + str(nbr_years_window) +
                                " years (centered on ENSO) of El Nino events composite during " + season_ev +
                                "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                                "hovmoeller (time - longitude) of TAUXA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " +
                                season_ev + enso_method3,
                                "hovmoeller (time - longitude) of TAUXA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " +
                                region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "hovmoeller (time - longitude) of TAUXA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " +
                                region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                            my_un = [my_units, my_units, Units, my_units, my_units]
                            my_mo = [ln_taux_ts_mod, en_taux_ts_mod, taux_hov_slope_mod, ln_taux_hov_mod,
                                     en_taux_hov_mod]
                            my_ob = [ln_taux_ts_obs, en_taux_ts_obs, taux_hov_slope_obs, ln_taux_hov_obs,
                                     en_taux_hov_obs]
                            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
                            my_ax = ["0", "0", "01", "01", "01"]
                            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                                dict_metric, dict_nc = fill_dict_axis(
                                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn,
                                    events1=ev1, events2=ev2, description=des)
                                nbr += 2
                            # save
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "arraySTD_" + dataset1: sm_std_mod,
                                     "description": Method.split(", ")[0]}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "arraySTD_" + dataset1: sm_std_obs,
                                     "description": Method.split(", ")[0]}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                                "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error,
                                "frequency": kwargs["frequency"]}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=taux_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=taux_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, \
                                my_eo, my_mo, my_ob, my_ty, my_un, my_units, nbr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "ref": Ref,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "time_frequency": kwargs["frequency"], "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoTauyTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   tauyfilemod, tauynamemod, tauyareafilemod, tauyareanamemod, tauylandmaskfilemod, tauylandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                   tauyfileobs, tauynameobs, tauyareafileobs, tauyareanameobs, tauylandmaskfileobs, tauylandmasknameobs,
                   sstbox, tauybox, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="",
                   dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoTauyTsRmse() function computes meridional wind stress anomalies life cycle associated with ENSO in a
    'tauybox' (usually the nino3) with a window of 'nbr_years_window' centered on ENSO (nbr_years_window/2 leading and
    lagging ENSO).
    It is the regression of 'tauybox' averaged TAUYA (meridional wind stress anomalies) time series onto 'region_ev'
    averaged SSTA (sea surface temperature anomalies) (usually the regression of nino3 TAUYA onto nino3.4 SSTA).

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param tauyfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUY
    :param tauynamemod: string
        name of TAUY variable (tauv, tauvo) in 'tauyfilemod'
    :param tauyareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled TAUY areacell
    :param tauyareanamemod: string, optional
        name of areacell for the TAUY variable (areacella, areacello,...) in 'tauyareafilemod'
    :param tauylandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled TAUY landmask
    :param tauylandmasknamemod: string, optional
        name of landmask for the TAUY variable (sftlf,...) in 'tauylandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param tauyfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUY
    :param tauynameobs: string
        name of TAUY variable (tauv, tauy) in 'tauyfileobs'
    :param tauyareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed TAUY areacell
    :param tauyareanameobs: string, optional
        name of areacell for the TAUY variable (areacella, areacello,...) in 'tauyareafileobs'
    :param tauylandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed TAUY landmask
    :param tauylandmasknameobs: string, optional
        name of landmask for the TAUY variable (sftlf,...) in 'tauylandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'nino3.4') for SST
    :param tauybox: string
        name of box (e.g. 'nino3') for TAUY
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoTauyTsRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_mod,
        time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO life cyle of TAUYA"
    Units = "" if kwargs["normalization"] else "1e-3 N/m2/C"
    Method = "temporal curve of " + tauybox + " averaged meridional wind stress anomalies during " + \
             str(nbr_years_window) + " years (centered on ENSO) regressed onto " + region_ev + " averaged SSTA " + \
             "during " + season_ev + enso_method3
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoTauyTsRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, sst_mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, sst_obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    tauy_mod, tauy_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        tauyfilemod, tauynamemod, "wind stress", metric, tauybox, file_area=tauyareafilemod, name_area=tauyareanamemod,
        file_mask=tauylandmaskfilemod, name_mask=tauylandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tauy_obs, tauy_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        tauyfileobs, tauynameobs, "wind stress", metric, tauybox, file_area=tauyareafileobs, name_area=tauyareanameobs,
        file_mask=tauylandmaskfileobs, name_mask=tauylandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tauy_mod, keyerror_mod3 = CheckTime(sst_mod, tauy_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tauy_obs, keyerror_obs3 = CheckTime(sst_obs, tauy_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_box_mod, _, keyerror_mod1 = PreProcessTS(sst_mod, "", areacell=sst_mod_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        sst_box_obs, _, keyerror_obs1 = PreProcessTS(sst_obs, "", areacell=sst_obs_areacell, average="horizontal",
                                                     compute_anom=False, region=region_ev, **kwargs)
        kwargs["smoothing"] = smooth
        # 1.2 TAUY averaged in 'tauybox' are normalized / detrended / smoothed (running average) if applicable
        tauy_ts_mod, Method, keyerror_mod2 = PreProcessTS(
            tauy_mod, Method, areacell=tauy_mod_areacell, average="horizontal", compute_anom=True, region=tauybox,
            **kwargs)
        tauy_ts_obs, _, keyerror_obs2 = PreProcessTS(tauy_obs, "", areacell=tauy_obs_areacell, average="horizontal",
                                                     compute_anom=True, region=tauybox, **kwargs)
        # change units
        tauy_ts_mod = tauy_ts_mod * 1e3
        tauy_ts_obs = tauy_ts_obs * 1e3
        del tauy_mod_areacell, tauy_obs_areacell, sst_mod_areacell, sst_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in sst_box_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in sst_box_obs.getAxisList()]),
                    "axes3": "(mod tauy) " + str([ax.id for ax in tauy_ts_mod.getAxisList()]),
                    "axes4": "(obs tauy) " + str([ax.id for ax in tauy_ts_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(sst_box_mod.shape), "shape2": "(obs sst) " + str(sst_box_obs.shape),
                    "shape3": "(mod tauy) " + str(tauy_ts_mod.shape), "shape4": "(obs tauy) " + str(tauy_ts_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(sst_box_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(sst_box_obs)),
                    "time3": "(mod tauy) " + str(TimeBounds(tauy_ts_mod)),
                    "time4": "(obs tauy) " + str(TimeBounds(tauy_ts_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_box_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_box_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod sst) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(obs sst) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "shape1": "(mod sst) " + str(enso_mod.shape), "shape2": "(obs sst) " + str(enso_obs.shape),
                    "time1": "(mod sst) " + str(TimeBounds(enso_mod)),
                    "time2": "(obs sst) " + str(TimeBounds(enso_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression time series
            # ------------------------------------------------
            tauy_slope_mod = LinearRegressionTsAgainstTs(
                tauy_ts_mod, enso_mod, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            tauy_slope_obs = LinearRegressionTsAgainstTs(
                tauy_ts_obs, enso_obs, nbr_years_window, return_stderr=False, frequency=kwargs["frequency"],
                debug=debug)
            if debug is True:
                dict_debug = {"axes1": "(mod tauy) " + str([ax.id for ax in tauy_slope_mod.getAxisList()]),
                              "axes2": "(obs tauy) " + str([ax.id for ax in tauy_slope_obs.getAxisList()]),
                              "shape1": "(mod tauy) " + str(tauy_slope_mod.shape),
                              "shape2": "(obs tauy) " + str(tauy_slope_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)

            # Computes the root mean square difference
            mv, keyerror = RmsAxis(tauy_slope_mod, tauy_slope_obs, axis=0, centered=centered_rmse, biased=biased_rmse)
            # Error on the metric
            mv_error = None

            # Supplementary metrics
            sm_corr = float(Correlation(tauy_slope_mod, tauy_slope_obs, axis=0, centered=1, biased=1))
            sm_corr_error = None
            sm_std_mod = Std(tauy_slope_mod, weights=None, axis=0, centered=1, biased=1)
            sm_std_obs = Std(tauy_slope_obs, weights=None, axis=0, centered=1, biased=1)
            sm_std = float(sm_std_mod) / float(sm_std_obs)
            sm_std_error = None

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(tauy_slope_mod), "observations": ArrayToList(tauy_slope_obs),
                              "axis": list(tauy_slope_mod.getAxis(0)[:])}

            if netcdf is True:
                # Read file and select the right region
                tauy_map_mod, tauy_map_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                    tauyfilemod, tauynamemod, "wind stress", metric, "equatorial_pacific", file_area=tauyareafilemod,
                    name_area=tauyareanamemod, file_mask=tauylandmaskfilemod, name_mask=tauylandmasknamemod,
                    maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                tauy_map_obs, tauy_map_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                    tauyfileobs, tauynameobs, "wind stress", metric, "equatorial_pacific", file_area=tauyareafileobs,
                    name_area=tauyareanameobs, file_mask=tauylandmaskfileobs, name_mask=tauylandmasknameobs,
                    maskland=True, maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                # Checks if the same time period is used for both variables
                sst_mod, tauy_map_mod, keyerror_mod2 = CheckTime(
                    sst_mod, tauy_map_mod, metric_name=metric, debug=debug, **kwargs)
                sst_obs, tauy_map_obs, keyerror_obs2 = CheckTime(
                    sst_obs, tauy_map_obs, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                if keyerror is None:
                    # Preprocess TAUY (computes anomalies, normalizes, detrends, smoothes, averages,...)
                    tauy_map_mod, _, keyerror_mod = PreProcessTS(
                        tauy_map_mod, "", areacell=tauy_map_mod_areacell, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    tauy_map_obs, _, keyerror_obs = PreProcessTS(
                        tauy_map_obs, "", areacell=tauy_map_obs_areacell, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del tauy_map_mod_areacell, tauy_map_obs_areacell
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(mod tauy) " + str([ax.id for ax in tauy_map_mod.getAxisList()]),
                                          "axes2": "(obs tauy) " + str([ax.id for ax in tauy_map_obs.getAxisList()]),
                                          "shape1": "(mod tauy) " + str(tauy_map_mod.shape),
                                          "shape2": "(obs tauy) " + str(tauy_map_obs.shape),
                                          "time1": "(mod tauy) " + str(TimeBounds(tauy_map_mod)),
                                          "time2": "(obs tauy) " + str(TimeBounds(tauy_map_obs))}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                        # Regridding
                        if isinstance(kwargs["regridding"], dict):
                            known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name",
                                          "regridder", "regridTool", "regridMethod"}
                            extra_args = set(kwargs["regridding"]) - known_args
                            if extra_args:
                                EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                            tauy_map_mod, tauy_map_obs, _ = TwoVarRegrid(
                                tauy_map_mod, tauy_map_obs, "", region="equatorial_pacific", **kwargs["regridding"])
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod tauy) " + str([ax.id for ax in tauy_map_mod.getAxisList()]),
                                    "axes2": "(obs tauy) " + str([ax.id for ax in tauy_map_obs.getAxisList()]),
                                    "shape1": "(mod tauy) " + str(tauy_map_mod.shape),
                                    "shape2": "(obs tauy) " + str(tauy_map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                        # change units
                        tauy_map_mod = tauy_map_mod * 1e3
                        tauy_map_obs = tauy_map_obs * 1e3
                        # Meridional average
                        tauy_hov_mod, keyerror_mod = AverageMeridional(tauy_map_mod)
                        tauy_hov_obs, keyerror_obs = AverageMeridional(tauy_map_obs)
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod tauy) " + str([ax.id for ax in tauy_hov_mod.getAxisList()]),
                                    "axes2": "(obs tauy) " + str([ax.id for ax in tauy_hov_obs.getAxisList()]),
                                    "shape1": "(mod tauy) " + str(tauy_hov_mod.shape),
                                    "shape2": "(obs tauy) " + str(tauy_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Linear regression
                            tauy_hov_slope_mod = LinearRegressionTsAgainstTs(
                                tauy_hov_mod, enso_mod, nbr_years_window, return_stderr=False,
                                frequency=kwargs["frequency"], debug=debug)
                            tauy_hov_slope_obs = LinearRegressionTsAgainstTs(
                                tauy_hov_obs, enso_obs, nbr_years_window, return_stderr=False,
                                frequency=kwargs["frequency"], debug=debug)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(mod tauy) " + str([ax.id for ax in tauy_hov_slope_mod.getAxisList()]),
                                    "axes2": "(obs tauy) " + str([ax.id for ax in tauy_hov_slope_obs.getAxisList()]),
                                    "shape1": "(mod tauy) " + str(tauy_hov_slope_mod.shape),
                                    "shape2": "(obs tauy) " + str(tauy_hov_slope_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionTsAgainstTs", 15, **dict_debug)
                            # Lists event years
                            ln_years_mod = DetectEvents(sst_box_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_mod = DetectEvents(sst_box_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_box_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_box_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs),
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # composites
                            if len(ln_years_mod) > 0:
                                ln_tauy_ts_mod = Composite(
                                    tauy_ts_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                ln_tauy_hov_mod = Composite(
                                    tauy_hov_mod, ln_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                ln_tauy_ts_mod = MyEmpty(
                                    tauy_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                                ln_tauy_hov_mod = MyEmpty(
                                    tauy_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
                            if len(en_years_mod) > 0:
                                en_tauy_ts_mod = Composite(
                                    tauy_ts_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                en_tauy_hov_mod = Composite(
                                    tauy_hov_mod, en_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                en_tauy_ts_mod = MyEmpty(
                                    tauy_ts_mod[:12 * nbr_years_window], time=True, time_id="months")
                                en_tauy_hov_mod = MyEmpty(
                                    tauy_hov_mod[:12 * nbr_years_window], time=True, time_id="months")
                            if len(ln_years_obs) > 0:
                                ln_tauy_ts_obs = Composite(
                                    tauy_ts_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                ln_tauy_hov_obs = Composite(
                                    tauy_hov_obs, ln_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                ln_tauy_ts_obs = MyEmpty(
                                    tauy_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                                ln_tauy_hov_obs = MyEmpty(
                                    tauy_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
                            if len(en_years_obs) > 0:
                                en_tauy_ts_obs = Composite(
                                    tauy_ts_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                                en_tauy_hov_obs = Composite(
                                    tauy_hov_obs, en_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            else:
                                en_tauy_ts_obs = MyEmpty(
                                    tauy_ts_obs[:12 * nbr_years_window], time=True, time_id="months")
                                en_tauy_hov_obs = MyEmpty(
                                    tauy_hov_obs[:12 * nbr_years_window], time=True, time_id="months")
                            # Supplementary metrics
                            my_units = "" if kwargs["normalization"] is True else "1e-3 N/m2"
                            dict_metric, dict_nc = dict(), dict()
                            nbr = 3
                            my_ev = ["nina", "nino", None, "nina", "nino"]
                            my_em = [ln_years_mod, en_years_mod, None, ln_years_mod, en_years_mod]
                            my_eo = [ln_years_obs, en_years_obs, None, ln_years_obs, en_years_obs]
                            my_de = [
                                "temporal curve of " + tauybox + " averaged TAUYA during " + str(nbr_years_window) +
                                " years (centered on ENSO) of La Nina events composite during " + season_ev + "; Nina" +
                                " = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "temporal curve of " + tauybox + " averaged TAUYA during " + str(nbr_years_window) +
                                " years (centered on ENSO) of El Nino events composite during " + season_ev +
                                "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method,
                                "hovmoeller (time - longitude) of TAUYA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) regressed onto " + region_ev + " averaged SSTA during " +
                                season_ev + enso_method3,
                                "hovmoeller (time - longitude) of TAUYA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) of La Nina events composite during " + season_ev + "; Nina = " +
                                region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                "hovmoeller (time - longitude) of TAUYA during " + str(nbr_years_window) + " years " +
                                "(centered on ENSO) of El Nino events composite during " + season_ev + "; Nino = " +
                                region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                            my_un = [my_units, my_units, Units, my_units, my_units]
                            my_mo = [ln_tauy_ts_mod, en_tauy_ts_mod, tauy_hov_slope_mod, ln_tauy_hov_mod,
                                     en_tauy_hov_mod]
                            my_ob = [ln_tauy_ts_obs, en_tauy_ts_obs, tauy_hov_slope_obs, ln_tauy_hov_obs,
                                     en_tauy_hov_obs]
                            my_ty = ["ts_nina", "ts_nino", "hov", "hov_nina", "hov_nino"]
                            my_ax = ["0", "0", "01", "01", "01"]
                            for jj, (evn, des, vna, add, uni, tab1, tab2, ev1, ev2, axi) in enumerate(
                                    zip(my_ev, my_de, ovar[1:], my_ty, my_un, my_mo, my_ob, my_em, my_eo, my_ax)):
                                dict_metric, dict_nc = fill_dict_axis(
                                    tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, nbr, vna, add, uni, axi, centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn,
                                    events1=ev1, events2=ev2, description=des)
                                nbr += 2
                            # save
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "arraySTD_" + dataset1: sm_std_mod,
                                     "description": Method.split(", ")[0]}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "arraySTD_" + dataset1: sm_std_obs,
                                     "description": Method.split(", ")[0]}
                            dict3 = {
                                "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                "metric_value_" + dataset2: mv, "metric_value_error_" + dataset2: mv_error,
                                "CORR_" + dataset2 + "_ts": sm_corr, "CORR_error_" + dataset2 + "_ts": sm_corr_error,
                                "STD_" + dataset2 + "_ts": sm_std, "STD_error_" + dataset2 + "_ts": sm_std_error,
                                "frequency": kwargs["frequency"]}
                            dict3.update(dict_metric)
                            SaveNetcdf(
                                file_name, global_attributes=dict3,
                                var1=tauy_slope_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                var2=tauy_slope_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                            del dict1, dict2, dict3, dict_metric, dict_nc, file_name, my_ax, my_de, my_ev, my_em, \
                                my_eo, my_mo, my_ob, my_ty, my_un, my_units, nbr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method, "ref": Ref,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "time_frequency": kwargs["frequency"], "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSshSst(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, sshfile, sshname,
                 sshareafile, sshareaname, sshlandmaskfile, sshlandmaskname, sshbox, dataset="", debug=False,
                 netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSshSst() function computes the regression of 'sstbox' sstA (sea surface temperature anomalies) onto
    'sshbox' sshA (sea surface height anomalies) (usually the regression of nino3 sstA onto nino3 sshA)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box ('nino3') for SST
    :param sshfile: string
        path_to/filename of the file (NetCDF) of SSH
    :param sshname: string
        name of SSH variable (ssh, sshg, zos) in 'sshfile'
    :param sshareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SSH
    :param sshareaname: string
        name of areacell variable (areacella, areacello) in 'sshareafile'
    :param sshlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SSH
    :param sshlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfile'
    :param sshbox: string
        name of box ('nino3') for SSH
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSshSst_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Sst-Ssh feedback"
    Units = "C/cm"
    Method = "Regression of " + sstbox + " sstA onto " + sshbox + " sshA"
    Method_NL = "The nonlinearity is the regression computed when sshA<0 minus the regression computed when sshA>0"
    Ref = "Using CDAT regression calculation"
    metric = "EnsoFbSshSst"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    ssh, ssh_areacell, keyerror2 = Read_data_mask_area(
        sshfile, sshname, "sea surface height", metric, sshbox, file_area=sshareafile, name_area=sshareaname,
        file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst, ssh, keyerror3 = CheckTime(sst, ssh, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag = {"value": None, "axis": None}
    keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, averages horizontally)
        sst, Method, keyerror1 = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        ssh, _, keyerror2 = PreProcessTS(
            ssh, "", areacell=ssh_areacell, average="horizontal", compute_anom=True, region=sshbox, **kwargs)
        del sst_areacell, ssh_areacell
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "axes2": "(ssh) " + str([ax.id for ax in ssh.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "shape2": "(ssh) " + str(ssh.shape),
                              "time1": "(ssh) " + str(TimeBounds(sst)), "time2": "(ssh) " + str(TimeBounds(ssh))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
            # Change units
            ssh = ssh * 1e2

            # Computes the linear regression for all points, for SSHA >=0 and for SSHA<=0
            val, v_pos, v_neg = LinearRegressionAndNonlinearity(sst, ssh, return_stderr=True, return_intercept=True)

            # Non linearities
            nl1 = v_neg[0] - v_pos[0]
            nl2 = v_neg[1] + v_pos[1]

            # Dive down diagnostic
            dive_down_diag = {"value": None, "axis": None}

            if netcdf is True:
                sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                ssh_map, ssh_map_areacell, keyerror2 = Read_data_mask_area(
                    sshfile, sshname, "sea surface height", metric, "equatorial_pacific", file_area=sshareafile,
                    name_area=sshareaname, file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                # Checks if the same time period is used for both variables
                sst_map, ssh_map, keyerror3 = CheckTime(sst_map, ssh_map, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
                    sst_map, _, keyerror1 = PreProcessTS(
                        sst_map, "", areacell=sst_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    ssh_map, _, keyerror2 = PreProcessTS(
                        ssh_map, "", areacell=ssh_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del ssh_map_areacell, sst_map_areacell
                    keyerror = add_up_errors([keyerror1, keyerror2])
                    if keyerror is None:
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        ssh_map = Regrid(ssh_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                          "axes2": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                                          "shape1": "(sst) " + str(sst_map.shape),
                                          "shape2": "(ssh) " + str(ssh_map.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid", 15, **dict_debug)
                        # Meridional average
                        sst_map, keyerror1 = AverageMeridional(sst_map)
                        ssh_map, keyerror2 = AverageMeridional(ssh_map)
                        keyerror = add_up_errors([keyerror1, keyerror2])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(ssh) " + str(ssh_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Zonal smoothing
                            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
                            ssh_map, _ = Smoothing(ssh_map, "", axis=1, window=31, method="square")
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(ssh) " + str(ssh_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing", 15, **dict_debug)
                            # Change units
                            ssh_map = ssh_map * 1e2
                            # Array year by year
                            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
                            ssh_yby = get_year_by_year(ssh_map, frequency=kwargs["frequency"])
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(ssh) " + str([ax.id for ax in ssh_yby.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(ssh) " + str(ssh_yby.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year", 15, **dict_debug)
                            # Computes the linear regression for all points, for SSHA >=0 and for SSHA<=0
                            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                                sst_map, ssh_map, return_stderr=False, return_intercept=False)
                            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                                sst_yby, ssh_yby, return_stderr=False, return_intercept=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(zonal Thermo Fb) " + str([ax.id for ax in cur_val.getAxisList()]),
                                    "axes2": "(hovtx Thermo Fb) " + str([ax.id for ax in hov_val.getAxisList()]),
                                    "shape1": "(zonal Thermo Fb) " + str(cur_val.shape),
                                    "shape2": "(hovtx Thermo Fb) " + str(hov_val.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionAndNonlinearity", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {
                                "units": "cm", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": sshbox + " sshA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict2 = {
                                "units": "C", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": sstbox + " sstA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict3 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of sstA onto sshA across longitudes"}
                            dict4 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of sstA onto sshA>0 across longitudes"}
                            dict5 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of sstA onto sshA<0 across longitudes"}
                            dict6 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of sstA onto sshA"}
                            dict7 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of sstA onto sshA>0"}
                            dict8 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of sstA onto sshA<0"}
                            dict9 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, var1_time_name="months_" + dataset, var2_time_name="months_" + dataset,
                                var1=ssh, var1_attributes=dict1, var1_name=ovar[0] + dataset, global_attributes=dict9,
                                var2=sst, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                                var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                                var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                                var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                                var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                                var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                                var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
                            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Create output
    metric_output = {
        "name": Name, "value": val[0], "value_error": val[1], "units": Units, "method": Method,
        "method_nonlinearity": Method_NL, "nyears": yearN, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": Ref, "nonlinearity": nl1, "nonlinearity_error": nl2,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstLhf(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, lhffile, lhfname,
                 lhfareafile, lhfareaname, lhflandmaskfile, lhflandmaskname, lhfbox, dataset="", debug=False,
                 netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstLhf() function computes the regression of 'lhfbox' lhfA (latent heat flux anomalies) onto 'sstbox' sstA
    (sea surface temperature anomalies) (usually the regression of nino3 lhfA onto nino3 sstA)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param lhffile: string
        path_to/filename of the file (NetCDF) of LHF
    :param lhfname: string
        name of LHF variable (lhf, hfls) in 'lhffile'
    :param lhfareafile: string
        path_to/filename of the file (NetCDF) of the areacell for LHF
    :param lhfareaname: string
        name of areacell variable (areacella, areacello) in 'lhfareafile'
    :param lhflandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for LHF
    :param lhflandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lhflandmaskfile'
    :param lhfbox: string
        name of box (nino3') for LHF
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstLhf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Lhf-Sst feedback (alpha_lh)"
    Units = "W/m2/C"
    Method = "Regression of " + lhfbox + " lhfA onto " + sstbox + " sstA"
    Method_NL = "The nonlinearity is the regression computed when sstA<0 minus the regression computed when sstA>0"
    Ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstLhf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    lhf, lhf_areacell, keyerror2 = Read_data_mask_area(
        lhffile, lhfname, "heat flux", metric, lhfbox, file_area=lhfareafile, name_area=lhfareaname,
        file_mask=lhflandmaskfile, name_mask=lhflandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst, lhf, keyerror3 = CheckTime(sst, lhf, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag = {"value": None, "axis": None}
    keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, averages horizontally)
        sst, Method, keyerror1 = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        lhf, _, keyerror2 = PreProcessTS(
            lhf, "", areacell=lhf_areacell, average="horizontal", compute_anom=True, region=lhfbox, **kwargs)
        del sst_areacell, lhf_areacell
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "axes2": "(lhf) " + str([ax.id for ax in lhf.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "shape2": "(lhf) " + str(lhf.shape),
                              "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(lhf) " + str(TimeBounds(lhf))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            val, v_pos, v_neg = LinearRegressionAndNonlinearity(lhf, sst, return_stderr=True, return_intercept=True)

            # Non linearities
            nl1 = v_neg[0] - v_pos[0]
            nl2 = v_neg[1] + v_pos[1]

            # Dive down diagnostic
            dive_down_diag = {"value": None, "axis": None}

            if netcdf is True:
                sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                lhf_map, lhf_map_areacell, keyerror2 = Read_data_mask_area(
                    lhffile, lhfname, "heat flux", metric, "equatorial_pacific", file_area=lhfareafile,
                    name_area=lhfareaname, file_mask=lhflandmaskfile, name_mask=lhflandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                # Checks if the same time period is used for both variables
                sst_map, lhf_map, keyerror3 = CheckTime(sst_map, lhf_map, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
                    sst_map, _, keyerror1 = PreProcessTS(
                        sst_map, "", areacell=sst_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    lhf_map, _, keyerror2 = PreProcessTS(
                        lhf_map, "", areacell=lhf_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del lhf_map_areacell, sst_map_areacell
                    keyerror = add_up_errors([keyerror1, keyerror2])
                    if keyerror is None:
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        lhf_map = Regrid(lhf_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                          "axes2": "(lhf) " + str([ax.id for ax in lhf_map.getAxisList()]),
                                          "shape1": "(sst) " + str(sst_map.shape),
                                          "shape2": "(lhf) " + str(lhf_map.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid", 15, **dict_debug)
                        # Meridional average
                        sst_map, keyerror1 = AverageMeridional(sst_map)
                        lhf_map, keyerror2 = AverageMeridional(lhf_map)
                        keyerror = add_up_errors([keyerror1, keyerror2])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(lhf) " + str([ax.id for ax in lhf_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(lhf) " + str(lhf_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Zonal smoothing
                            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
                            lhf_map, _ = Smoothing(lhf_map, "", axis=1, window=31, method="square")
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(lhf) " + str([ax.id for ax in lhf_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(lhf) " + str(lhf_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing", 15, **dict_debug)
                            # Array year by year
                            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
                            lhf_yby = get_year_by_year(lhf_map, frequency=kwargs["frequency"])
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(lhf) " + str([ax.id for ax in lhf_yby.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(lhf) " + str(lhf_yby.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year", 15, **dict_debug)
                            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
                            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                                lhf_map, sst_map, return_stderr=False, return_intercept=False)
                            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                                lhf_yby, sst_yby, return_stderr=False, return_intercept=False)
                            if debug is True:
                                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionAndNonlinearity", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {
                                "units": "C", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": sstbox + " sstA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict2 = {
                                "units": "W/m2", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": lhfbox + " lhfA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict3 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of lhfA onto sstA across longitudes"}
                            dict4 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of lhfA onto sstA>0 across longitudes"}
                            dict5 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of lhfA onto sstA<0 across longitudes"}
                            dict6 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of lhfA onto sstA"}
                            dict7 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of lhfA onto sstA>0"}
                            dict8 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of lhfA onto sstA<0"}
                            dict9 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, var1_time_name="months_" + dataset, var2_time_name="months_" + dataset,
                                var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, global_attributes=dict9,
                                var2=lhf, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                                var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                                var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                                var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                                var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                                var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                                var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
                            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Create output
    metric_output = {
        "name": Name, "value": val[0], "value_error": val[1], "units": Units, "method": Method,
        "method_nonlinearity": Method_NL, "nyears": yearN, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": Ref, "nonlinearity": nl1, "nonlinearity_error": nl2,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstLwr(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, lwrfile, lwrname,
                 lwrareafile, lwrareaname, lwrlandmaskfile, lwrlandmaskname, lwrbox, dataset="", debug=False,
                 netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstLwr() function computes the regression of 'lwrbox' lwrA (net surface longwave radiation anomalies) onto
    'sstbox' sstA (sea surface temperature anomalies) (usually the regression of nino3 lwrA onto nino3 sstA)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param lwrfile: string
        path_to/filename of the file (NetCDF) of LWR
    :param lwrname: string
        name of LWR variable (lwr, rlds - rlus) in 'lwrfile'
    :param lwrareafile: string
        path_to/filename of the file (NetCDF) of the areacell for LWR
    :param lwrareaname: string
        name of areacell variable (areacella, areacello) in 'lwrareafile'
    :param lwrlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for LWR
    :param lwrlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'lwrlandmaskfile'
    :param lwrbox: string
        name of box (nino3') for LWR
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstLwr_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Lwr-Sst feedback (alpha_lh)"
    Units = "W/m2/C"
    Method = "Regression of " + lwrbox + " lwrA onto " + sstbox + " sstA"
    Method_NL = "The nonlinearity is the regression computed when sstA<0 minus the regression computed when sstA>0"
    Ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstLwr"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    lwr, lwr_areacell, keyerror2 = Read_data_mask_area_multifile(
        lwrfile, lwrname, "heat flux", "lwr", metric, lwrbox, file_area=lwrareafile, name_area=lwrareaname,
        file_mask=lwrlandmaskfile, name_mask=lwrlandmaskname, maskland=True, maskocean=False, debug=debug,
        interpreter="project_interpreter_var2", **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst, lwr, keyerror3 = CheckTime(sst, lwr, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag = {"value": None, "axis": None}
    keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, averages horizontally)
        sst, Method, keyerror1 = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        lwr, _, keyerror2 = PreProcessTS(
            lwr, "", areacell=lwr_areacell, average="horizontal", compute_anom=True, region=lwrbox, **kwargs)
        del sst_areacell, lwr_areacell
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "axes2": "(lwr) " + str([ax.id for ax in lwr.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "shape2": "(lwr) " + str(lwr.shape),
                              "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(lwr) " + str(TimeBounds(lwr))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            val, v_pos, v_neg = LinearRegressionAndNonlinearity(lwr, sst, return_stderr=True, return_intercept=True)

            # Non linearities
            nl1 = v_neg[0] - v_pos[0]
            nl2 = v_neg[1] + v_pos[1]

            # Dive down diagnostic
            dive_down_diag = {"value": None, "axis": None}
            if netcdf is True:
                sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                lwr_map, lwr_map_areacell, keyerror2 = Read_data_mask_area_multifile(
                    lwrfile, lwrname, "heat flux", "lwr", metric, "equatorial_pacific", file_area=lwrareafile,
                    name_area=lwrareaname, file_mask=lwrlandmaskfile, name_mask=lwrlandmaskname, maskland=True,
                    maskocean=False, debug=debug, interpreter="project_interpreter_var2", **kwargs)
                # Checks if the same time period is used for both variables
                sst_map, lwr_map, keyerror3 = CheckTime(sst_map, lwr_map, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
                    sst_map, _, keyerror1 = PreProcessTS(
                        sst_map, "", areacell=sst_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    lwr_map, _, keyerror2 = PreProcessTS(
                        lwr_map, "", areacell=lwr_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del lwr_map_areacell, sst_map_areacell
                    keyerror = add_up_errors([keyerror1, keyerror2])
                    if keyerror is None:
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        lwr_map = Regrid(lwr_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                          "axes2": "(lwr) " + str([ax.id for ax in lwr_map.getAxisList()]),
                                          "shape1": "(sst) " + str(sst_map.shape),
                                          "shape2": "(lwr) " + str(lwr_map.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid", 15, **dict_debug)
                        # Meridional average
                        sst_map, keyerror1 = AverageMeridional(sst_map)
                        lwr_map, keyerror2 = AverageMeridional(lwr_map)
                        keyerror = add_up_errors([keyerror1, keyerror2])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(lwr) " + str([ax.id for ax in lwr_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(lwr) " + str(lwr_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Zonal smoothing
                            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
                            lwr_map, _ = Smoothing(lwr_map, "", axis=1, window=31, method="square")
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(lwr) " + str([ax.id for ax in lwr_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(lwr) " + str(lwr_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing", 15, **dict_debug)
                            # Array year by year
                            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
                            lwr_yby = get_year_by_year(lwr_map, frequency=kwargs["frequency"])
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(lwr) " + str([ax.id for ax in lwr_yby.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(lwr) " + str(lwr_yby.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year", 15, **dict_debug)
                            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
                            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                                lwr_map, sst_map, return_stderr=False, return_intercept=False)
                            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                                lwr_yby, sst_yby, return_stderr=False, return_intercept=False)
                            if debug is True:
                                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionAndNonlinearity", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {
                                "units": "C", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": sstbox + " sstA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict2 = {
                                "units": "W/m2", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": lwrbox + " lwrA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict3 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of lwrA onto sstA across longitudes"}
                            dict4 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of lwrA onto sstA>0 across longitudes"}
                            dict5 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of lwrA onto sstA<0 across longitudes"}
                            dict6 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of lwrA onto sstA"}
                            dict7 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of lwrA onto sstA>0"}
                            dict8 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of lwrA onto sstA<0"}
                            dict9 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, var1_time_name="months_" + dataset, var2_time_name="months_" + dataset,
                                var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, global_attributes=dict9,
                                var2=lwr, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                                var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                                var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                                var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                                var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                                var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                                var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
                            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Create output
    metric_output = {
        "name": Name, "value": val[0], "value_error": val[1], "units": Units, "method": Method,
        "method_nonlinearity": Method_NL, "nyears": yearN, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": Ref, "nonlinearity": nl1, "nonlinearity_error": nl2,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstShf(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, shffile, shfname,
                 shfareafile, shfareaname, shflandmaskfile, shflandmaskname, shfbox, dataset="", debug=False,
                 netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstShf() function computes the regression of 'shfbox' shfA (sensible heat flux anomalies) onto 'sstbox'
    sstA (sea surface temperature anomalies) (usually the regression of nino3 shfA onto nino3 sstA)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param shffile: string
        path_to/filename of the file (NetCDF) of SHF
    :param shfname: string
        name of SHF variable (shf, hfss) in 'shffile'
    :param shfareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SHF
    :param shfareaname: string
        name of areacell variable (areacella, areacello) in 'shfareafile'
    :param shflandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SHF
    :param shflandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'shflandmaskfile'
    :param shfbox: string
        name of box (nino3') for SHF
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstShf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Shf-Sst feedback (alpha_lh)"
    Units = "W/m2/C"
    Method = "Regression of " + shfbox + " shfA onto " + sstbox + " sstA"
    Method_NL = "The nonlinearity is the regression computed when sstA<0 minus the regression computed when sstA>0"
    Ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstShf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    shf, shf_areacell, keyerror2 = Read_data_mask_area(
        shffile, shfname, "heat flux", metric, shfbox, file_area=shfareafile, name_area=shfareaname,
        file_mask=shflandmaskfile, name_mask=shflandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst, shf, keyerror3 = CheckTime(sst, shf, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag = {"value": None, "axis": None}
    keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, averages horizontally)
        sst, Method, keyerror1 = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        shf, _, keyerror2 = PreProcessTS(
            shf, "", areacell=shf_areacell, average="horizontal", compute_anom=True, region=shfbox, **kwargs)
        del sst_areacell, shf_areacell
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "axes2": "(shf) " + str([ax.id for ax in shf.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "shape2": "(shf) " + str(shf.shape),
                              "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(shf) " + str(TimeBounds(shf))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            val, v_pos, v_neg = LinearRegressionAndNonlinearity(shf, sst, return_stderr=True, return_intercept=True)

            # Non linearities
            nl1 = v_neg[0] - v_pos[0]
            nl2 = v_neg[1] + v_pos[1]

            # Dive down diagnostic
            dive_down_diag = {"value": None, "axis": None}

            if netcdf is True:
                sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                shf_map, shf_map_areacell, keyerror2 = Read_data_mask_area(
                    shffile, shfname, "heat flux", metric, "equatorial_pacific", file_area=shfareafile,
                    name_area=shfareaname, file_mask=shflandmaskfile, name_mask=shflandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                # Checks if the same time period is used for both variables
                sst_map, shf_map, keyerror3 = CheckTime(sst_map, shf_map, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
                    sst_map, _, keyerror1 = PreProcessTS(
                        sst_map, "", areacell=sst_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    shf_map, _, keyerror2 = PreProcessTS(
                        shf_map, "", areacell=shf_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del shf_map_areacell, sst_map_areacell
                    keyerror = add_up_errors([keyerror1, keyerror2])
                    if keyerror is None:
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        shf_map = Regrid(shf_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                          "axes2": "(shf) " + str([ax.id for ax in shf_map.getAxisList()]),
                                          "shape1": "(sst) " + str(sst_map.shape),
                                          "shape2": "(shf) " + str(shf_map.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid", 15, **dict_debug)
                        # Meridional average
                        sst_map, keyerror1 = AverageMeridional(sst_map)
                        shf_map, keyerror2 = AverageMeridional(shf_map)
                        keyerror = add_up_errors([keyerror1, keyerror2])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(shf) " + str([ax.id for ax in shf_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(shf) " + str(shf_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Zonal smoothing
                            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
                            shf_map, _ = Smoothing(shf_map, "", axis=1, window=31, method="square")
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(shf) " + str([ax.id for ax in shf_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(shf) " + str(shf_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing", 15, **dict_debug)
                            # Array year by year
                            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
                            shf_yby = get_year_by_year(shf_map, frequency=kwargs["frequency"])
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(shf) " + str([ax.id for ax in shf_yby.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(shf) " + str(shf_yby.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year", 15, **dict_debug)
                            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
                            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                                shf_map, sst_map, return_stderr=False, return_intercept=False)
                            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                                shf_yby, sst_yby, return_stderr=False, return_intercept=False)
                            if debug is True:
                                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionAndNonlinearity", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {
                                "units": "C", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": sstbox + " sstA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict2 = {
                                "units": "W/m2", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": shfbox + " shfA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict3 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of shfA onto sstA across longitudes"}
                            dict4 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of shfA onto sstA>0 across longitudes"}
                            dict5 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of shfA onto sstA<0 across longitudes"}
                            dict6 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of shfA onto sstA"}
                            dict7 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of shfA onto sstA>0"}
                            dict8 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of shfA onto sstA<0"}
                            dict9 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, var1_time_name="months_" + dataset, var2_time_name="months_" + dataset,
                                var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, global_attributes=dict9,
                                var2=shf, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                                var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                                var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                                var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                                var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                                var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                                var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
                            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Create output
    metric_output = {
        "name": Name, "value": val[0], "value_error": val[1], "units": Units, "method": Method,
        "method_nonlinearity": Method_NL, "nyears": yearN, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": Ref, "nonlinearity": nl1, "nonlinearity_error": nl2,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstSwr(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, swrfile, swrname,
                 swrareafile, swrareaname, swrlandmaskfile, swrlandmaskname, swrbox, dataset="", debug=False,
                 netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstSwr() function computes the regression of 'swrbox' swrA (net surface shortwave radiation anomalies)
    onto 'sstbox' sstA (sea surface temperature anomalies) (usually the regression of nino3 swrA onto nino3 sstA)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param swrfile: string
        path_to/filename of the file (NetCDF) of SWR
    :param swrname: string
        name of SWR variable (swr, rsds - rsus) in 'swrfile'
    :param swrareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SWR
    :param swrareaname: string
        name of areacell variable (areacella, areacello) in 'swrareafile'
    :param swrlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SWR
    :param swrlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'swrlandmaskfile'
    :param swrbox: string
        name of box (nino3') for SWR
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstSwr_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Swr-Sst feedback (alpha_lh)"
    Units = "W/m2/C"
    Method = "Regression of " + swrbox + " swrA onto " + sstbox + " sstA"
    Method_NL = "The nonlinearity is the regression computed when sstA<0 minus the regression computed when sstA>0"
    Ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstSwr"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    swr, swr_areacell, keyerror2 = Read_data_mask_area_multifile(
        swrfile, swrname, "heat flux", "swr", metric, swrbox, file_area=swrareafile, name_area=swrareaname,
        file_mask=swrlandmaskfile, name_mask=swrlandmaskname, maskland=True, maskocean=False, debug=debug,
        interpreter="project_interpreter_var2", **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst, swr, keyerror3 = CheckTime(sst, swr, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag = {"value": None, "axis": None}
    keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, averages horizontally)
        sst, Method, keyerror1 = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        swr, _, keyerror2 = PreProcessTS(
            swr, "", areacell=swr_areacell, average="horizontal", compute_anom=True, region=swrbox, **kwargs)
        del sst_areacell, swr_areacell
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "axes2": "(swr) " + str([ax.id for ax in swr.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "shape2": "(swr) " + str(swr.shape),
                              "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(swr) " + str(TimeBounds(swr))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            val, v_pos, v_neg = LinearRegressionAndNonlinearity(swr, sst, return_stderr=True, return_intercept=True)

            # Non linearities
            nl1 = v_neg[0] - v_pos[0]
            nl2 = v_neg[1] + v_pos[1]

            # Dive down diagnostic
            dive_down_diag = {"value": None, "axis": None}

            if netcdf is True:
                sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                swr_map, swr_map_areacell, keyerror2 = Read_data_mask_area_multifile(
                    swrfile, swrname, "heat flux", "swr", metric, "equatorial_pacific", file_area=swrareafile,
                    name_area=swrareaname, file_mask=swrlandmaskfile, name_mask=swrlandmaskname, maskland=True,
                    maskocean=False, debug=debug, interpreter="project_interpreter_var2", **kwargs)
                # Checks if the same time period is used for both variables
                sst_map, swr_map, keyerror3 = CheckTime(sst_map, swr_map, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS,...)
                    sst_map, _, keyerror1 = PreProcessTS(
                        sst_map, "", areacell=sst_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    swr_map, _, keyerror2 = PreProcessTS(
                        swr_map, "", areacell=swr_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del swr_map_areacell, sst_map_areacell
                    keyerror = add_up_errors([keyerror1, keyerror2])
                    if keyerror is None:
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        swr_map = Regrid(swr_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                          "axes2": "(swr) " + str([ax.id for ax in swr_map.getAxisList()]),
                                          "shape1": "(sst) " + str(sst_map.shape),
                                          "shape2": "(swr) " + str(swr_map.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid", 15, **dict_debug)
                        # Meridional average
                        sst_map, keyerror1 = AverageMeridional(sst_map)
                        swr_map, keyerror2 = AverageMeridional(swr_map)
                        keyerror = add_up_errors([keyerror1, keyerror2])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(swr) " + str([ax.id for ax in swr_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(swr) " + str(swr_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Zonal smoothing
                            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
                            swr_map, _ = Smoothing(swr_map, "", axis=1, window=31, method="square")
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(swr) " + str([ax.id for ax in swr_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(swr) " + str(swr_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing", 15, **dict_debug)
                            # Array year by year
                            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
                            swr_yby = get_year_by_year(swr_map, frequency=kwargs["frequency"])
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(swr) " + str([ax.id for ax in swr_yby.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(swr) " + str(swr_yby.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year", 15, **dict_debug)
                            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
                            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                                swr_map, sst_map, return_stderr=False, return_intercept=False)
                            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                                swr_yby, sst_yby, return_stderr=False, return_intercept=False)
                            if debug is True:
                                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionAndNonlinearity", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {
                                "units": "C", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": sstbox + " sstA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict2 = {
                                "units": "W/m2", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": swrbox + " swrA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict3 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of swrA onto sstA across longitudes"}
                            dict4 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of swrA onto sstA>0 across longitudes"}
                            dict5 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of swrA onto sstA<0 across longitudes"}
                            dict6 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of swrA onto sstA"}
                            dict7 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of swrA onto sstA>0"}
                            dict8 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of swrA onto sstA<0"}
                            dict9 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, var1_time_name="months_" + dataset, var2_time_name="months_" + dataset,
                                var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, global_attributes=dict9,
                                var2=swr, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                                var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                                var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                                var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                                var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                                var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                                var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
                            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Create output
    metric_output = {
        "name": Name, "value": val[0], "value_error": val[1], "units": Units, "method": Method,
        "method_nonlinearity": Method_NL, "nyears": yearN, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": Ref, "nonlinearity": nl1, "nonlinearity_error": nl2,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstTaux(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, tauxfile,
                  tauxname, tauxareafile, tauxareaname, tauxlandmaskfile, tauxlandmaskname, tauxbox, dataset="",
                  debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstTaux() function computes the regression of 'tauxbox' tauxA (surface downward zonal stress anomalies)
    onto 'sstbox' sstA (sea surface temperature anomalies) (usually the regression of nino4 tauxA onto nino3 sstA)

    Author:	Eric Guilyardi : Eric.Guilyardi@locean-ipsl.upmc.fr
    Co-author: Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon Jan  9 11:05:18 CET 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box ('nino3') for SST
    :param tauxfile: string
        path_to/filename of the file (NetCDF) of TAUX
    :param tauxname: string
        name of TAUX variable (taux, tauu) in 'tauxfile'
    :param tauxareafile: string
        path_to/filename of the file (NetCDF) of the areacell for TAUX
    :param tauxareaname: string
        name of areacell variable (areacella, areacello) in 'tauxareafile'
    :param tauxlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for TAUX
    :param tauxlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfile'
    :param tauxbox: string
        name of box ('nino4') for TAUX
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstTaux_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Taux-Sst feedback (mu)"
    Units = "1e-3 N/m2/C"
    Method = "Regression of " + tauxbox + " tauxA onto " + sstbox + " sstA"
    Method_NL = "The nonlinearity is the regression computed when sstA<0 minus the regression computed when sstA>0"
    Ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstTaux"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    taux, taux_areacell, keyerror2 = Read_data_mask_area(
        tauxfile, tauxname, "wind stress", metric, tauxbox, file_area=tauxareafile, name_area=tauxareaname,
        file_mask=tauxlandmaskfile, name_mask=tauxlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst, taux, keyerror3 = CheckTime(sst, taux, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag = {"value": None, "axis": None}
    keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, averages horizontally)
        sst, Method, keyerror1 = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        taux, _, keyerror2 = PreProcessTS(
            taux, "", areacell=taux_areacell, average="horizontal", compute_anom=True, region=tauxbox, **kwargs)
        del sst_areacell, taux_areacell
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "axes2": "(taux) " + str([ax.id for ax in taux.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "shape2": "(taux) " + str(taux.shape),
                              "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(taux) " + str(TimeBounds(taux))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
            # Change units
            taux = taux * 1e3

            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            val, v_pos, v_neg = LinearRegressionAndNonlinearity(taux, sst, return_stderr=True, return_intercept=True)

            # Non linearities
            nl1 = v_neg[0] - v_pos[0]
            nl2 = v_neg[1] + v_pos[1]

            # Dive down diagnostic
            dive_down_diag = {"value": None, "axis": None}

            if netcdf is True:
                taux_map, taux_map_areacell, keyerror1 = Read_data_mask_area(
                    tauxfile, tauxname, "wind stress", metric, "equatorial_pacific", file_area=tauxareafile,
                    name_area=tauxareaname, file_mask=tauxlandmaskfile, name_mask=tauxlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                # Checks if the same time period is used for both variables
                sst, taux_map, keyerror2 = CheckTime(sst, taux_map, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
                    taux_map, _, keyerror = PreProcessTS(
                        taux_map, "", areacell=taux_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del taux_map_areacell
                    if keyerror is None:
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        taux_map = Regrid(taux_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(taux) " + str([ax.id for ax in taux_map.getAxisList()]),
                                          "shape1": "(taux) " + str(taux_map.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid", 15, **dict_debug)
                        # Meridional average
                        taux_map, keyerror = AverageMeridional(taux_map)
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(taux) " + str([ax.id for ax in taux_map.getAxisList()]),
                                              "shape2": "(taux) " + str(taux_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Zonal smoothing
                            taux_map, _ = Smoothing(taux_map, "", axis=1, window=31, method="square")
                            if debug is True:
                                dict_debug = {"axes1": "(taux) " + str([ax.id for ax in taux_map.getAxisList()]),
                                              "shape1": "(taux) " + str(taux_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing", 15, **dict_debug)
                            # Change units
                            taux_map = taux_map * 1e3
                            # Sst to map
                            sst_map = TsToMap(sst, taux_map)
                            # Array year by year
                            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
                            taux_yby = get_year_by_year(taux_map, frequency=kwargs["frequency"])
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(taux) " + str([ax.id for ax in taux_yby.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(taux) " + str(taux_yby.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year", 15, **dict_debug)
                            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
                            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                                taux_map, sst_map, return_stderr=False, return_intercept=False)
                            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                                taux_yby, sst_yby, return_stderr=False, return_intercept=False)
                            if debug is True:
                                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionAndNonlinearity", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {
                                "units": "C", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": sstbox + " sstA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict2 = {
                                "units": "1e-3 N/m2", "number_of_years_used": yearN,
                                "time_period": str(actualtimebounds), "description": tauxbox + " tauxA",
                                "diagnostic_value": val[0], "diagnostic_value_error": val[1], "slope": val[0],
                                "intercept": val[2], "slope_neg": v_neg[0], "intercept_neg": v_neg[2],
                                "slope_pos": v_pos[0], "intercept_pos": v_pos[2]}
                            dict3 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of " + tauxbox + " tauxA onto sstA across longitudes"}
                            dict4 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of " + tauxbox + " tauxA onto sstA>0 across longitudes"}
                            dict5 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of " + tauxbox + " tauxA onto sstA<0 across longitudes"}
                            dict6 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of " + tauxbox +
                                               " tauxA onto sstA"}
                            dict7 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of " + tauxbox +
                                               " tauxA onto sstA>0"}
                            dict8 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of " + tauxbox +
                                               " tauxA onto sstA<0"}
                            dict9 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, var1_time_name="months_" + dataset, var2_time_name="months_" + dataset,
                                var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, global_attributes=dict9,
                                var2=taux, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                                var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                                var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                                var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                                var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                                var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                                var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
                            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Create output
    metric_output = {
        "name": Name, "value": val[0], "value_error": val[1], "units": Units, "method": Method,
        "method_nonlinearity": Method_NL, "nyears": yearN, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": Ref, "nonlinearity": nl1, "nonlinearity_error": nl2,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbSstThf(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, thffile, thfname,
                 thfareafile, thfareaname, thflandmaskfile, thflandmaskname, thfbox, dataset="", debug=False,
                 netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbSstThf() function computes the regression of 'thfbox' thfA (total heat flux anomalies) onto 'sstbox' sstA
    (sea surface temperature anomalies) (usually the regression of nino3 thfA onto nino3 sstA)
    The total heat flux is the sum of four term:
         - net surface shortwave radiation,
         - net surface longwave radiation,
         - latent heat flux,
         - sensible heat flux

    The total heat flux is not always available is models or observations.
    Either the user computes it and sends the filename and the varname or he feeds into thffile and thfname of this
    function a list() of the four needed files and variable names (CMIP: rsds-rsus, rlds-rlus, hfls, hfss)

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr
    Co-author:

    Created on Thu Oct  5 2017

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: string
        name of box (nino3') for SST
    :param thffile: string
        path_to/filename of the file (NetCDF) of THF
    :param thfname: string
        name of THF variable (thf, netflux, thflx, thf + lwr + lhf + shf) (may be a list of variables) in 'thffile'
    :param thfareafile: string
        path_to/filename of the file (NetCDF) of the areacell for THF
    :param thfareaname: string
        name of areacell variable (areacella, areacello) in 'thfareafile'
    :param thflandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for THF
    :param thflandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'thflandmaskfile'
    :param thfbox: string
        name of box (nino3') for THF
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbSstThf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Thf-Sst feedback (alpha)"
    Units = "W/m2/C"
    Method = "Regression of " + thfbox + " thfA onto " + sstbox + " sstA"
    Method_NL = "The nonlinearity is the regression computed when sstA<0 minus the regression computed when sstA>0"
    Ref = "Using CDAT regression calculation"
    metric = "EnsoFbSstThf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst, sst_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    thf, thf_areacell, keyerror2 = Read_data_mask_area_multifile(
        thffile, thfname, "heat flux", "thf", metric, thfbox, file_area=thfareafile, name_area=thfareaname,
        file_mask=thflandmaskfile, name_mask=thflandmaskname, maskland=True, maskocean=False, debug=debug,
        interpreter="project_interpreter_var2", **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst, thf, keyerror3 = CheckTime(sst, thf, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag = {"value": None, "axis": None}
    keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, averages horizontally)
        sst, Method, keyerror1 = PreProcessTS(
            sst, Method, areacell=sst_areacell, average="horizontal", compute_anom=True, region=sstbox, **kwargs)
        thf, _, keyerror2 = PreProcessTS(
            thf, "", areacell=thf_areacell, average="horizontal", compute_anom=True, region=thfbox, **kwargs)
        del sst_areacell, thf_areacell
        keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst.getAxisList()]),
                              "axes2": "(thf) " + str([ax.id for ax in thf.getAxisList()]),
                              "shape1": "(sst) " + str(sst.shape), "shape2": "(thf) " + str(thf.shape),
                              "time1": "(sst) " + str(TimeBounds(sst)), "time2": "(thf) " + str(TimeBounds(thf))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
            val, v_pos, v_neg = LinearRegressionAndNonlinearity(thf, sst, return_stderr=True, return_intercept=True)

            # Non linearities
            nl1 = v_neg[0] - v_pos[0]
            nl2 = v_neg[1] + v_pos[1]

            # Dive down diagnostic
            dive_down_diag = {"value": None, "axis": None}

            if netcdf is True:
                sst_map, sst_map_areacell, keyerror1 = Read_data_mask_area(
                    sstfile, sstname, "temperature", metric, "equatorial_pacific", file_area=sstareafile,
                    name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                thf_map, thf_map_areacell, keyerror2 = Read_data_mask_area_multifile(
                    thffile, thfname, "heat flux", "thf", metric, "equatorial_pacific", file_area=thfareafile,
                    name_area=thfareaname, file_mask=thflandmaskfile, name_mask=thflandmaskname, maskland=True,
                    maskocean=False, debug=debug, interpreter="project_interpreter_var2", **kwargs)
                # Checks if the same time period is used for both variables
                sst_map, thf_map, keyerror3 = CheckTime(sst_map, thf_map, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
                if keyerror is None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
                    sst_map, _, keyerror1 = PreProcessTS(
                        sst_map, "", areacell=sst_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    thf_map, _, keyerror2 = PreProcessTS(
                        thf_map, "", areacell=thf_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del thf_map_areacell, sst_map_areacell
                    keyerror = add_up_errors([keyerror1, keyerror2])
                    if keyerror is None:
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        sst_map = Regrid(sst_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        thf_map = Regrid(thf_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                          "axes2": "(thf) " + str([ax.id for ax in thf_map.getAxisList()]),
                                          "shape1": "(sst) " + str(sst_map.shape),
                                          "shape2": "(thf) " + str(thf_map.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid", 15, **dict_debug)
                        # Meridional average
                        sst_map, keyerror1 = AverageMeridional(sst_map)
                        thf_map, keyerror2 = AverageMeridional(thf_map)
                        keyerror = add_up_errors([keyerror1, keyerror2])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(thf) " + str([ax.id for ax in thf_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(thf) " + str(thf_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Zonal smoothing
                            sst_map, _ = Smoothing(sst_map, "", axis=1, window=31, method="square")
                            thf_map, _ = Smoothing(thf_map, "", axis=1, window=31, method="square")
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(thf) " + str([ax.id for ax in thf_map.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(thf) " + str(thf_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing", 15, **dict_debug)
                            # Array year by year
                            sst_yby = get_year_by_year(sst_map, frequency=kwargs["frequency"])
                            thf_yby = get_year_by_year(thf_map, frequency=kwargs["frequency"])
                            if debug is True:
                                dict_debug = {"axes1": "(sst) " + str([ax.id for ax in sst_map.getAxisList()]),
                                              "axes2": "(thf) " + str([ax.id for ax in thf_yby.getAxisList()]),
                                              "shape1": "(sst) " + str(sst_map.shape),
                                              "shape2": "(thf) " + str(thf_yby.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year", 15, **dict_debug)
                            # Computes the linear regression for all points, for SSTA >=0 and for SSTA<=0
                            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                                thf_map, sst_map, return_stderr=False, return_intercept=False)
                            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                                thf_yby, sst_yby, return_stderr=False, return_intercept=False)
                            if debug is True:
                                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionAndNonlinearity", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {
                                "units": "C", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": sstbox + " sstA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict2 = {
                                "units": "W/m2", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": thfbox + " nhfA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict3 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of nhfA onto sstA across longitudes"}
                            dict4 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of nhfA onto sstA>0 across longitudes"}
                            dict5 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of nhfA onto sstA<0 across longitudes"}
                            dict6 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of nhfA onto sstA"}
                            dict7 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of nhfA onto sstA>0"}
                            dict8 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of nhfA onto sstA<0"}
                            dict9 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, var1_time_name="months_" + dataset, var2_time_name="months_" + dataset,
                                var1=sst, var1_attributes=dict1, var1_name=ovar[0] + dataset, global_attributes=dict9,
                                var2=thf, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                                var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                                var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                                var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                                var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                                var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                                var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
                            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Create output
    metric_output = {
        "name": Name, "value": val[0], "value_error": val[1], "units": Units, "method": Method,
        "method_nonlinearity": Method_NL, "nyears": yearN, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": Ref, "nonlinearity": nl1, "nonlinearity_error": nl2,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoFbTauxSsh(tauxfile, tauxname, tauxareafile, tauxareaname, tauxlandmaskfile, tauxlandmaskname, tauxbox,
                  sshfile, sshname, sshareafile, sshareaname, sshlandmaskfile, sshlandmaskname, sshbox, dataset="",
                  debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoFbTauxSsh() function computes the regression of 'sshbox' sshA (sea surface height anomalies) onto 'tauxbox'
    tauxA (surface downward zonal stress anomalies) (usually the regression of nino3 sshA onto nino4 tauxA)

    Author:	Eric Guilyardi : Eric.Guilyardi@locean-ipsl.upmc.fr
    Co-author: Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon Jan  9 11:05:18 CET 2017

    Inputs:
    ------
    :param tauxfile: string
        path_to/filename of the file (NetCDF) of TAUX
    :param tauxname: string
        name of TAUX variable (taux, tauu) in 'tauxfile'
    :param tauxareafile: string
        path_to/filename of the file (NetCDF) of the areacell for TAUX
    :param tauxareaname: string
        name of areacell variable (areacella, areacello) in 'tauxareafile'
    :param tauxlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for TAUX
    :param tauxlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfile'
    :param tauxbox: string
        name of box ('nino4') for TAUX
    :param sshfile: string
        path_to/filename of the file (NetCDF) of SSH
    :param sshname: string
        name of SSH variable (ssh, sshg, zos) in 'sshfile'
    :param sshareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SSH
    :param sshareaname: string
        name of areacell variable (areacella, areacello) in 'sshareafile'
    :param sshlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SSH
    :param sshlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfile'
    :param sshbox: string
        name of box ('nino3') for SSH
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoFbTauxSsh_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, nonlinearity,
        nonlinearity_error

    Method:
    -------
        uses tools from uvcdat library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Ssh-Taux feedback"
    Units = "1e3 cm/N/m2"
    Method = "Regression of " + sshbox + " sshA onto " + tauxbox + " tauxA"
    Method_NL = "The nonlinearity is the regression computed when tauxA<0 minus the regression computed when tauxA>0"
    Ref = "Using CDAT regression calculation"
    metric = "EnsoFbTauxSsh"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    ssh, ssh_areacell, keyerror1 = Read_data_mask_area(
        sshfile, sshname, "sea surface height", metric, sshbox, file_area=sshareafile, name_area=sshareaname,
        file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    taux, taux_areacell, keyerror2 = Read_data_mask_area(
        tauxfile, tauxname, "wind stress", metric, tauxbox, file_area=tauxareafile, name_area=tauxareaname,
        file_mask=tauxlandmaskfile, name_mask=tauxlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    ssh, taux, keyerror3 = CheckTime(ssh, taux, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(ssh.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(ssh)

    val, v_pos, v_neg, nl1, nl2 = [None, None], [None, None], [None, None], None, None
    dive_down_diag = {"value": None, "axis": None}
    keyerror = add_up_errors([keyerror1, keyerror2, keyerror3])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, averages horizontally)
        ssh, Method, keyerror1 = PreProcessTS(
            ssh, Method, areacell=ssh_areacell, average="horizontal", compute_anom=True, region=sshbox, **kwargs)
        taux, _, keyerror3 = PreProcessTS(
            taux, "", areacell=taux_areacell, average="horizontal", compute_anom=True, region=tauxbox, **kwargs)
        del ssh_areacell, taux_areacell
        keyerror = add_up_errors([keyerror1, keyerror2])
        if keyerror is not None:
            if debug is True:
                dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh.getAxisList()]),
                              "axes2": "(taux) " + str([ax.id for ax in taux.getAxisList()]),
                              "shape1": "(ssh) " + str(ssh.shape), "shape2": "(taux) " + str(taux.shape),
                              "time1": "(ssh) " + str(TimeBounds(ssh)), "time2": "(taux) " + str(TimeBounds(taux))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
            # Change units
            ssh = ssh * 1e2
            taux = taux * 1e3

            # Computes the linear regression for all points, for SSHA >=0 and for SSHA<=0
            val, v_pos, v_neg = LinearRegressionAndNonlinearity(ssh, taux, return_stderr=True, return_intercept=True)

            # Non linearities
            nl1 = v_neg[0] - v_pos[0]
            nl2 = v_neg[1] + v_pos[1]

            # Dive down diagnostic
            dive_down_diag = {"value": None, "axis": None}

            if netcdf is True:
                ssh_map, ssh_map_areacell, keyerror1 = Read_data_mask_area(
                    sshfile, sshname, "sea surface height", metric, "equatorial_pacific", file_area=sshareafile,
                    name_area=sshareaname, file_mask=sshlandmaskfile, name_mask=sshlandmaskname, maskland=True,
                    maskocean=False, debug=debug, **kwargs)
                # Checks if the same time period is used for both variables
                ssh_map, taux, keyerror2 = CheckTime(ssh_map, taux, metric_name=metric, debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror1, keyerror2])
                if keyerror is not None:
                    # Preprocess variables (computes anomalies, normalizes, detrends TS, smooths TS, ...)
                    ssh_map, _, keyerror = PreProcessTS(
                        ssh_map, "", areacell=ssh_map_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del ssh_map_areacell
                    if keyerror is None:
                        # Regridding
                        if "regridding" not in list(kwargs.keys()) or isinstance(kwargs["regridding"], dict) is False:
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        ssh_map = Regrid(ssh_map, None, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                                          "shape1": "(ssh) " + str(ssh_map.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after Regrid", 15, **dict_debug)
                        # Meridional average
                        ssh_map, keyerror = AverageMeridional(ssh_map)
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                                              "shape2": "(ssh) " + str(ssh_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # Zonal smoothing
                            ssh_map, _ = Smoothing(ssh_map, "", axis=1, window=31, method="square")
                            if debug is True:
                                dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                                              "shape1": "(ssh) " + str(ssh_map.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Smoothing", 15, **dict_debug)
                            # Change units
                            ssh_map = ssh_map * 1e2
                            # Ssh to map
                            taux_map = TsToMap(taux, ssh_map)
                            # Array year by year
                            ssh_yby = get_year_by_year(ssh_map, frequency=kwargs["frequency"])
                            taux_yby = get_year_by_year(taux_map, frequency=kwargs["frequency"])
                            if debug is True:
                                dict_debug = {"axes1": "(ssh) " + str([ax.id for ax in ssh_map.getAxisList()]),
                                              "axes2": "(taux) " + str([ax.id for ax in taux_yby.getAxisList()]),
                                              "shape1": "(ssh) " + str(ssh_map.shape),
                                              "shape2": "(taux) " + str(taux_yby.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after get_year_by_year", 15, **dict_debug)
                            # Computes the linear regression for all points, for SSHA >=0 and for SSHA<=0
                            cur_val, cur_v_pos, cur_v_neg = LinearRegressionAndNonlinearity(
                                ssh_map, taux_map, return_stderr=False, return_intercept=False)
                            hov_val, hov_v_pos, hov_v_neg = LinearRegressionAndNonlinearity(
                                ssh_yby, taux_yby, return_stderr=False, return_intercept=False)
                            if debug is True:
                                dict_debug = {"axes1": "(zonal alpha) " + str([ax.id for ax in cur_val.getAxisList()]),
                                              "axes2": "(hovtx alpha) " + str([ax.id for ax in hov_val.getAxisList()]),
                                              "shape1": "(zonal alpha) " + str(cur_val.shape),
                                              "shape2": "(hovtx alpha) " + str(hov_val.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after LinearRegressionAndNonlinearity", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {
                                "units": "10-3 N/m2", "number_of_years_used": yearN,
                                "time_period": str(actualtimebounds),
                                "description": tauxbox + " tauxA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict2 = {
                                "units": "cm", "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": sshbox + " sshA", "diagnostic_value": val[0],
                                "diagnostic_value_error": val[1], "slope": val[0], "intercept": val[2],
                                "slope_neg": v_neg[0], "intercept_neg": v_neg[2], "slope_pos": v_pos[0],
                                "intercept_pos": v_pos[2]}
                            dict3 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of " + sshbox + " sshA onto tauxA across longitudes"}
                            dict4 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of " + sshbox + " sshA onto tauxA>0 across longitudes"}
                            dict5 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "regression of " + sshbox + " sshA onto tauxA<0 across longitudes"}
                            dict6 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of " + sshbox +
                                               " sshA onto tauxA"}
                            dict7 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of " + sshbox +
                                               " sshA onto tauxA>0"}
                            dict8 = {
                                "units": Units, "number_of_years_used": yearN, "time_period": str(actualtimebounds),
                                "description": "Hovmoeller diagram (time-longitude) of regression of " + sshbox +
                                               " sshA onto tauxA<0"}
                            dict9 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                     "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, var1_time_name="months_" + dataset, var2_time_name="months_" + dataset,
                                var1=taux, var1_attributes=dict1, var1_name=ovar[0] + dataset, global_attributes=dict9,
                                var2=ssh, var2_attributes=dict2, var2_name=ovar[1] + dataset,
                                var3=cur_val, var3_attributes=dict3, var3_name=ovar[2] + dataset,
                                var4=cur_v_pos, var4_attributes=dict4, var4_name=ovar[3] + dataset,
                                var5=cur_v_neg, var5_attributes=dict5, var5_name=ovar[4] + dataset,
                                var6=hov_val, var6_attributes=dict6, var6_name=ovar[5] + dataset,
                                var7=hov_v_pos, var7_attributes=dict7, var7_name=ovar[6] + dataset,
                                var8=hov_v_neg, var8_attributes=dict8, var8_name=ovar[7] + dataset)
                            del dict1, dict2, dict3, dict4, dict5, dict6, dict7, dict8, dict9, file_name
    # Create output
    metric_output = {
        "name": Name, "value": val[0], "value_error": val[1], "units": Units, "method": Method,
        "method_nonlinearity": Method_NL, "nyears": yearN, "time_frequency": kwargs["frequency"],
        "time_period": actualtimebounds, "ref": Ref, "nonlinearity": nl1, "nonlinearity_error": nl2,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def EnsoPrMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod, prfilemod,
              prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs, sstnameobs,
              sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs, prnameobs,
              prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox, event_definition,
              centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False, netcdf_name="",
              metname="", **kwargs):
    """
    The EnsoPrMap() function computes precipitation anomalies pattern associated with ENSO on the globe.
    It is the regression of 'prbox' prA (precipitation anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during boreal winter (usually the regression of global prA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param prbox: string
        name of box (e.g. 'global') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrMap_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO PRA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "mm/day/C"
    Method = "map of " + prbox + " precipitation anomalies regressed onto " + region_ev + " averaged SSTA during " + \
             season_ev + enso_method3
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoPrMap"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, pr_mod, keyerror_mod3 = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, pr_obs, keyerror_obs3 = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        # 1.2 PR 'prbox' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        kwargs["smoothing"] = smooth
        del mod_areacell, obs_areacell, pr_mod_areacell, pr_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
            pr_mod = SeasonalMean(pr_mod, season_ev, compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=prbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod.shape), "shape2": "(pr obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            pr_mod_slope = LinearRegressionTsAgainstMap(pr_mod, enso_mod, return_stderr=False)
            pr_obs_slope = LinearRegressionTsAgainstMap(pr_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {
                    "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                    "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                    "shape1": "(pr mod) " + str(pr_mod_slope.shape), "shape2": "(pr obs) " + str(pr_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            pr_mod_slope, keyerror_mod = BasinMask(
                pr_mod_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            pr_obs_slope, keyerror_obs = BasinMask(
                pr_obs_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod_slope.shape),
                                  "shape2": "(pr obs) " + str(pr_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                prRmse, keyerror = RmsAxis(
                    pr_mod_slope, pr_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                prRmseErr = None
                # Metric 2
                prCorr = float(Correlation(pr_mod_slope, pr_obs_slope, axis="xy", centered=1, biased=1))
                prCorrErr = None
                # Metric 3
                std_mod = Std(pr_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(pr_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                prStd = float(std_mod) / float(std_obs)
                prStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    pr_mod_land, pr_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitations", metric, prbox, file_area=prareafilemod,
                        name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    pr_obs_land, pr_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitations", metric, prbox, file_area=prareafileobs,
                        name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, pr_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, pr_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, pr_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, pr_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        pr_mod_land, _, keyerror_mod = PreProcessTS(
                            pr_mod_land, "", areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
                        pr_obs_land, _, keyerror_obs = PreProcessTS(
                            pr_obs_land, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
                        kwargs["smoothing"] = smooth
                        del pr_mod_areacell, pr_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            pr_mod_land = SeasonalMean(pr_mod_land, season_ev, compute_anom=True)
                            pr_obs_land = SeasonalMean(pr_obs_land, season_ev, compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                pr_mod_land, pr_obs_land, _ = TwoVarRegrid(
                                    pr_mod_land, pr_obs_land, "", region=prbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                        "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                        "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                        "shape2": "(pr obs) " + str(pr_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            pr_mod_land_slope = LinearRegressionTsAgainstMap(pr_mod_land, enso_mod, return_stderr=False)
                            pr_obs_land_slope = LinearRegressionTsAgainstMap(pr_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land_slope.getAxisList()]),
                                    "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land_slope.getAxisList()]),
                                    "shape1": "(pr mod) " + str(pr_mod_land_slope.shape),
                                    "shape2": "(pr obs) " + str(pr_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # samples
                            en_mod = Composite(pr_mod, en_years_mod, kwargs["frequency"])
                            ln_mod = Composite(pr_mod, ln_years_mod, kwargs["frequency"])
                            en_obs = Composite(pr_obs, en_years_obs, kwargs["frequency"])
                            ln_obs = Composite(pr_obs, ln_years_obs, kwargs["frequency"])
                            en_mod_land = Composite(pr_mod_land, en_years_mod, kwargs["frequency"])
                            ln_mod_land = Composite(pr_mod_land, ln_years_mod, kwargs["frequency"])
                            en_obs_land = Composite(pr_obs_land, en_years_obs, kwargs["frequency"])
                            ln_obs_land = Composite(pr_obs_land, ln_years_obs, kwargs["frequency"])
                            # mask Pacific
                            en_mod, keyerror_mod1 = BasinMask(
                                en_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_mod, keyerror_mod2 = BasinMask(
                                ln_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            en_obs, keyerror_obs1 = BasinMask(
                                en_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_obs, keyerror_obs2 = BasinMask(
                                ln_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                            if keyerror is None:
                                my_units = "" if kwargs["normalization"] is True else "mm/day"
                                # Metrics ENSO events global
                                dict_metric, dict_nc = dict(), dict()
                                nbr = 3
                                my_de = [
                                    "map of " + prbox + " PRA of La Nina events composite during " + season_ev +
                                    "; Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev +
                                    enso_method,
                                    "map of " + prbox + " PRA of El Nino events composite during " + season_ev +
                                    "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev +
                                    enso_method]
                                for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                        zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                            [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=de)
                                    nbr += 2
                                list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                for ii, reg in enumerate(list_region):
                                    # select region
                                    dictreg = ReferenceRegions(reg)
                                    tmp1 = pr_mod_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    tmp2 = pr_obs_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, description=Method.split(", ")[0].replace(prbox, reg))
                                    nbr += 2
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                [ln_years_obs, en_years_obs])):
                                        tmp1 = tab1(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = tab2(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg + "_" + evn,
                                            my_units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de.replace(prbox, reg))
                                        nbr += 2
                                    del dictreg, tmp1, tmp2
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                         "description": Method.split(", ")[0]}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                         "description": Method.split(", ")[0]}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: prRmse,
                                    "metric_valueRMSE_error_" + dataset2: prRmseErr,
                                    "metric_valueCORR_" + dataset2: prCorr,
                                    "metric_valueCORR_error_" + dataset2: prCorrErr,
                                    "metric_valueSTD_" + dataset2: prStd, "metric_valueSTD_error_" + dataset2: prStdErr}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=pr_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=pr_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if prCorr is not None:
        prCorr = 1 - prCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(prRmse), "line2": "metric value_error: " + str(prRmseErr),
                      "line3": "metric value: " + str(prCorr), "line4": "metric value_error: " + str(prCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": prRmse, "Rmse__value_error": prRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": prCorr, "Corr__value_error": prCorrErr, "Corr__units": "", "Std__value": prStd,
        "Std__value_error": prStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "ref": Ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoPrMapDjf(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                 prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs,
                 sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs,
                 prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox,
                 event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                 netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrMapDjf() function computes precipitation anomalies pattern associated with ENSO on the globe.
    It is the regression of 'prbox' prA (precipitation anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during DJF (usually the regression of global prA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param prbox: string
        name of box (e.g. 'global') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrMapDjf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO DJF PRA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "mm/day/C"
    Method = "map of " + prbox + " precipitation anomalies regressed onto " + region_ev + " averaged SSTA during DJF"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoPrMapDjf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod_box, pr_mod, keyerror_mod3 = CheckTime(sst_mod_box, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs_box, pr_obs, keyerror_obs3 = CheckTime(sst_obs_box, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod_box.shape[0] / 12))
    yearN_obs = int(round(sst_obs_box.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod_box)
    actualtimebounds_obs = TimeBounds(sst_obs_box)

    prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 PR in 'prbox' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        del pr_mod_areacell, pr_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "DJF", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "DJF", compute_anom=True)
            pr_mod = SeasonalMean(pr_mod, "DJF", compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, "DJF", compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=prbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod.shape), "shape2": "(pr obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            pr_mod_slope = LinearRegressionTsAgainstMap(pr_mod, enso_mod, return_stderr=False)
            pr_obs_slope = LinearRegressionTsAgainstMap(pr_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {
                    "axes1": "(mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                    "axes2": "(obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                    "shape1": "(mod) " + str(pr_mod_slope.shape), "shape2": "(obs) " + str(pr_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            pr_mod_slope, keyerror_mod = BasinMask(
                pr_mod_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            pr_obs_slope, keyerror_obs = BasinMask(
                pr_obs_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod_slope.shape),
                                  "shape2": "(pr obs) " + str(pr_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                prRmse, keyerror = RmsAxis(
                    pr_mod_slope, pr_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                prRmseErr = None
                # Metric 2
                prCorr = float(Correlation(pr_mod_slope, pr_obs_slope, axis="xy", centered=1, biased=1))
                prCorrErr = None
                # Metric 3
                std_mod = Std(pr_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(pr_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                prStd = float(std_mod) / float(std_obs)
                prStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    pr_mod_land, pr_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitations", metric, prbox, file_area=prareafilemod,
                        name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    pr_obs_land, pr_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitations", metric, prbox, file_area=prareafileobs,
                        name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, pr_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, pr_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, pr_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, pr_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        pr_mod_land, _, keyerror_mod = PreProcessTS(
                            pr_mod_land, "", areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
                        pr_obs_land, _, keyerror_obs = PreProcessTS(
                            pr_obs_land, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
                        del pr_mod_areacell, pr_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            pr_mod_land = SeasonalMean(pr_mod_land, "DJF", compute_anom=True)
                            pr_obs_land = SeasonalMean(pr_obs_land, "DJF", compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                pr_mod_land, pr_obs_land, _ = TwoVarRegrid(
                                    pr_mod_land, pr_obs_land, "", region=prbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                        "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                        "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                        "shape2": "(pr obs) " + str(pr_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            pr_mod_land_slope = LinearRegressionTsAgainstMap(pr_mod_land, enso_mod, return_stderr=False)
                            pr_obs_land_slope = LinearRegressionTsAgainstMap(pr_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land_slope.getAxisList()]),
                                    "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land_slope.getAxisList()]),
                                    "shape1": "(pr mod) " + str(pr_mod_land_slope.shape),
                                    "shape2": "(pr obs) " + str(pr_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(pr_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(pr_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(pr_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(pr_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(pr_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(pr_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(pr_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(pr_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "mm/day"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + prbox + " PRA of La Nina events composite during DJF; Nina = " +
                                        region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                        "map of " + prbox + " PRA of El Nino events composite during DJF; Nino = " +
                                        region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = pr_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = pr_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(prbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(prbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: prRmse,
                                             "metric_valueRMSE_error_" + dataset2: prRmseErr,
                                             "metric_valueCORR_" + dataset2: prCorr,
                                             "metric_valueCORR_error_" + dataset2: prCorrErr,
                                             "metric_valueSTD_" + dataset2: prStd,
                                             "metric_valueSTD_error_" + dataset2: prStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=pr_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=pr_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if prCorr is not None:
        prCorr = 1 - prCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(prRmse), "line2": "metric value_error: " + str(prRmseErr),
                      "line3": "metric value: " + str(prCorr), "line4": "metric value_error: " + str(prCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": prRmse, "Rmse__value_error": prRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": prCorr, "Corr__value_error": prCorrErr, "Corr__units": "", "Std__value": prStd,
        "Std__value_error": prStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoPrMapJja(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                 prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs,
                 sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs,
                 prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox,
                 event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                 netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrMapJja() function computes precipitation anomalies pattern associated with ENSO on the globe.
    It is the regression of 'prbox' prA (precipitation anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during JJA (usually the regression of global prA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param prbox: string
        name of box (e.g. 'global') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrMapJja_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO JJA PRA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "mm/day/C"
    Method = "map of " + prbox + " precipitation anomalies regressed onto " + region_ev + " averaged SSTA during JJA"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoPrMapJja"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod_box, pr_mod, keyerror_mod3 = CheckTime(sst_mod_box, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs_box, pr_obs, keyerror_obs3 = CheckTime(sst_obs_box, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod_box.shape[0] / 12))
    yearN_obs = int(round(sst_obs_box.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod_box)
    actualtimebounds_obs = TimeBounds(sst_obs_box)

    prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 PR in 'prbox' are normalized / detrended / smoothed (running average) if applicable
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        del pr_mod_areacell, pr_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "JJA", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "JJA", compute_anom=True)
            pr_mod = SeasonalMean(pr_mod, "JJA", compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, "JJA", compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                    "axes4": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(pr mod) " + str(pr_mod.shape), "shape4": "(pr obs) " + str(pr_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(pr mod) " + str(TimeBounds(pr_mod)), "time4": "(pr obs) " + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=prbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod.shape), "shape2": "(pr obs) " + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            pr_mod_slope = LinearRegressionTsAgainstMap(pr_mod, enso_mod, return_stderr=False)
            pr_obs_slope = LinearRegressionTsAgainstMap(pr_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {
                    "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                    "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                    "shape1": "(pr mod) " + str(pr_mod_slope.shape), "shape2": "(pr obs) " + str(pr_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            pr_mod_slope, keyerror_mod = BasinMask(
                pr_mod_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            pr_obs_slope, keyerror_obs = BasinMask(
                pr_obs_slope, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_slope.getAxisList()]),
                                  "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_slope.getAxisList()]),
                                  "shape1": "(pr mod) " + str(pr_mod_slope.shape),
                                  "shape2": "(pr obs) " + str(pr_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                prRmse, keyerror = RmsAxis(
                    pr_mod_slope, pr_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                prRmseErr = None
                # Metric 2
                prCorr = float(Correlation(pr_mod_slope, pr_obs_slope, axis="xy", centered=1, biased=1))
                prCorrErr = None
                # Metric 3
                std_mod = Std(pr_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(pr_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                prStd = float(std_mod) / float(std_obs)
                prStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    pr_mod_land, pr_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitations", metric, prbox, file_area=prareafilemod,
                        name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    pr_obs_land, pr_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitations", metric, prbox, file_area=prareafileobs,
                        name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, pr_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, pr_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, pr_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, pr_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        pr_mod_land, _, keyerror_mod = PreProcessTS(
                            pr_mod_land, "", areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
                        pr_obs_land, _, keyerror_obs = PreProcessTS(
                            pr_obs_land, "", areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
                        del pr_mod_areacell, pr_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            pr_mod_land = SeasonalMean(pr_mod_land, "JJA", compute_anom=True)
                            pr_obs_land = SeasonalMean(pr_obs_land, "JJA", compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                              "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                              "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                              "shape2": "(pr obs) " + str(pr_obs_land.shape),
                                              "time1": "(pr mod) " + str(TimeBounds(pr_mod_land)),
                                              "time2": "(pr obs) " + str(TimeBounds(pr_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                pr_mod_land, pr_obs_land, _ = TwoVarRegrid(
                                    pr_mod_land, pr_obs_land, "", region=prbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land.getAxisList()]),
                                        "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land.getAxisList()]),
                                        "shape1": "(pr mod) " + str(pr_mod_land.shape),
                                        "shape2": "(pr obs) " + str(pr_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            pr_mod_land_slope = LinearRegressionTsAgainstMap(pr_mod_land, enso_mod, return_stderr=False)
                            pr_obs_land_slope = LinearRegressionTsAgainstMap(pr_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(pr mod) " + str([ax.id for ax in pr_mod_land_slope.getAxisList()]),
                                    "axes2": "(pr obs) " + str([ax.id for ax in pr_obs_land_slope.getAxisList()]),
                                    "shape1": "(pr mod) " + str(pr_mod_land_slope.shape),
                                    "shape2": "(pr obs) " + str(pr_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(pr_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(pr_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(pr_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(pr_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(pr_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(pr_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(pr_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(pr_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=prbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "mm/day"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + prbox + " PRA of La Nina events composite during JJA (before " +
                                        "events); Nina = " + region_ev + " SSTA < -" + my_thresh + " during " +
                                        season_ev + enso_method,
                                        "map of " + prbox + " PRA of El Nino events composite during JJA (before " +
                                        "events); Nino = " + region_ev + " SSTA > " + my_thresh + " during " +
                                        season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = pr_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = pr_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(prbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(prbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: prRmse,
                                             "metric_valueRMSE_error_" + dataset2: prRmseErr,
                                             "metric_valueCORR_" + dataset2: prCorr,
                                             "metric_valueCORR_error_" + dataset2: prCorrErr,
                                             "metric_valueSTD_" + dataset2: prStd,
                                             "metric_valueSTD_error_" + dataset2: prStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=pr_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=pr_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if prCorr is not None:
        prCorr = 1 - prCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(prRmse), "line2": "metric value_error: " + str(prRmseErr),
                      "line3": "metric value: " + str(prCorr), "line4": "metric value_error: " + str(prCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": prRmse, "Rmse__value_error": prRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": prCorr, "Corr__value_error": prCorrErr, "Corr__units": "", "Std__value": prStd,
        "Std__value_error": prStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoPrDjfTel(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                 prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs,
                 sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs,
                 prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox,
                 event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                 netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrDjfTel() function computes precipitations anomalies associated with El Nino and La Nina events in many AR5
        reference regions, then precipitations during DJF preceding the events are composited for each selected event
        and the difference (El Nino PR - La Nina PR) is computed in each region.
    The first rmse(observations vs model) is the metric.
    The second metric is the number of regions where observations and models agree on the sign of the teleconnection

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param box: string
        name of box (e.g. 'nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrDjfTel_2'
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, Rmse__value (rms [NinoPr-NinaPr]), Rmse__value_error, Rmse__units, method,
        SignAgree__value (sign agreement [NinoPr-NinaPr]), SignAgree__value_error, SignAgree__units, nyears_model,
        nyears_observations, nina_model, nino_model, nina_observations, nino_observations, time_frequency,
        time_period_model, time_period_observations, ref, keyerror, dive_down_diag, units

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    threshold = event_definition["threshold"]
    normalize = event_definition["normalization"]
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Nino composite minus Nina composite during DJF preceeding the events in each region"
    Method = "Nino events = " + region_ev + " sstA > " + str(threshold) + ", Nina events = " + region_ev + " sstA < -" \
             + str(threshold) + " during " + season_ev + "; Precipitations associated with El Nino/La Nina events " + \
             " during the preceeding DJF are composited and the difference (El Nino PR - La Nina PR) is computed in" + \
             " each region"
    if kwargs["normalization"]:
        Units = ""
    else:
        Units = "mm/day"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoPrDjfTel"
    if metname == "":
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    if not isinstance(prbox, list):
        prbox = [prbox]
    prbox = sorted(prbox, key=str.lower)
    prmap_mod, _, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, prbox[0], file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    prmap_obs, _, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, prbox[0], file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, prmap_mod, keyerror_mod3 = CheckTime(sst_mod, prmap_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, prmap_obs, keyerror_obs3 = CheckTime(sst_obs, prmap_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        compRmse, compRmseErr, signAgreement, signAgreementErr = None, None, None, None
        nina_years_mod, nina_years_obs, nino_years_mod, nino_years_obs = None, None, None, None
        dive_down_diag = {"model": None, "observations": None, "axis": None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs:
            compRmse, compRmseErr, signAgreement, signAgreementErr = None, None, None, None
            nina_years_mod, nina_years_obs, nino_years_mod, nino_years_obs = None, None, None, None
            dive_down_diag = {"model": None, "observations": None, "axis": None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape),
                              "time1": "(mod) " + str(TimeBounds(sst_mod)),
                              "time2": "(obs) " + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # 1.2 SSTA < 'threshold' (SSTA > 'threshold') during 'season' are considered as La Nina (El Nino) events
            # Lists event years
            nina_years_mod = DetectEvents(sst_mod, season_ev, -threshold, normalization=normalize, nino=False)
            nino_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            nina_years_obs = DetectEvents(sst_obs, season_ev, -threshold, normalization=normalize, nino=False)
            nino_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {"nina1": "(mod) " + str(nina_years_mod), "nina2": "(obs) " + str(nina_years_obs),
                              "nino1": "(mod) " + str(nino_years_mod), "nino2": "(obs) " + str(nino_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

            # ------------------------------------------------
            # 2. compute composite
            # ------------------------------------------------
            # smoothing is not applied
            if "smoothing" in list(kwargs.keys()):
                smooth = deepcopy(kwargs["smoothing"])
                kwargs["smoothing"] = False
            else:
                smooth = False
            list_composite_mod, list_composite_obs = list(), list()
            loop_keyerror = None
            loop_box = list()
            for reg in prbox:
                if debug is True:
                    EnsoErrorsWarnings.debug_mode("\033[92m", "region = " + str(reg), 15)
                # Read if the given region is defined as a land region, an oceanic region, or both
                dict_reg = ReferenceRegions(reg)
                maskland = dict_reg["maskland"] if "maskland" in list(dict_reg.keys()) else False
                maskoce = dict_reg["maskocean"] if "maskocean" in list(dict_reg.keys()) else False
                pr_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                    prfilemod, prnamemod, "precipitations", metric, reg, file_area=prareafilemod,
                    name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod,
                    maskland=maskland, maskocean=maskoce, time_bounds=kwargs["time_bounds_mod"], debug=debug,
                    **kwargs)
                pr_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                    prfileobs, prnameobs, "precipitations", metric, reg, file_area=prareafileobs,
                    name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs,
                    maskland=maskland, maskocean=maskoce, time_bounds=kwargs["time_bounds_obs"], debug=debug,
                    **kwargs)
                if keyerror_mod is not None or keyerror_obs is not None:
                    loop_keyerror = add_up_errors([loop_keyerror, keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                      "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                      "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape),
                                      "time1": "(mod) " + str(TimeBounds(pr_mod)),
                                      "time2": "(obs) " + str(TimeBounds(pr_obs))}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after Read_data_mask_area", 20, **dict_debug)
                    # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                    pr_mod, Method, keyerror_mod = PreProcessTS(
                        pr_mod, Method, areacell=mod_areacell, average="horizontal", compute_anom=False, region=reg,
                        **kwargs)
                    pr_obs, _, keyerror_obs = PreProcessTS(
                        pr_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=reg,
                        **kwargs)
                    del mod_areacell, obs_areacell
                    if keyerror_mod is not None or keyerror_obs is not None:
                        loop_keyerror = add_up_errors([loop_keyerror, keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                          "shape1": "(mod) " + str(pr_mod.shape),
                                          "shape2": "(obs) " + str(pr_obs.shape),
                                          "time1": "(mod) " + str(TimeBounds(pr_mod)),
                                          "time2": "(obs) " + str(TimeBounds(pr_obs))}
                            EnsoErrorsWarnings.debug_mode(
                                "\033[92m", "after PreProcessTS " + str(reg), 20, **dict_debug)

                        # Seasonal mean
                        pr_mod = SeasonalMean(pr_mod, "DJF", compute_anom=False)
                        pr_obs = SeasonalMean(pr_obs, "DJF", compute_anom=False)

                        # composites
                        composite_nina_mod = Composite(pr_mod, nina_years_mod, kwargs["frequency"])
                        composite_nino_mod = Composite(pr_mod, nino_years_mod, kwargs["frequency"])
                        composite_nina_obs = Composite(pr_obs, nina_years_obs, kwargs["frequency"])
                        composite_nino_obs = Composite(pr_obs, nino_years_obs, kwargs["frequency"])

                        # list composites
                        list_composite_mod.append(float(composite_nino_mod - composite_nina_mod))
                        list_composite_obs.append(float(composite_nino_obs - composite_nina_obs))
                        loop_box.append(reg)
                        del composite_nina_mod, composite_nina_obs, composite_nino_mod, composite_nino_obs
                del dict_reg, keyerror_mod, keyerror_obs, maskland, maskoce, mod_areacell, obs_areacell, pr_mod, pr_obs

            # create arrays
            ar5 = "AR5 reference regions"
            ref = "https://www.ipcc-data.org/guidelines/pages/ar5_regions.html"
            list_composite_mod = ArrayListAx(
                list_composite_mod, loop_box, ax_name_ax="box", ax_long_name=ar5, ax_ref=ref)
            list_composite_obs = ArrayListAx(
                list_composite_obs, loop_box, ax_name_ax="box", ax_long_name=ar5, ax_ref=ref)

            if "smoothing" in list(kwargs.keys()):
                kwargs["smoothing"] = smooth
                del smooth

            # Computes the root mean square difference
            compRmse, keyerror = RmsAxis(
                list_composite_mod, list_composite_obs, centered=centered_rmse, biased=biased_rmse)
            compRmseErr = None

            # Computes the percentage of regions where observations and model agree on the sign of the teleconnection
            signAgreement = sum([1. for vmod, vobs in zip(list_composite_mod, list_composite_obs)
                                 if NUMPYsign(vmod) == NUMPYsign(vobs)]) / len(list_composite_mod)
            signAgreementErr = NUMPYsqrt(signAgreement * (1 - signAgreement) / len(list_composite_mod)) * 1.65

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(list_composite_mod), "observations": ArrayToList(list_composite_obs),
                              "axis": loop_box}
            if keyerror is not None or loop_keyerror is not None:
                keyerror = add_up_errors([keyerror, loop_keyerror])

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "nina_years": str(nina_years_mod), "nino_years": str(nino_years_mod)}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "nina_years": str(nina_years_obs), "nino_years": str(nino_years_obs)}
                dict3 = {
                    "metric_name": Name, "metric_valueRMSE_" + dataset2: compRmse,
                    "metric_valueRMSE_error_" + dataset2: compRmseErr,
                    "metric_valueSignAgree_" + dataset2: signAgreement,
                    "metric_valueSignAgree_error_" + dataset2: signAgreementErr, "metric_method": Method,
                    "metric_reference": Ref, "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, var1=list_composite_mod, var1_attributes=dict1,
                           var1_name="prComp_box__" + dataset1, var2=list_composite_obs, var2_attributes=dict2,
                           var2_name="prComp_box__" + dataset2, global_attributes=dict3)
                del dict1, dict2, dict3, file_name

    # Create output
    metric_output = {
        "name": Name, "Rmse__value": compRmse, "Rmse__value_error": signAgreement, "Rmse__units": Units,
        "method": Method, "SignAgree__value": signAgreement, "SignAgree__value_error": signAgreementErr,
        "SignAgree__units": "%", "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "nina_model": nina_years_mod, "nino_model": nino_years_mod, "nina_observations": nina_years_obs,
        "nino_observations": nino_years_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoPrJjaTel(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                 prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs,
                 sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs,
                 prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox,
                 event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                 netcdf_name="", metname="", **kwargs):
    """
    The EnsoPrJjaTel() function computes precipitations anomalies associated with El Nino and La Nina events in many AR5
        reference regions, then precipitations during JJA preceding the events are composited for each selected event
        and the difference (El Nino PR - La Nina PR) is computed in each region.
    The first rmse(observations vs model) is the metric.
    The second metric is the number of regions where observations and models agree on the sign of the teleconnection

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param box: string
        name of box (e.g. 'nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoPrJjaTel_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, Rmse__value (rms [NinoPr-NinaPr]), Rmse__value_error, Rmse__units, method,
        SignAgree__value (sign agreement [NinoPr-NinaPr]), SignAgree__value_error, SignAgree__units, nyears_model,
        nyears_observations, nina_model, nino_model, nina_observations, nino_observations, time_frequency,
        time_period_model, time_period_observations, ref, keyerror, dive_down_diag, units

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    threshold = event_definition["threshold"]
    normalize = event_definition["normalization"]
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "Nino composite minus Nina composite during JJA preceeding the events in each region"
    Method = "Nino events = " + region_ev + " sstA > " + str(threshold) + ", Nina events = " + region_ev + " sstA < -" \
             + str(threshold) + " during " + season_ev + "; Precipitations associated with El Nino/La Nina events " + \
             " during the preceeding JJA are composited and the difference (El Nino PR - La Nina PR) is computed in" + \
             " each region"
    if kwargs["normalization"]:
        Units = ""
    else:
        Units = "mm/day"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "EnsoPrJjaTel"
    if metname == "":
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    if not isinstance(prbox, list):
        prbox = [prbox]
    prbox = sorted(prbox, key=str.lower)
    prmap_mod, _, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, prbox[0], file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    prmap_obs, _, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, prbox[0], file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, prmap_mod, keyerror_mod3 = CheckTime(sst_mod, prmap_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, prmap_obs, keyerror_obs3 = CheckTime(sst_obs, prmap_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        compRmse, compRmseErr, signAgreement, signAgreementErr = None, None, None, None
        nina_years_mod, nina_years_obs, nino_years_mod, nino_years_obs = None, None, None, None
        dive_down_diag = {"model": None, "observations": None, "axis": None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs:
            compRmse, compRmseErr, signAgreement, signAgreementErr = None, None, None, None
            nina_years_mod, nina_years_obs, nino_years_mod, nino_years_obs = None, None, None, None
            dive_down_diag = {"model": None, "observations": None, "axis": None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape),
                              "time1": "(mod) " + str(TimeBounds(sst_mod)),
                              "time2": "(obs) " + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # 1.2 SSTA < 'threshold' (SSTA > 'threshold') during 'season' are considered as La Nina (El Nino) events
            # Lists event years
            nina_years_mod = DetectEvents(sst_mod, season_ev, -threshold, normalization=normalize, nino=False)
            nino_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            nina_years_obs = DetectEvents(sst_obs, season_ev, -threshold, normalization=normalize, nino=False)
            nino_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {"nina1": "(mod) " + str(nina_years_mod), "nina2": "(obs) " + str(nina_years_obs),
                              "nino1": "(mod) " + str(nino_years_mod), "nino2": "(obs) " + str(nino_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

            # ------------------------------------------------
            # 2. compute composite
            # ------------------------------------------------
            # smoothing is not applied
            if "smoothing" in list(kwargs.keys()):
                smooth = deepcopy(kwargs["smoothing"])
                kwargs["smoothing"] = False
            else:
                smooth = False
            list_composite_mod, list_composite_obs = list(), list()
            loop_keyerror = None
            loop_box = list()
            for reg in prbox:
                if debug is True:
                    EnsoErrorsWarnings.debug_mode("\033[92m", "region = " + str(reg), 15)
                # Read if the given region is defined as a land region, an oceanic region, or both
                dict_reg = ReferenceRegions(reg)
                maskland = dict_reg["maskland"] if "maskland" in list(dict_reg.keys()) else False
                maskoce = dict_reg["maskocean"] if "maskocean" in list(dict_reg.keys()) else False
                pr_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                    prfilemod, prnamemod, "precipitations", metric, reg, file_area=prareafilemod,
                    name_area=prareanamemod, file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod,
                    maskland=maskland, maskocean=maskoce, time_bounds=kwargs["time_bounds_mod"], debug=debug,
                    **kwargs)
                pr_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                    prfileobs, prnameobs, "precipitations", metric, reg, file_area=prareafileobs,
                    name_area=prareanameobs, file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs,
                    maskland=maskland, maskocean=maskoce, time_bounds=kwargs["time_bounds_obs"], debug=debug,
                    **kwargs)
                if keyerror_mod is not None or keyerror_obs is not None:
                    loop_keyerror = add_up_errors([loop_keyerror, keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                      "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                      "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape),
                                      "time1": "(mod) " + str(TimeBounds(pr_mod)),
                                      "time2": "(obs) " + str(TimeBounds(pr_obs))}
                        EnsoErrorsWarnings.debug_mode("\033[92m", "after Read_data_mask_area", 20, **dict_debug)
                    # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                    pr_mod, Method, keyerror_mod = PreProcessTS(
                        pr_mod, Method, areacell=mod_areacell, average="horizontal", compute_anom=False, region=reg,
                        **kwargs)
                    pr_obs, _, keyerror_obs = PreProcessTS(
                        pr_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=reg,
                        **kwargs)
                    del mod_areacell, obs_areacell
                    if keyerror_mod is not None or keyerror_obs is not None:
                        loop_keyerror = add_up_errors([loop_keyerror, keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                          "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                          "shape1": "(mod) " + str(pr_mod.shape),
                                          "shape2": "(obs) " + str(pr_obs.shape),
                                          "time1": "(mod) " + str(TimeBounds(pr_mod)),
                                          "time2": "(obs) " + str(TimeBounds(pr_obs))}
                            EnsoErrorsWarnings.debug_mode(
                                "\033[92m", "after PreProcessTS " + str(reg), 20, **dict_debug)

                        # Seasonal mean
                        pr_mod = SeasonalMean(pr_mod, "JJA", compute_anom=False)
                        pr_obs = SeasonalMean(pr_obs, "JJA", compute_anom=False)

                        # composites
                        composite_nina_mod = Composite(pr_mod, nina_years_mod, kwargs["frequency"])
                        composite_nino_mod = Composite(pr_mod, nino_years_mod, kwargs["frequency"])
                        composite_nina_obs = Composite(pr_obs, nina_years_obs, kwargs["frequency"])
                        composite_nino_obs = Composite(pr_obs, nino_years_obs, kwargs["frequency"])

                        # list composites
                        list_composite_mod.append(float(composite_nino_mod - composite_nina_mod))
                        list_composite_obs.append(float(composite_nino_obs - composite_nina_obs))
                        loop_box.append(reg)
                        del composite_nina_mod, composite_nina_obs, composite_nino_mod, composite_nino_obs
                del dict_reg, keyerror_mod, keyerror_obs, maskland, maskoce, mod_areacell, obs_areacell, pr_mod, pr_obs

            # create arrays
            ar5 = "AR5 reference regions"
            ref = "https://www.ipcc-data.org/guidelines/pages/ar5_regions.html"
            list_composite_mod = ArrayListAx(
                list_composite_mod, loop_box, ax_name_ax="box", ax_long_name=ar5, ax_ref=ref)
            list_composite_obs = ArrayListAx(
                list_composite_obs, loop_box, ax_name_ax="box", ax_long_name=ar5, ax_ref=ref)

            if "smoothing" in list(kwargs.keys()):
                kwargs["smoothing"] = smooth
                del smooth

            # Computes the root mean square difference
            compRmse, keyerror = RmsAxis(
                list_composite_mod, list_composite_obs, centered=centered_rmse, biased=biased_rmse)
            compRmseErr = None

            # Computes the percentage of regions where observations and model agree on the sign of the teleconnection
            signAgreement = sum([1. for vmod, vobs in zip(list_composite_mod, list_composite_obs)
                                 if NUMPYsign(vmod) == NUMPYsign(vobs)]) / len(list_composite_mod)
            signAgreementErr = NUMPYsqrt(signAgreement * (1 - signAgreement) / len(list_composite_mod)) * 1.65

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(list_composite_mod), "observations": ArrayToList(list_composite_obs),
                              "axis": loop_box}
            if keyerror is not None or loop_keyerror is not None:
                keyerror = add_up_errors([keyerror, loop_keyerror])

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {"units": Units, "number_of_years_used": yearN_mod, "time_period": str(actualtimebounds_mod),
                         "nina_years": str(nina_years_mod), "nino_years": str(nino_years_mod)}
                dict2 = {"units": Units, "number_of_years_used": yearN_obs, "time_period": str(actualtimebounds_obs),
                         "nina_years": str(nina_years_obs), "nino_years": str(nino_years_obs)}
                dict3 = {
                    "metric_name": Name, "metric_valueRMSE_" + dataset2: compRmse,
                    "metric_valueRMSE_error_" + dataset2: compRmseErr,
                    "metric_valueSignAgree_" + dataset2: signAgreement,
                    "metric_valueSignAgree_error_" + dataset2: signAgreementErr, "metric_method": Method,
                    "metric_reference": Ref, "frequency": kwargs["frequency"]}
                SaveNetcdf(file_name, var1=list_composite_mod, var1_attributes=dict1,
                           var1_name="prComp_box__" + dataset1, var2=list_composite_obs, var2_attributes=dict2,
                           var2_name="prComp_box__" + dataset2, global_attributes=dict3)
                del dict1, dict2, dict3, file_name

    # Create output
    metric_output = {
        "name": Name, "Rmse__value": compRmse, "Rmse__value_error": signAgreement, "Rmse__units": Units,
        "method": Method, "SignAgree__value": signAgreement, "SignAgree__value_error": signAgreementErr,
        "SignAgree__units": "%", "nyears_model": yearN_mod, "nyears_observations": yearN_obs,
        "nina_model": nina_years_mod, "nino_model": nino_years_mod, "nina_observations": nina_years_obs,
        "nino_observations": nino_years_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSlpMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               slpfilemod, slpnamemod, slpareafilemod, slpareanamemod, slplandmaskfilemod, slplandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
               slpfileobs, slpnameobs, slpareafileobs, slpareanameobs, slplandmaskfileobs, slplandmasknameobs, sstbox,
               slpbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False,
               netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSlpMap() function computes sea level pressure anomalies pattern associated with ENSO on the globe.
    It is the regression of 'slpbox' slpA (sea level pressure anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during boreal winter (usually the regression of global slpA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param slpfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SLP
    :param slpnamemod: string
        name of SLP variable (psl) in 'slpfilemod'
    :param slpareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP areacell
    :param slpareanamemod: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafilemod'
    :param slplandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP landmask
    :param slplandmasknamemod: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param slpfileobs: string
        path_to/filename of the file (NetCDF) of the observed SLP
    :param slpnameobs: string
        name of SLP variable (slp) in 'slpfileobs'
    :param slpareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP areacell
    :param slpareanameobs: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafileobs'
    :param slplandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP landmask
    :param slplandmasknameobs: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param slpbox: string
        name of box (e.g. 'global') for SLP
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSlpMap_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO SLPA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "hPa/C"
    Method = "map of " + slpbox + " sea level pressure anomalies regressed onto " + region_ev + " averaged SSTA " + \
             "during " + season_ev + enso_method3
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSlpMap"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    slp_mod, slp_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod, name_area=slpareanamemod,
        file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    slp_obs, slp_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs, name_area=slpareanameobs,
        file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, slp_mod, keyerror_mod3 = CheckTime(sst_mod, slp_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, slp_obs, keyerror_obs3 = CheckTime(sst_obs, slp_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        # 1.2 SLP in 'slpbox' are normalized / detrended / smoothed (running average) if applicable
        slp_mod, Method, keyerror_mod2 = PreProcessTS(
            slp_mod, Method, areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
        slp_obs, _, keyerror_obs2 = PreProcessTS(
            slp_obs, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
        kwargs["smoothing"] = smooth
        del mod_areacell, obs_areacell, slp_mod_areacell, slp_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
            slp_mod = SeasonalMean(slp_mod, season_ev, compute_anom=True) * 1e-2
            slp_obs = SeasonalMean(slp_obs, season_ev, compute_anom=True) * 1e-2
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                slp_mod, slp_obs, Method = TwoVarRegrid(slp_mod, slp_obs, Method, region=slpbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                        "shape1": "(slp mod) " + str(slp_mod.shape), "shape2": "(slp obs) " + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            slp_mod_slope = LinearRegressionTsAgainstMap(slp_mod, enso_mod, return_stderr=False)
            slp_obs_slope = LinearRegressionTsAgainstMap(slp_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                              "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                              "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            slp_mod_slope, keyerror_mod = BasinMask(
                slp_mod_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            slp_obs_slope, keyerror_obs = BasinMask(
                slp_obs_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                                  "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                                  "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                                  "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                slpRmse, keyerror = RmsAxis(
                    slp_mod_slope, slp_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                slpRmseErr = None
                # Metric 2
                slpCorr = float(Correlation(slp_mod_slope, slp_obs_slope, axis="xy", centered=1, biased=1))
                slpCorrErr = None
                # Metric 3
                std_mod = Std(slp_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(slp_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                slpStd = float(std_mod) / float(std_obs)
                slpStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    slp_mod_land, slp_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod,
                        name_area=slpareanamemod, file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    slp_obs_land, slp_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs,
                        name_area=slpareanameobs, file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, slp_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, slp_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, slp_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, slp_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # slpocess
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        slp_mod_land, _, keyerror_mod = PreProcessTS(
                            slp_mod_land, "", areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
                        slp_obs_land, _, keyerror_obs = PreProcessTS(
                            slp_obs_land, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
                        kwargs["smoothing"] = smooth
                        del slp_mod_areacell, slp_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            slp_mod_land = SeasonalMean(slp_mod_land, season_ev, compute_anom=True) * 1e-2
                            slp_obs_land = SeasonalMean(slp_obs_land, season_ev, compute_anom=True) * 1e-2
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                slp_mod_land, slp_obs_land, _ = TwoVarRegrid(
                                    slp_mod_land, slp_obs_land, "", region=slpbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                        "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                        "shape2": "(slp obs) " + str(slp_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            slp_mod_land_slope = LinearRegressionTsAgainstMap(
                                slp_mod_land, enso_mod, return_stderr=False)
                            slp_obs_land_slope = LinearRegressionTsAgainstMap(
                                slp_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land_slope.getAxisList()]),
                                    "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land_slope.getAxisList()]),
                                    "shape1": "(slp mod) " + str(slp_mod_land_slope.shape),
                                    "shape2": "(slp obs) " + str(slp_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # samples
                            en_mod = Composite(slp_mod, en_years_mod, kwargs["frequency"])
                            ln_mod = Composite(slp_mod, ln_years_mod, kwargs["frequency"])
                            en_obs = Composite(slp_obs, en_years_obs, kwargs["frequency"])
                            ln_obs = Composite(slp_obs, ln_years_obs, kwargs["frequency"])
                            en_mod_land = Composite(slp_mod_land, en_years_mod, kwargs["frequency"])
                            ln_mod_land = Composite(slp_mod_land, ln_years_mod, kwargs["frequency"])
                            en_obs_land = Composite(slp_obs_land, en_years_obs, kwargs["frequency"])
                            ln_obs_land = Composite(slp_obs_land, ln_years_obs, kwargs["frequency"])
                            # mask Pacific
                            en_mod, keyerror_mod1 = BasinMask(
                                en_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_mod, keyerror_mod2 = BasinMask(
                                ln_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            en_obs, keyerror_obs1 = BasinMask(
                                en_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_obs, keyerror_obs2 = BasinMask(
                                ln_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                            if keyerror is None:
                                my_units = "" if kwargs["normalization"] is True else "hPa"
                                # Metrics ENSO events global
                                dict_metric, dict_nc = dict(), dict()
                                nbr = 3
                                my_de = [
                                    "map of " + slpbox + " SLPA of La Nina events composite during " + season_ev +
                                    "; Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev +
                                    enso_method,
                                    "map of " + slpbox + " SLPA of El Nino events composite during " + season_ev +
                                    "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev +
                                    enso_method]
                                for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                        zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                            [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=de)
                                    nbr += 2
                                list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                for ii, reg in enumerate(list_region):
                                    # select region
                                    dictreg = ReferenceRegions(reg)
                                    tmp1 = slp_mod_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    tmp2 = slp_obs_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, description=Method.split(", ")[0].replace(slpbox, reg))
                                    nbr += 2
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                [ln_years_obs, en_years_obs])):
                                        tmp1 = tab1(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = tab2(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg + "_" + evn,
                                            my_units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de.replace(slpbox, reg))
                                        nbr += 2
                                    del dictreg, tmp1, tmp2
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                         "description": Method.split(", ")[0]}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                         "description": Method.split(", ")[0]}
                                dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                         "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: slpRmse,
                                         "metric_valueRMSE_error_" + dataset2: slpRmseErr,
                                         "metric_valueCORR_" + dataset2: slpCorr,
                                         "metric_valueCORR_error_" + dataset2: slpCorrErr,
                                         "metric_valueSTD_" + dataset2: slpStd,
                                         "metric_valueSTD_error_" + dataset2: slpStdErr}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=slp_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=slp_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if slpCorr is not None:
        slpCorr = 1 - slpCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(slpRmse), "line2": "metric value_error: " + str(slpRmseErr),
                      "line3": "metric value: " + str(slpCorr), "line4": "metric value_error: " + str(slpCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": slpRmse, "Rmse__value_error": slpRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": slpCorr, "Corr__value_error": slpCorrErr, "Corr__units": "", "Std__value": slpStd,
        "Std__value_error": slpStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "ref": Ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSlpMapDjf(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  slpfilemod, slpnamemod, slpareafilemod, slpareanamemod, slplandmaskfilemod, slplandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  slpfileobs, slpnameobs, slpareafileobs, slpareanameobs, slplandmaskfileobs, slplandmasknameobs,
                  sstbox, slpbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                  debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSlpMapDjf() function computes sea level pressure anomalies pattern associated with ENSO on the globe.
    It is the regression of 'slpbox' slpA (sea level pressure anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during DJF (usually the regression of global slpA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param slpfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SLP
    :param slpnamemod: string
        name of SLP variable (slp) in 'slpfilemod'
    :param slpareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP areacell
    :param slpareanamemod: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafilemod'
    :param slplandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP landmask
    :param slplandmasknamemod: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param slpfileobs: string
        path_to/filename of the file (NetCDF) of the observed SLP
    :param slpnameobs: string
        name of SLP variable (slp) in 'slpfileobs'
    :param slpareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP areacell
    :param slpareanameobs: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafileobs'
    :param slplandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP landmask
    :param slplandmasknameobs: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param slpbox: string
        name of box (e.g. 'global') for SLP
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSlpMapDjf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO DJF SLPA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "hPa/C"
    Method = "map of " + slpbox + " sea level pressure anomalies regressed onto " + region_ev + \
             " averaged SSTA during DJF"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSlpMapDjf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    slp_mod, slp_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod, name_area=slpareanamemod,
        file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    slp_obs, slp_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs, name_area=slpareanameobs,
        file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod_box, slp_mod, keyerror_mod3 = CheckTime(sst_mod_box, slp_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs_box, slp_obs, keyerror_obs3 = CheckTime(sst_obs_box, slp_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod_box.shape[0] / 12))
    yearN_obs = int(round(sst_obs_box.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod_box)
    actualtimebounds_obs = TimeBounds(sst_obs_box)

    slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 SLP in 'slpbox' are normalized / detrended / smoothed (running average) if applicable
        slp_mod, Method, keyerror_mod2 = PreProcessTS(
            slp_mod, Method, areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
        slp_obs, _, keyerror_obs2 = PreProcessTS(
            slp_obs, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
        del slp_mod_areacell, slp_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "DJF", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "DJF", compute_anom=True)
            slp_mod = SeasonalMean(slp_mod, "DJF", compute_anom=True) * 1e-2
            slp_obs = SeasonalMean(slp_obs, "DJF", compute_anom=True) * 1e-2
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                slp_mod, slp_obs, Method = TwoVarRegrid(slp_mod, slp_obs, Method, region=slpbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                        "shape1": "(slp mod) " + str(slp_mod.shape), "shape2": "(slp obs) " + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            slp_mod_slope = LinearRegressionTsAgainstMap(slp_mod, enso_mod, return_stderr=False)
            slp_obs_slope = LinearRegressionTsAgainstMap(slp_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                              "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                              "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            slp_mod_slope, keyerror_mod = BasinMask(
                slp_mod_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            slp_obs_slope, keyerror_obs = BasinMask(
                slp_obs_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                                  "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                                  "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                                  "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                slpRmse, keyerror = RmsAxis(
                    slp_mod_slope, slp_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                slpRmseErr = None
                # Metric 2
                slpCorr = float(Correlation(slp_mod_slope, slp_obs_slope, axis="xy", centered=1, biased=1))
                slpCorrErr = None
                # Metric 3
                std_mod = Std(slp_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(slp_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                slpStd = float(std_mod) / float(std_obs)
                slpStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    slp_mod_land, slp_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod,
                        name_area=slpareanamemod, file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    slp_obs_land, slp_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs,
                        name_area=slpareanameobs, file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, slp_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, slp_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, slp_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, slp_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # slpocess
                        slp_mod_land, _, keyerror_mod = PreProcessTS(
                            slp_mod_land, "", areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
                        slp_obs_land, _, keyerror_obs = PreProcessTS(
                            slp_obs_land, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
                        del slp_mod_areacell, slp_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            slp_mod_land = SeasonalMean(slp_mod_land, "DJF", compute_anom=True) * 1e-2
                            slp_obs_land = SeasonalMean(slp_obs_land, "DJF", compute_anom=True) * 1e-2
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                slp_mod_land, slp_obs_land, _ = TwoVarRegrid(
                                    slp_mod_land, slp_obs_land, "", region=slpbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                        "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                        "shape2": "(slp obs) " + str(slp_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            slp_mod_land_slope = LinearRegressionTsAgainstMap(
                                slp_mod_land, enso_mod, return_stderr=False)
                            slp_obs_land_slope = LinearRegressionTsAgainstMap(
                                slp_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land_slope.getAxisList()]),
                                    "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land_slope.getAxisList()]),
                                    "shape1": "(slp mod) " + str(slp_mod_land_slope.shape),
                                    "shape2": "(slp obs) " + str(slp_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(slp_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(slp_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(slp_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(slp_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(slp_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(slp_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(slp_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(slp_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "hPa"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + slpbox + " SLPA of La Nina events composite during DJF; Nina = " +
                                        region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                        "map of " + slpbox + " SLPA of El Nino events composite during DJF; Nino = " +
                                        region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = slp_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = slp_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(slpbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(slpbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: slpRmse,
                                             "metric_valueRMSE_error_" + dataset2: slpRmseErr,
                                             "metric_valueCORR_" + dataset2: slpCorr,
                                             "metric_valueCORR_error_" + dataset2: slpCorrErr,
                                             "metric_valueSTD_" + dataset2: slpStd,
                                             "metric_valueSTD_error_" + dataset2: slpStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=slp_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=slp_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if slpCorr is not None:
        slpCorr = 1 - slpCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(slpRmse), "line2": "metric value_error: " + str(slpRmseErr),
                      "line3": "metric value: " + str(slpCorr), "line4": "metric value_error: " + str(slpCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": slpRmse, "Rmse__value_error": slpRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": slpCorr, "Corr__value_error": slpCorrErr, "Corr__units": "", "Std__value": slpStd,
        "Std__value_error": slpStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSlpMapJja(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  slpfilemod, slpnamemod, slpareafilemod, slpareanamemod, slplandmaskfilemod, slplandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  slpfileobs, slpnameobs, slpareafileobs, slpareanameobs, slplandmaskfileobs, slplandmasknameobs,
                  sstbox, slpbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                  debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSlpMapJja() function computes sea level pressure anomalies pattern associated with ENSO on the globe.
    It is the regression of 'slpbox' slpA (sea level pressure anomalies) onto 'region_ev' sstA (sea surface temperature
    anomalies) during JJA (usually the regression of global slpA onto nino3.4 sstA)
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param slpfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SLP
    :param slpnamemod: string
        name of SLP variable (slp) in 'slpfilemod'
    :param slpareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP areacell
    :param slpareanamemod: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafilemod'
    :param slplandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP landmask
    :param slplandmasknamemod: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param slpfileobs: string
        path_to/filename of the file (NetCDF) of the observed SLP
    :param slpnameobs: string
        name of SLP variable (slp) in 'slpfileobs'
    :param slpareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP areacell
    :param slpareanameobs: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafileobs'
    :param slplandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP landmask
    :param slplandmasknameobs: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param slpbox: string
        name of box (e.g. 'global') for SLP
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSlpMapJja_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO JJA SLPA pattern"
    Units = "" if kwargs["normalization"] else "hPa/C"
    Method = "map of " + slpbox + " sea level pressure anomalies regressed onto " + region_ev + \
             " averaged SSTA during JJA"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSlpMapJja"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    slp_mod, slp_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod, name_area=slpareanamemod,
        file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    slp_obs, slp_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs, name_area=slpareanameobs,
        file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod_box, slp_mod, keyerror_mod3 = CheckTime(sst_mod_box, slp_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs_box, slp_obs, keyerror_obs3 = CheckTime(sst_obs_box, slp_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod_box.shape[0] / 12))
    yearN_obs = int(round(sst_obs_box.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod_box)
    actualtimebounds_obs = TimeBounds(sst_obs_box)

    slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 SLP in 'slpbox' are normalized / detrended / smoothed (running average) if applicable
        slp_mod, Method, keyerror_mod2 = PreProcessTS(
            slp_mod, Method, areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
        slp_obs, _, keyerror_obs2 = PreProcessTS(
            slp_obs, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
        del slp_mod_areacell, slp_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "JJA", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "JJA", compute_anom=True)
            slp_mod = SeasonalMean(slp_mod, "JJA", compute_anom=True) * 1e-2
            slp_obs = SeasonalMean(slp_obs, "JJA", compute_anom=True) * 1e-2
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                    "axes4": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(slp mod) " + str(slp_mod.shape), "shape4": "(slp obs) " + str(slp_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(slp mod) " + str(TimeBounds(slp_mod)), "time4": "(slp obs) " + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                slp_mod, slp_obs, Method = TwoVarRegrid(slp_mod, slp_obs, Method, region=slpbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod.getAxisList()]),
                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs.getAxisList()]),
                        "shape1": "(slp mod) " + str(slp_mod.shape), "shape2": "(slp obs) " + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            slp_mod_slope = LinearRegressionTsAgainstMap(slp_mod, enso_mod, return_stderr=False)
            slp_obs_slope = LinearRegressionTsAgainstMap(slp_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                              "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                              "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            slp_mod_slope, keyerror_mod = BasinMask(
                slp_mod_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            slp_obs_slope, keyerror_obs = BasinMask(
                slp_obs_slope, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_slope.getAxisList()]),
                                  "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_slope.getAxisList()]),
                                  "shape1": "(slp mod) " + str(slp_mod_slope.shape),
                                  "shape2": "(slp obs) " + str(slp_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                slpRmse, keyerror = RmsAxis(
                    slp_mod_slope, slp_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                slpRmseErr = None
                # Metric 2
                slpCorr = float(Correlation(slp_mod_slope, slp_obs_slope, axis="xy", centered=1, biased=1))
                slpCorrErr = None
                # Metric 3
                std_mod = Std(slp_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(slp_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                slpStd = float(std_mod) / float(std_obs)
                slpStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    slp_mod_land, slp_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        slpfilemod, slpnamemod, "pressure", metric, slpbox, file_area=slpareafilemod,
                        name_area=slpareanamemod, file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    slp_obs_land, slp_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        slpfileobs, slpnameobs, "pressure", metric, slpbox, file_area=slpareafileobs,
                        name_area=slpareanameobs, file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, slp_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, slp_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, slp_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, slp_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # slpocess
                        slp_mod_land, _, keyerror_mod = PreProcessTS(
                            slp_mod_land, "", areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
                        slp_obs_land, _, keyerror_obs = PreProcessTS(
                            slp_obs_land, "", areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
                        del slp_mod_areacell, slp_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            slp_mod_land = SeasonalMean(slp_mod_land, "JJA", compute_anom=True) * 1e-2
                            slp_obs_land = SeasonalMean(slp_obs_land, "JJA", compute_anom=True) * 1e-2
                            if debug is True:
                                dict_debug = {"axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                              "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                              "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                              "shape2": "(slp obs) " + str(slp_obs_land.shape),
                                              "time1": "(slp mod) " + str(TimeBounds(slp_mod_land)),
                                              "time2": "(slp obs) " + str(TimeBounds(slp_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                slp_mod_land, slp_obs_land, _ = TwoVarRegrid(
                                    slp_mod_land, slp_obs_land, "", region=slpbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land.getAxisList()]),
                                        "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land.getAxisList()]),
                                        "shape1": "(slp mod) " + str(slp_mod_land.shape),
                                        "shape2": "(slp obs) " + str(slp_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            slp_mod_land_slope = LinearRegressionTsAgainstMap(
                                slp_mod_land, enso_mod, return_stderr=False)
                            slp_obs_land_slope = LinearRegressionTsAgainstMap(
                                slp_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(slp mod) " + str([ax.id for ax in slp_mod_land_slope.getAxisList()]),
                                    "axes2": "(slp obs) " + str([ax.id for ax in slp_obs_land_slope.getAxisList()]),
                                    "shape1": "(slp mod) " + str(slp_mod_land_slope.shape),
                                    "shape2": "(slp obs) " + str(slp_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(slp_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(slp_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(slp_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(slp_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(slp_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(slp_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(slp_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(slp_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=slpbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "hPa"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + slpbox + " SLPA of La Nina events composite during JJA (before " +
                                        "events); Nina = " + region_ev + " SSTA < -" + my_thresh + " during " +
                                        season_ev + enso_method,
                                        "map of " + slpbox + " SLPA of El Nino events composite during JJA (before " +
                                        "events); Nino = " + region_ev + " SSTA > " + my_thresh + " during " +
                                        season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = slp_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = slp_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(slpbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(slpbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: slpRmse,
                                             "metric_valueRMSE_error_" + dataset2: slpRmseErr,
                                             "metric_valueCORR_" + dataset2: slpCorr,
                                             "metric_valueCORR_error_" + dataset2: slpCorrErr,
                                             "metric_valueSTD_" + dataset2: slpStd,
                                             "metric_valueSTD_error_" + dataset2: slpStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=slp_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=slp_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if slpCorr is not None:
        slpCorr = 1 - slpCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(slpRmse), "line2": "metric value_error: " + str(slpRmseErr),
                      "line3": "metric value: " + str(slpCorr), "line4": "metric value_error: " + str(slpCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": slpRmse, "Rmse__value_error": slpRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": slpCorr, "Corr__value_error": slpCorrErr, "Corr__units": "", "Std__value": slpStd,
        "Std__value_error": slpStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSstMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, tasbox,
               event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
               netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstMap() function computes surface temperature anomalies pattern associated with ENSO on the globe.
    It is the regression of 'tasbox' TASA (surface temperature anomalies) onto 'region_ev' averaged SSTA (sea surface
    temperature anomalies) during boreal winter (usually the regression of global TASA onto nino3.4 SSTA).
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in "sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param tasbox: string
        name of box (e.g. 'global') for TAS
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstMap_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return EnsoSstMapMetric: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO TASA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "C/C"
    Method = "map of " + tasbox + " surface temperature anomalies regressed onto " + region_ev + \
             " averaged SSTA during " + season_ev + enso_method3
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSstMap"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    tas_mod, tas_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tas_obs, tas_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tas_mod, keyerror_mod3 = CheckTime(sst_mod, tas_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tas_obs, keyerror_obs3 = CheckTime(sst_obs, tas_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    tasCorr, tasCorrErr, tasRmse, tasRmseErr, tasStd, tasStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
        kwargs["smoothing"] = smooth_ev
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        # 1.2 TAS in 'tasbox' are normalized / detrended / smoothed (running average) if applicable
        tas_mod, Method, keyerror_mod2 = PreProcessTS(
            tas_mod, Method, areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
        tas_obs, _, keyerror_obs2 = PreProcessTS(
            tas_obs, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
        kwargs["smoothing"] = smooth
        del mod_areacell, obs_areacell, tas_mod_areacell, tas_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
            tas_mod = SeasonalMean(tas_mod, season_ev, compute_anom=True)
            tas_obs = SeasonalMean(tas_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tas_mod, tas_obs, Method = TwoVarRegrid(
                    tas_mod, tas_obs, Method, region=tasbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {
                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                        "shape1": "(tas mod) " + str(tas_mod.shape), "shape2": "(tas obs) " + str(tas_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            tas_mod_slope = LinearRegressionTsAgainstMap(tas_mod, enso_mod, return_stderr=False)
            tas_obs_slope = LinearRegressionTsAgainstMap(tas_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                              "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                              "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            tas_mod_slope, keyerror_mod = BasinMask(
                tas_mod_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            tas_obs_slope, keyerror_obs = BasinMask(
                tas_obs_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                                  "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                                  "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                                  "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                tasRmse, keyerror = RmsAxis(
                    tas_mod_slope, tas_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                tasRmseErr = None
                # Metric 2
                tasCorr = float(Correlation(tas_mod_slope, tas_obs_slope, axis="xy", centered=1, biased=1))
                tasCorrErr = None
                # Metric 3
                std_mod = Std(tas_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(tas_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                tasStd = float(std_mod) / float(std_obs)
                tasStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    tas_mod_land, tas_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod,
                        name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    tas_obs_land, tas_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs,
                        name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, tas_mod_land, keyerror_mod2 = CheckTime(
                        sst_mod, tas_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, tas_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, tas_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                        kwargs["smoothing"] = smooth_ev
                        tas_mod_land, _, keyerror_mod = PreProcessTS(
                            tas_mod_land, "", areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
                        tas_obs_land, _, keyerror_obs = PreProcessTS(
                            tas_obs_land, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
                        kwargs["smoothing"] = smooth
                        del tas_mod_areacell, tas_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            tas_mod_land = SeasonalMean(tas_mod_land, season_ev, compute_anom=True)
                            tas_obs_land = SeasonalMean(tas_obs_land, season_ev, compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                tas_mod_land, ts_obs, _ = TwoVarRegrid(
                                    tas_mod_land, tas_obs_land, "", region=tasbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                        "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                        "shape2": "(tas obs) " + str(tas_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            tas_mod_land_slope = LinearRegressionTsAgainstMap(
                                tas_mod_land, enso_mod, return_stderr=False)
                            tas_obs_land_slope = LinearRegressionTsAgainstMap(
                                tas_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land_slope.getAxisList()]),
                                    "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land_slope.getAxisList()]),
                                    "shape1": "(tas mod) " + str(tas_mod_land_slope.shape),
                                    "shape2": "(tas obs) " + str(tas_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                        nino=True, compute_season=True, duration=length_ev)
                            ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                        nino=False, compute_season=True, duration=length_ev)
                            if debug is True:
                                dict_debug = {
                                    "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                    "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                    "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                    "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                            # samples
                            en_mod = Composite(tas_mod, en_years_mod, kwargs["frequency"])
                            ln_mod = Composite(tas_mod, ln_years_mod, kwargs["frequency"])
                            en_obs = Composite(tas_obs, en_years_obs, kwargs["frequency"])
                            ln_obs = Composite(tas_obs, ln_years_obs, kwargs["frequency"])
                            en_mod_land = Composite(tas_mod_land, en_years_mod, kwargs["frequency"])
                            ln_mod_land = Composite(tas_mod_land, ln_years_mod, kwargs["frequency"])
                            en_obs_land = Composite(tas_obs_land, en_years_obs, kwargs["frequency"])
                            ln_obs_land = Composite(tas_obs_land, ln_years_obs, kwargs["frequency"])
                            # mask Pacific
                            en_mod, keyerror_mod1 = BasinMask(
                                en_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_mod, keyerror_mod2 = BasinMask(
                                ln_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            en_obs, keyerror_obs1 = BasinMask(
                                en_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            ln_obs, keyerror_obs2 = BasinMask(
                                ln_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                            if keyerror is None:
                                my_units = "" if kwargs["normalization"] is True else "C"
                                # Metrics ENSO events global
                                dict_metric, dict_nc = dict(), dict()
                                nbr = 3
                                my_de = [
                                    "map of " + tasbox + " TASA of La Nina events composite during " + season_ev +
                                    "; Nina = " + region_ev + " SSTA < -" + my_thresh + " during " + season_ev +
                                    enso_method,
                                    "map of " + tasbox + " TASA of El Nino events composite during " + season_ev +
                                    "; Nino = " + region_ev + " SSTA > " + my_thresh + " during " + season_ev +
                                    enso_method]
                                for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                        zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                            [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, ev_name=evn, events1=ev1, events2=ev2, description=de)
                                    nbr += 2
                                list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                for ii, reg in enumerate(list_region):
                                    # select region
                                    dictreg = ReferenceRegions(reg)
                                    tmp1 = tas_mod_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    tmp2 = tas_obs_land_slope(
                                        longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                    dict_metric, dict_nc = fill_dict_axis(
                                        tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                        yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                        centered_rmse=centered_rmse, biased_rmse=biased_rmse, dict_metric=dict_metric,
                                        dict_nc=dict_nc, description=Method.split(", ")[0].replace(tasbox, reg))
                                    nbr += 2
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                [ln_years_obs, en_years_obs])):
                                        tmp1 = tab1(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = tab2(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg + "_" + evn,
                                            my_units, "xy", centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de.replace(tasbox, reg))
                                        nbr += 2
                                    del dictreg, tmp1, tmp2
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                         "description": Method.split(", ")[0]}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                         "description": Method.split(", ")[0]}
                                dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                         "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: tasRmse,
                                         "metric_valueRMSE_error_" + dataset2: tasRmseErr,
                                         "metric_valueCORR_" + dataset2: tasCorr,
                                         "metric_valueCORR_error_" + dataset2: tasCorrErr,
                                         "metric_valueSTD_" + dataset2: tasStd,
                                         "metric_valueSTD_error_" + dataset2: tasStdErr}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=tas_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=tas_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if tasCorr is not None:
        tasCorr = 1 - tasCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(tasRmse), "line2": "metric value_error: " + str(tasRmseErr),
                      "line3": "metric value: " + str(tasCorr), "line4": "metric value_error: " + str(tasCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": tasRmse, "Rmse__value_error": tasRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": tasCorr, "Corr__value_error": tasCorrErr, "Corr__units": "", "Std__value": tasStd,
        "Std__value_error": tasStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs,
        "ref": Ref, "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSstMapDjf(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  tasbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False,
                  netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstMapDjf() function computes surface temperature anomalies pattern associated with ENSO on the globe.
    It is the regression of 'tasbox' TASA (surface temperature anomalies) onto 'region_ev' averaged SSTA (sea surface
    temperature anomalies) during DJF (usually the regression of global TASA onto nino3.4 SSTA).
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param tasbox: string
        name of box (e.g. 'global') for TAS
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstMapDjf_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return EnsoSstMapMetric: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO DJF TASA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "C/C"
    Method = "map of " + tasbox + " surface temperature anomalies regressed onto " + region_ev + \
             " averaged SSTA during DJF"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSstMapDjf"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    tas_mod, tas_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tas_obs, tas_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tas_mod, keyerror_mod3 = CheckTime(sst_mod_box, tas_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tas_obs, keyerror_obs3 = CheckTime(sst_obs_box, tas_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    tasCorr, tasCorrErr, tasRmse, tasRmseErr, tasStd, tasStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 TAS in 'tasbox' are normalized / detrended / smoothed (running average) if applicable
        tas_mod, Method, keyerror_mod2 = PreProcessTS(
            tas_mod, Method, areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
        tas_obs, _, keyerror_obs2 = PreProcessTS(
            tas_obs, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
        del tas_mod_areacell, tas_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "DJF", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "DJF", compute_anom=True)
            tas_mod = SeasonalMean(tas_mod, "DJF", compute_anom=True)
            tas_obs = SeasonalMean(tas_obs, "DJF", compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tas_mod, tas_obs, Method = TwoVarRegrid(tas_mod, tas_obs, Method, region=tasbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                                  "axes2": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                                  "shape1": "(tas mod) " + str(tas_mod.shape),
                                  "shape2": "(tas obs) " + str(tas_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            tas_mod_slope = LinearRegressionTsAgainstMap(tas_mod, enso_mod, return_stderr=False)
            tas_obs_slope = LinearRegressionTsAgainstMap(tas_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                              "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                              "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            tas_mod_slope, keyerror_mod = BasinMask(
                tas_mod_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            tas_obs_slope, keyerror_obs = BasinMask(
                tas_obs_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {
                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                        "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                        "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                tasRmse, keyerror = RmsAxis(
                    tas_mod_slope, tas_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                tasRmseErr = None
                # Metric 2
                tasCorr = float(Correlation(tas_mod_slope, tas_obs_slope, axis="xy", centered=1, biased=1))
                tasCorrErr = None
                # Metric 3
                std_mod = Std(tas_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(tas_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                tasStd = float(std_mod) / float(std_obs)
                tasStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    tas_mod_land, tas_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod,
                        name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    tas_obs_land, tas_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs,
                        name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, tas_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, tas_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, tas_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, tas_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        tas_mod_land, _, keyerror_mod = PreProcessTS(
                            tas_mod_land, "", areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
                        tas_obs_land, _, keyerror_obs = PreProcessTS(
                            tas_obs_land, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
                        del tas_mod_areacell, tas_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            tas_mod_land = SeasonalMean(tas_mod_land, "DJF", compute_anom=True)
                            tas_obs_land = SeasonalMean(tas_obs_land, "DJF", compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                tas_mod_land, tas_obs_land, _ = TwoVarRegrid(
                                    tas_mod_land, tas_obs_land, "", region=tasbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                        "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                        "shape2": "(tas obs) " + str(tas_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            tas_mod_land_slope = LinearRegressionTsAgainstMap(
                                tas_mod_land, enso_mod, return_stderr=False)
                            tas_obs_land_slope = LinearRegressionTsAgainstMap(
                                tas_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land_slope.getAxisList()]),
                                    "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land_slope.getAxisList()]),
                                    "shape1": "(tas mod) " + str(tas_mod_land_slope.shape),
                                    "shape2": "(tas obs) " + str(tas_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(tas_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(tas_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(tas_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(tas_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(tas_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(tas_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(tas_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(tas_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "C"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + tasbox + " TASA of La Nina events composite during DJF; Nina = " +
                                        region_ev + " SSTA < -" + my_thresh + " during " + season_ev + enso_method,
                                        "map of " + tasbox + " TASA of El Nino events composite during DJF; Nino = " +
                                        region_ev + " SSTA > " + my_thresh + " during " + season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = tas_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = tas_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(tasbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(tasbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: tasRmse,
                                             "metric_valueRMSE_error_" + dataset2: tasRmseErr,
                                             "metric_valueCORR_" + dataset2: tasCorr,
                                             "metric_valueCORR_error_" + dataset2: tasCorrErr,
                                             "metric_valueSTD_" + dataset2: tasStd,
                                             "metric_valueSTD_error_" + dataset2: tasStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=tas_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=tas_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if tasCorr is not None:
        tasCorr = 1 - tasCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(tasRmse), "line2": "metric value_error: " + str(tasRmseErr),
                      "line3": "metric value: " + str(tasCorr), "line4": "metric value_error: " + str(tasCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": tasRmse, "Rmse__value_error": tasRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": tasCorr, "Corr__value_error": tasCorrErr, "Corr__units": "", "Std__value": tasStd,
        "Std__value_error": tasStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def EnsoSstMapJja(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  tasbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False,
                  netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The EnsoSstMapJja() function computes surface temperature anomalies pattern associated with ENSO on the globe.
    It is the regression of 'tasbox' TASA (surface temperature anomalies) onto 'region_ev' averaged SSTA (sea surface
    temperature anomalies) during JJA (usually the regression of global TASA onto nino3.4 SSTA).
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param tasbox: string
        name of box (e.g. 'global') for TAS
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'EnsoSstMapJja_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return EnsoSstMapMetric: dict
        name, value (rms [obs;model]), value_error, units, method, value2 (corr [obs;model]),
        value_error2, units2, value3 (std_model / std_obs), value_error3, units3, nyears_model, nyears_observations,
        time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    length_ev = event_definition["duration_min"]
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    smooth_ev = event_definition["smoothing"]
    thresh_ev = event_definition["threshold"]
    normalize = event_definition["normalization"]
    enso_method, enso_method1, enso_method2, enso_method3 = "", "", "", ""
    if isinstance(smooth_ev, dict) is True or isinstance(length_ev, int) is True:
        if isinstance(smooth_ev, dict) is True:
            enso_method1 = "SSTA smoothed with " + smooth_ev["window"] + "mo " + smooth_ev["method"] + \
                           " weighted running mean"
            enso_method3 = " (" + enso_method1 + ")"
        if isinstance(length_ev, int) is True:
            enso_method2 = "threshold met during at least " + str(length_ev) + " consecutive months"
        if isinstance(smooth_ev, dict) is True and isinstance(length_ev, int) is True:
            enso_method = " (" + enso_method1 + "; " + enso_method2 + ")"
        else:
            enso_method = " (" + enso_method1 + enso_method2 + ")"
    my_thresh = str(thresh_ev) + "std" if normalize is True else str(thresh_ev) + "C"

    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ENSO JJA TASA teleconnection pattern"
    Units = "" if kwargs["normalization"] else "C/C"
    Method = "map of " + tasbox + " surface temperature anomalies regressed onto " + region_ev + \
             " averaged SSTA during JJA"
    Ref = "Using CDAT regridding, correlation (centered and biased), std (centered and biased) and " + \
          "rms (uncentered and biased) calculation"
    metric = "EnsoSstMapJja"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod_box, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs_box, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
    tas_mod, tas_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tas_obs, tas_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tas_mod, keyerror_mod3 = CheckTime(sst_mod_box, tas_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tas_obs, keyerror_obs3 = CheckTime(sst_obs_box, tas_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    tasCorr, tasCorrErr, tasRmse, tasRmseErr, tasStd, tasStdErr = None, None, None, None, None, None
    dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}
    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3])
    if keyerror is None:
        # ------------------------------------------------
        # 1. Preprocess
        # ------------------------------------------------
        # 1.1 SST averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev,
            **kwargs)
        # 1.2 TAS in 'tasbox' are normalized / detrended / smoothed (running average) if applicable
        tas_mod, Method, keyerror_mod2 = PreProcessTS(
            tas_mod, Method, areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
        tas_obs, _, keyerror_obs2 = PreProcessTS(
            tas_obs, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
        del tas_mod_areacell, tas_obs_areacell
        keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        if keyerror is None:
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(sst_mod.shape), "shape2": "(sst obs) " + str(sst_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(sst_mod)), "time2": "(sst obs) " + str(TimeBounds(sst_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # ------------------------------------------------
            # 2. Seasonal mean and anomalies
            # ------------------------------------------------
            enso_mod = SeasonalMean(sst_mod, "JJA", compute_anom=True)
            enso_obs = SeasonalMean(sst_obs, "JJA", compute_anom=True)
            tas_mod = SeasonalMean(tas_mod, "JJA", compute_anom=True)
            tas_obs = SeasonalMean(tas_obs, "JJA", compute_anom=True)
            if debug is True:
                dict_debug = {
                    "axes1": "(sst mod) " + str([ax.id for ax in enso_mod.getAxisList()]),
                    "axes2": "(sst obs) " + str([ax.id for ax in enso_obs.getAxisList()]),
                    "axes3": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                    "axes4": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                    "shape1": "(sst mod) " + str(enso_mod.shape), "shape2": "(sst obs) " + str(enso_obs.shape),
                    "shape3": "(tas mod) " + str(tas_mod.shape), "shape4": "(tas obs) " + str(tas_obs.shape),
                    "time1": "(sst mod) " + str(TimeBounds(enso_mod)),
                    "time2": "(sst obs) " + str(TimeBounds(enso_obs)),
                    "time3": "(tas mod) " + str(TimeBounds(tas_mod)), "time4": "(tas obs) " + str(TimeBounds(tas_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after SeasonalMean", 15, **dict_debug)

            # ------------------------------------------------
            # 3. Regression map
            # ------------------------------------------------
            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tas_mod, tas_obs, Method = TwoVarRegrid(tas_mod, tas_obs, Method, region=tasbox, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod.getAxisList()]),
                                  "axes2": "(tas obs) " + str([ax.id for ax in tas_obs.getAxisList()]),
                                  "shape1": "(tas mod) " + str(tas_mod.shape),
                                  "shape2": "(tas obs) " + str(tas_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # regression
            tas_mod_slope = LinearRegressionTsAgainstMap(tas_mod, enso_mod, return_stderr=False)
            tas_obs_slope = LinearRegressionTsAgainstMap(tas_obs, enso_obs, return_stderr=False)
            if debug is True:
                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                              "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                              "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after LinearRegressionTsAgainstMap", 15, **dict_debug)

            # mask Pacific
            tas_mod_slope, keyerror_mod = BasinMask(
                tas_mod_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            tas_obs_slope, keyerror_obs = BasinMask(
                tas_obs_slope, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {
                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_slope.getAxisList()]),
                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_slope.getAxisList()]),
                        "shape1": "(tas mod) " + str(tas_mod_slope.shape),
                        "shape2": "(tas obs) " + str(tas_obs_slope.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after BasinMask", 15, **dict_debug)

                # Metric 1
                tasRmse, keyerror = RmsAxis(
                    tas_mod_slope, tas_obs_slope, axis="xy", centered=centered_rmse, biased=biased_rmse)
                tasRmseErr = None
                # Metric 2
                tasCorr = float(Correlation(tas_mod_slope, tas_obs_slope, axis="xy", centered=1, biased=1))
                tasCorrErr = None
                # Metric 3
                std_mod = Std(tas_mod_slope, weights=None, axis="xy", centered=1, biased=1)
                std_obs = Std(tas_obs_slope, weights=None, axis="xy", centered=1, biased=1)
                tasStd = float(std_mod) / float(std_obs)
                tasStdErr = None

                # Dive down diagnostic
                dive_down_diag = {"model": None, "observations": None, "axisLat": None, "axisLon": None}

                if netcdf is True:
                    # read
                    tas_mod_land, tas_mod_areacell, keyerror_mod1 = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, tasbox, file_area=sstareafilemod,
                        name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    tas_obs_land, tas_obs_areacell, keyerror_obs1 = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, tasbox, file_area=sstareafileobs,
                        name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs,
                        maskland=False, maskocean=True, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    sst_mod, tas_mod_land, keyerror_mod1 = CheckTime(
                        sst_mod, tas_mod_land, metric_name=metric, debug=debug, **kwargs)
                    sst_obs, tas_obs_land, keyerror_obs2 = CheckTime(
                        sst_obs, tas_obs_land, metric_name=metric, debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                    if keyerror is None:
                        # process
                        tas_mod_land, _, keyerror_mod = PreProcessTS(
                            tas_mod_land, "", areacell=tas_mod_areacell, compute_anom=False, region=tasbox, **kwargs)
                        tas_obs_land, _, keyerror_obs = PreProcessTS(
                            tas_obs_land, "", areacell=tas_obs_areacell, compute_anom=False, region=tasbox, **kwargs)
                        del tas_mod_areacell, tas_obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after PreProcessTS", 15, **dict_debug)
                            # anomalies
                            tas_mod_land = SeasonalMean(tas_mod_land, "JJA", compute_anom=True)
                            tas_obs_land = SeasonalMean(tas_obs_land, "JJA", compute_anom=True)
                            if debug is True:
                                dict_debug = {"axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                              "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                              "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                              "shape2": "(tas obs) " + str(tas_obs_land.shape),
                                              "time1": "(tas mod) " + str(TimeBounds(tas_mod_land)),
                                              "time2": "(tas obs) " + str(TimeBounds(tas_obs_land))}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after SeasonalMean", 15, **dict_debug)
                            # regridding
                            if isinstance(kwargs["regridding"], dict):
                                tas_mod_land, tas_obs_land, _ = TwoVarRegrid(
                                    tas_mod_land, tas_obs_land, "", region=tasbox, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {
                                        "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land.getAxisList()]),
                                        "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land.getAxisList()]),
                                        "shape1": "(tas mod) " + str(tas_mod_land.shape),
                                        "shape2": "(tas obs) " + str(tas_obs_land.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "divedown after TwoVarRegrid", 15, **dict_debug)
                            # regression
                            tas_mod_land_slope = LinearRegressionTsAgainstMap(
                                tas_mod_land, enso_mod, return_stderr=False)
                            tas_obs_land_slope = LinearRegressionTsAgainstMap(
                                tas_obs_land, enso_obs, return_stderr=False)
                            if debug is True:
                                dict_debug = {
                                    "axes1": "(tas mod) " + str([ax.id for ax in tas_mod_land_slope.getAxisList()]),
                                    "axes2": "(tas obs) " + str([ax.id for ax in tas_obs_land_slope.getAxisList()]),
                                    "shape1": "(tas mod) " + str(tas_mod_land_slope.shape),
                                    "shape2": "(tas obs) " + str(tas_obs_land_slope.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "divedown after LinearRegressionTsAgainstMap", 15, **dict_debug)
                            # ENSO events: SSTA > (<) "threshold" during "season" are considered as El Nino
                            # (La Nina) events
                            smooth = deepcopy(kwargs["smoothing"]) if "smoothing" in list(kwargs.keys()) else False
                            kwargs["smoothing"] = smooth_ev
                            sst_mod, _, keyerror_mod = PreProcessTS(
                                sst_mod_box, "", areacell=mod_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            sst_obs, _, keyerror_obs = PreProcessTS(
                                sst_obs_box, "", areacell=obs_areacell, average="horizontal", compute_anom=False,
                                region=region_ev, **kwargs)
                            kwargs["smoothing"] = smooth
                            del mod_areacell, obs_areacell
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                # Lists event years
                                en_years_mod = DetectEvents(sst_mod, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_mod = DetectEvents(sst_mod, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                en_years_obs = DetectEvents(sst_obs, season_ev, thresh_ev, normalization=normalize,
                                                            nino=True, compute_season=True, duration=length_ev)
                                ln_years_obs = DetectEvents(sst_obs, season_ev, -thresh_ev, normalization=normalize,
                                                            nino=False, compute_season=True, duration=length_ev)
                                if debug is True:
                                    dict_debug = {
                                        "nino1": "(mod) nbr(" + str(len(en_years_mod)) + "): " + str(en_years_mod),
                                        "nina1": "(mod) nbr(" + str(len(ln_years_mod)) + "): " + str(ln_years_mod),
                                        "nino2": "(obs) nbr(" + str(len(en_years_obs)) + "): " + str(en_years_obs),
                                        "nina2": "(obs) nbr(" + str(len(ln_years_obs)) + "): " + str(ln_years_obs)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)
                                # samples
                                en_mod = Composite(tas_mod, en_years_mod, kwargs["frequency"])
                                ln_mod = Composite(tas_mod, ln_years_mod, kwargs["frequency"])
                                en_obs = Composite(tas_obs, en_years_obs, kwargs["frequency"])
                                ln_obs = Composite(tas_obs, ln_years_obs, kwargs["frequency"])
                                en_mod_land = Composite(tas_mod_land, en_years_mod, kwargs["frequency"])
                                ln_mod_land = Composite(tas_mod_land, ln_years_mod, kwargs["frequency"])
                                en_obs_land = Composite(tas_obs_land, en_years_obs, kwargs["frequency"])
                                ln_obs_land = Composite(tas_obs_land, ln_years_obs, kwargs["frequency"])
                                # mask Pacific
                                en_mod, keyerror_mod1 = BasinMask(
                                    en_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_mod, keyerror_mod2 = BasinMask(
                                    ln_mod, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                en_obs, keyerror_obs1 = BasinMask(
                                    en_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                ln_obs, keyerror_obs2 = BasinMask(
                                    ln_obs, "pacific", box=tasbox, lat1=-15, lat2=15, latkey="between", debug=debug)
                                keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
                                if keyerror is None:
                                    my_units = "" if kwargs["normalization"] is True else "C"
                                    # Metrics ENSO events global
                                    dict_metric, dict_nc = dict(), dict()
                                    nbr = 3
                                    my_de = [
                                        "map of " + tasbox + " TASA of La Nina events composite during JJA (before " +
                                        "events); Nina = " + region_ev + " SSTA < -" + my_thresh + " during " +
                                        season_ev + enso_method,
                                        "map of " + tasbox + " TASA of El Nino events composite during JJA (before " +
                                        "events); Nino = " + region_ev + " SSTA > " + my_thresh + " during " +
                                        season_ev + enso_method]
                                    for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                            zip(["nina", "nino"], my_de, [ln_mod, en_mod], [ln_obs, en_obs],
                                                [ln_years_mod, en_years_mod], [ln_years_obs, en_years_obs])):
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tab1, tab2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map", my_units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc, ev_name=evn, events1=ev1,
                                            events2=ev2, description=de)
                                        nbr += 2
                                    list_region = ["africaSE", "americaN", "americaS", "asiaS", "oceania"]
                                    for ii, reg in enumerate(list_region):
                                        # select region
                                        dictreg = ReferenceRegions(reg)
                                        tmp1 = tas_mod_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        tmp2 = tas_obs_land_slope(
                                            longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                        dict_metric, dict_nc = fill_dict_axis(
                                            tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                            yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2], "map_" + reg, Units, "xy",
                                            centered_rmse=centered_rmse, biased_rmse=biased_rmse,
                                            dict_metric=dict_metric, dict_nc=dict_nc,
                                            description=Method.split(", ")[0].replace(tasbox, reg))
                                        nbr += 2
                                        for jj, (evn, de, tab1, tab2, ev1, ev2) in enumerate(
                                                zip(["nina", "nino"], my_de, [ln_mod_land, en_mod_land],
                                                    [ln_obs_land, en_obs_land], [ln_years_mod, en_years_mod],
                                                    [ln_years_obs, en_years_obs])):
                                            tmp1 = tab1(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            tmp2 = tab2(
                                                longitude=dictreg["longitude"], latitude=dictreg["latitude"])
                                            dict_metric, dict_nc = fill_dict_axis(
                                                tmp1, tmp2, dataset1, dataset2, actualtimebounds_mod,
                                                actualtimebounds_obs, yearN_mod, yearN_obs, nbr, ovar[(nbr - 1) // 2],
                                                "map_" + reg + "_" + evn, my_units, "xy", centered_rmse=centered_rmse,
                                                biased_rmse=biased_rmse, dict_metric=dict_metric, dict_nc=dict_nc,
                                                ev_name=evn, events1=ev1, events2=ev2,
                                                description=de.replace(tasbox, reg))
                                            nbr += 2
                                        del dictreg, tmp1, tmp2
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {
                                        "units": Units, "number_of_years_used": yearN_mod,
                                        "time_period": str(actualtimebounds_mod), "spatialSTD_" + dataset1: std_mod,
                                        "description": Method.split(", ")[0]}
                                    dict2 = {
                                        "units": Units, "number_of_years_used": yearN_obs,
                                        "time_period": str(actualtimebounds_obs), "spatialSTD_" + dataset2: std_obs,
                                        "description": Method.split(", ")[0]}
                                    dict3 = {"metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                             "frequency": kwargs["frequency"], "metric_valueRMSE_" + dataset2: tasRmse,
                                             "metric_valueRMSE_error_" + dataset2: tasRmseErr,
                                             "metric_valueCORR_" + dataset2: tasCorr,
                                             "metric_valueCORR_error_" + dataset2: tasCorrErr,
                                             "metric_valueSTD_" + dataset2: tasStd,
                                             "metric_valueSTD_error_" + dataset2: tasStdErr}
                                    dict3.update(dict_metric)
                                    SaveNetcdf(
                                        file_name, global_attributes=dict3,
                                        var1=tas_mod_slope, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                        var2=tas_obs_slope, var2_attributes=dict2, var2_name=ovar[0] + dataset2,
                                        **dict_nc)
                                    del dict1, dict2, dict3, dict_metric, dict_nc, file_name, list_region, my_de, nbr
    if tasCorr is not None:
        tasCorr = 1 - tasCorr
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(tasRmse), "line2": "metric value_error: " + str(tasRmseErr),
                      "line3": "metric value: " + str(tasCorr), "line4": "metric value_error: " + str(tasCorrErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "Rmse__value": tasRmse, "Rmse__value_error": tasRmseErr, "Rmse__units": Units, "method": Method,
        "Corr__value": tasCorr, "Corr__value_error": tasCorrErr, "Corr__units": "", "Std__value": tasStd,
        "Std__value_error": tasStdErr, "Std__units": Units + " / " + Units, "nyears_model": yearN_mod,
        "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag, "units": ""}
    return metric_output


def grad_lat_pr(prfile, prname, prareafile, prareaname, prlandmaskfile, prlandmaskname, prbox, dataset="", debug=False,
                netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The grad_lat_pr() function computes the difference between the precipitation in the eastern off-equatorial and
    equatorial Pacific.
    The definition of Cai et al. (2014; https://doi.org/10.1038/nclimate2100) is used.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon March  9 18:21:32 CET 2021

    Inputs:
    ------
    :param prfile: string
        path_to/filename of the file (NetCDF) of PR
    :param prname: string
        name of PR variable (e.g., pr, prec, precip) in 'prfile'
    :param prareafile: string
        path_to/filename of the file (NetCDF) of the areacell for PR
    :param prareaname: string
        name of areacell variable (areacella, areacello) in 'prareafile'
    :param prlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for PR
    :param prlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfile'
    :param prbox: list
        list of region names (['grad_off', 'grad_equ']) for PR
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'grad_lat_pr_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "meridional PR gradient in the eastern Pacific"
    lat = ReferenceRegions(prbox[0])['latitude']
    lat0 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat1 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(prbox[0])['longitude']
    lon0 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon1 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    lat = ReferenceRegions(prbox[0])['latitude']
    lat2 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat3 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(prbox[0])['longitude']
    lon2 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon3 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    units = "mm/day"
    method = str(prbox[0]) + " averaged (" + str(lat0[0]) + "-" + str(lat1[1]) + " ; " + str(lon0[0]) + "-" + \
             str(lon1[1]) + ") minus " + str(prbox[1]) + " averaged (" + str(lat2[0]) + "-" + str(lat3[1]) + " ; " + \
             str(lon2[0]) + "-" + str(lon3[1]) + ") PR"
    reference = "Using CDAT averager and std"
    metric = "grad_lat_pr"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    off, off_areacell, off_keyerror = Read_data_mask_area(
        prfile, prname, "precipitations", metric, prbox[0], file_area=prareafile, name_area=prareaname,
        file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    equ, equ_areacell, equ_keyerror = Read_data_mask_area(
        prfile, prname, "precipitations", metric, prbox[1], file_area=prareafile, name_area=prareaname,
        file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    n_year = int(round(off.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(off)

    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    keyerror = add_up_errors([equ_keyerror, off_keyerror])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        off, method, off_keyerror = PreProcessTS(
            off, method, areacell=off_areacell, average="horizontal", region=prbox[0], **kwargs)
        equ, _, equ_keyerror = PreProcessTS(
            equ, "", areacell=equ_areacell, average="horizontal", region=prbox[1], **kwargs)
        del equ_areacell, off_areacell
        keyerror = add_up_errors([equ_keyerror, off_keyerror])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(equ) " + str([ax.id for ax in equ.getAxisList()]),
                              "axes2": "(off) " + str([ax.id for ax in off.getAxisList()]),
                              "shape1": "(equ) " + str(equ.shape), "shape2": "(off) " + str(off.shape),
                              "time1": "(equ) " + str(TimeBounds(equ)), "time2": "(off) " + str(TimeBounds(off))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Compute monthly gradient
            grad = off - equ

            # Computes the standard deviation
            mv, keyerror = AverageTemporal(grad)
            if keyerror is None:
                mv = float(mv)

                # Standard Error of the Standard Deviation (function of nyears)
                mv_error = float(Std(grad)) / NUMPYsqrt(len(grad))

                # Dive down diagnostic
                dive_down_diag = {"value": None, "axis": None}

                if netcdf is True:
                    # additional diagnostic
                    # Read file and select the right region
                    pr_map, areacell, keyerror = Read_data_mask_area(
                        prfile, prname, "precipitations", metric, "equatorial_pacific_LatExt2", file_area=prareafile,
                        name_area=prareaname, file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True,
                        maskocean=False, debug=debug, **kwargs)
                    if keyerror is None:
                        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        pr_map, _, keyerror = PreProcessTS(
                            pr_map, "", areacell=areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del areacell
                        if keyerror is None:
                            # Regridding
                            if "regridding" not in list(kwargs.keys()) or \
                                    isinstance(kwargs["regridding"], dict) is False:
                                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf",
                                                        "regridMethod": "linear", "newgrid_name": "generic_1x1deg"}
                            pr_map = Regrid(pr_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                            # Zonal average
                            pr_lat = pr_map(longitude=ReferenceRegions(prbox[0])["longitude"])
                            pr_lat, keyerror = AverageZonal(pr_lat)
                            if keyerror is None:
                                # Supplementary metrics
                                std_lat = float(Std(pr_lat, weights=None, axis=0, centered=1, biased=1))
                                std_map = float(Std(pr_map, weights=None, axis="xy", centered=1, biased=1))
                                # Dive down diagnostic
                                dive_down_diag = {"value": ArrayToList(pr_lat), "axis": list(pr_lat.getAxis(0)[:])}
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {
                                    "units": units, "number_of_years_used": n_year, "time_period":
                                        str(actualtimebounds),
                                    "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_lat,
                                    "description": "mean PR across latitudes (zonal averaged [" + str(lon0[0]) + "-" +
                                                   str(lon1[1]) + "])"}
                                dict2 = {
                                    "units": units, "number_of_years_used": n_year,
                                    "time_period": str(actualtimebounds),
                                    "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_map,
                                    "description": "mean PR map"}
                                dict3 = {"metric_name": name, "metric_method": method, "metric_reference": reference,
                                         "frequency": kwargs["frequency"]}
                                SaveNetcdf(file_name, global_attributes=dict3,
                                           var1=pr_lat, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                                           var2=pr_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
                                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": n_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": reference, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def grad_lat_sst(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, dataset="",
                 debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The grad_lat_sst() function computes the difference between the temperature in the eastern off-equatorial and
    equatorial Pacific.
    The definition of Cai et al. (2014; https://doi.org/10.1038/nclimate2100) is used.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon March  9 18:21:32 CET 2021

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: list
        list of region names (['grad_off', 'grad_equ']) for SST
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'grad_lat_sst_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "meridional SST gradient in the eastern Pacific"
    lat = ReferenceRegions(sstbox[0])['latitude']
    lat0 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat1 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sstbox[0])['longitude']
    lon0 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon1 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    lat = ReferenceRegions(sstbox[0])['latitude']
    lat2 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat3 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sstbox[0])['longitude']
    lon2 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon3 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    units = "C"
    method = str(sstbox[0]) + " averaged (" + str(lat0[0]) + "-" + str(lat1[1]) + " ; " + str(lon0[0]) + "-" + \
             str(lon1[1]) + ") minus " + str(sstbox[1]) + " averaged (" + str(lat2[0]) + "-" + str(lat3[1]) + " ; " + \
             str(lon2[0]) + "-" + str(lon3[1]) + ") SST"
    reference = "Using CDAT averager and std"
    metric = "grad_lat_sst"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    off, off_areacell, off_keyerror = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox[0], file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    equ, equ_areacell, equ_keyerror = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox[1], file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    n_year = int(round(off.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(off)

    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    keyerror = add_up_errors([equ_keyerror, off_keyerror])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        off, method, off_keyerror = PreProcessTS(
            off, method, areacell=off_areacell, average="horizontal", region=sstbox[0], **kwargs)
        equ, _, equ_keyerror = PreProcessTS(
            equ, "", areacell=equ_areacell, average="horizontal", region=sstbox[1], **kwargs)
        del equ_areacell, off_areacell
        keyerror = add_up_errors([equ_keyerror, off_keyerror])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(equ) " + str([ax.id for ax in equ.getAxisList()]),
                              "axes2": "(off) " + str([ax.id for ax in off.getAxisList()]),
                              "shape1": "(equ) " + str(equ.shape), "shape2": "(off) " + str(off.shape),
                              "time1": "(equ) " + str(TimeBounds(equ)), "time2": "(off) " + str(TimeBounds(off))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Compute monthly gradient
            grad = off - equ

            # Computes the standard deviation
            mv, keyerror = AverageTemporal(grad)
            if keyerror is None:
                mv = float(mv)

                # Standard Error of the Standard Deviation (function of nyears)
                mv_error = float(Std(grad)) / NUMPYsqrt(len(grad))

                # Dive down diagnostic
                dive_down_diag = {"value": None, "axis": None}

                if netcdf is True:
                    # additional diagnostic
                    # Read file and select the right region
                    sst_map, areacell, keyerror = Read_data_mask_area(
                        sstfile, sstname, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafile,
                        name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                        maskocean=False, debug=debug, **kwargs)
                    if keyerror is None:
                        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        sst_map, _, keyerror = PreProcessTS(
                            sst_map, "", areacell=areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del areacell
                        if keyerror is None:
                            # Regridding
                            if "regridding" not in list(kwargs.keys()) or \
                                    isinstance(kwargs["regridding"], dict) is False:
                                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf",
                                                        "regridMethod": "linear", "newgrid_name": "generic_1x1deg"}
                            sst_map = Regrid(sst_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                            # Zonal average
                            sst_lat = sst_map(longitude=ReferenceRegions(sstbox[0])["longitude"])
                            sst_lat, keyerror = AverageZonal(sst_lat)
                            if keyerror is None:
                                # Supplementary metrics
                                std_lat = float(Std(sst_lat, weights=None, axis=0, centered=1, biased=1))
                                std_map = float(Std(sst_map, weights=None, axis="xy", centered=1, biased=1))
                                # Dive down diagnostic
                                dive_down_diag = {"value": ArrayToList(sst_lat), "axis": list(sst_lat.getAxis(0)[:])}
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {
                                    "units": units, "number_of_years_used": n_year,
                                    "time_period": str(actualtimebounds),
                                    "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_lat,
                                    "description": "mean SST across latitudes (zonal averaged [" + str(lon0[0]) + "-" +
                                                   str(lon1[1]) + "])"}
                                dict2 = {
                                    "units": units, "number_of_years_used": n_year,
                                    "time_period": str(actualtimebounds),
                                    "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_map,
                                    "description": "mean SST map"}
                                dict3 = {"metric_name": name, "metric_method": method, "metric_reference": reference,
                                         "frequency": kwargs["frequency"]}
                                SaveNetcdf(file_name, global_attributes=dict3,
                                           var1=sst_lat, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                                           var2=sst_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
                                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": n_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": reference, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def grad_lon_pr(prfile, prname, prareafile, prareaname, prlandmaskfile, prlandmaskname, prbox, dataset="", debug=False,
                netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The grad_lon_pr() function computes the difference between the precipitation in the western and eastern tropical
    Pacific.
    The definition of Burls and Fedorov (2014; https://doi.org/10.1175/JCLI-D-13-00255.1) is used.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon March  9 03:30:12 CET 2021

    Inputs:
    ------
    :param prfile: string
        path_to/filename of the file (NetCDF) of PR
    :param prname: string
        name of PR variable (e.g., pr, prec, precip) in 'prfile'
    :param prareafile: string
        path_to/filename of the file (NetCDF) of the areacell for PR
    :param prareaname: string
        name of areacell variable (areacella, areacello) in 'prareafile'
    :param prlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for PR
    :param prlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfile'
    :param prbox: list
        list of region names (['grad_west', 'grad_east']) for PR
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'grad_lon_pr_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "zonal PR gradient in the tropical Pacific"
    lat = ReferenceRegions(prbox[0])['latitude']
    lat0 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat1 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(prbox[0])['longitude']
    lon0 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon1 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    lat = ReferenceRegions(prbox[0])['latitude']
    lat2 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat3 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(prbox[0])['longitude']
    lon2 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon3 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    units = "mm/day"
    method = str(prbox[0]) + " averaged (" + str(lat0[0]) + "-" + str(lat1[1]) + " ; " + str(lon0[0]) + "-" + \
             str(lon1[1]) + ") minus " + str(prbox[1]) + " averaged (" + str(lat2[0]) + "-" + str(lat3[1]) + " ; " + \
             str(lon2[0]) + "-" + str(lon3[1]) + ") PR"
    reference = "Using CDAT averager and std"
    metric = "grad_lon_pr"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    west, west_areacell, west_keyerror = Read_data_mask_area(
        prfile, prname, "precipitations", metric, prbox[0], file_area=prareafile, name_area=prareaname,
        file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    east, east_areacell, east_keyerror = Read_data_mask_area(
        prfile, prname, "precipitations", metric, prbox[1], file_area=prareafile, name_area=prareaname,
        file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    n_year = int(round(west.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(west)

    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    keyerror = add_up_errors([east_keyerror, west_keyerror])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        west, method, west_keyerror = PreProcessTS(
            west, method, areacell=west_areacell, average="horizontal", region=prbox[0], **kwargs)
        east, _, east_keyerror = PreProcessTS(
            east, "", areacell=east_areacell, average="horizontal", region=prbox[1], **kwargs)
        del east_areacell, west_areacell
        keyerror = add_up_errors([east_keyerror, west_keyerror])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(east) " + str([ax.id for ax in east.getAxisList()]),
                              "axes2": "(west) " + str([ax.id for ax in west.getAxisList()]),
                              "shape1": "(east) " + str(east.shape), "shape2": "(west) " + str(west.shape),
                              "time1": "(east) " + str(TimeBounds(east)), "time2": "(west) " + str(TimeBounds(west))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Compute monthly gradient
            grad = west - east

            # Computes the standard deviation
            mv, keyerror = AverageTemporal(grad)
            if keyerror is None:
                mv = float(mv)

                # Standard Error of the Standard Deviation (function of nyears)
                mv_error = float(Std(grad)) / NUMPYsqrt(len(grad))

                # Dive down diagnostic
                dive_down_diag = {"value": None, "axis": None}

                if netcdf is True:
                    # additional diagnostic
                    # Read file and select the right region
                    pr_map, areacell, keyerror = Read_data_mask_area(
                        prfile, prname, "precipitations", metric, "equatorial_pacific_LatExt2", file_area=prareafile,
                        name_area=prareaname, file_mask=prlandmaskfile, name_mask=prlandmaskname, maskland=True,
                        maskocean=False, debug=debug, **kwargs)
                    if keyerror is None:
                        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        pr_map, _, keyerror = PreProcessTS(
                            pr_map, "", areacell=areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del areacell
                        if keyerror is None:
                            # Regridding
                            if "regridding" not in list(kwargs.keys()) or \
                                    isinstance(kwargs["regridding"], dict) is False:
                                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf",
                                                        "regridMethod": "linear", "newgrid_name": "generic_1x1deg"}
                            pr_map = Regrid(pr_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                            # Meridional average
                            pr_lon = pr_map(latitude=ReferenceRegions(prbox[0])["latitude"])
                            pr_lon, keyerror = AverageMeridional(pr_lon)
                            if keyerror is None:
                                # Supplementary metrics
                                std_lon = float(Std(pr_lon, weights=None, axis=0, centered=1, biased=1))
                                std_map = float(Std(pr_map, weights=None, axis="xy", centered=1, biased=1))
                                # Dive down diagnostic
                                dive_down_diag = {"value": ArrayToList(pr_lon), "axis": list(pr_lon.getAxis(0)[:])}
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {
                                    "units": units, "number_of_years_used": n_year,
                                    "time_period": str(actualtimebounds),
                                    "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_lon,
                                    "description": "mean PR across longitudes (meridional averaged [" + str(lat0[0]) +
                                                   "-" + str(lat1[1]) + "])"}
                                dict2 = {
                                    "units": units, "number_of_years_used": n_year,
                                    "time_period": str(actualtimebounds),
                                    "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_map,
                                    "description": "mean PR map"}
                                dict3 = {"metric_name": name, "metric_method": method, "metric_reference": reference,
                                         "frequency": kwargs["frequency"]}
                                SaveNetcdf(file_name, global_attributes=dict3,
                                           var1=pr_lon, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                                           var2=pr_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
                                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": n_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": reference, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def grad_lon_sst(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, sstbox, dataset="",
                 debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The grad_lon_sst() function computes the difference between the temperature in the western and eastern tropical
    Pacific.
    The definition of Burls and Fedorov (2014; https://doi.org/10.1175/JCLI-D-13-00255.1) is used.

    Author:	Yann Planton : yann.planton@locean-ipsl.upmc.fr

    Created on Mon March  9 03:30:12 CET 2021

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param sstbox: list
        list of region names (['grad_west', 'grad_east']) for SST
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'grad_lon_sst_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears, time_frequency, time_period, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from CDAT library

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    name = "zonal SST gradient in the tropical Pacific"
    lat = ReferenceRegions(sstbox[0])['latitude']
    lat0 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat1 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sstbox[0])['longitude']
    lon0 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon1 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    lat = ReferenceRegions(sstbox[0])['latitude']
    lat2 = str(int(round(abs(lat[0])))) + "S" if lat[0] < 0 else str(int(round(lat[0]))) + "N"
    lat3 = str(int(round(abs(lat[1])))) + "S" if lat[0] < 0 else str(int(round(lat[1]))) + "N"
    lon = ReferenceRegions(sstbox[0])['longitude']
    lon2 = str(int(round(lon[0]))) if lon[0] == 180 else (str(int(round(lon[0]))) + "E" if lon[0] < 180 else
                                                          str(int(round(abs(lon[0] - 360)))) + "W")
    lon3 = str(int(round(lon[1]))) if lon[1] == 180 else (str(int(round(lon[1]))) + "E" if lon[1] < 180 else
                                                          str(int(round(abs(lon[1] - 360)))) + "W")
    units = "C"
    method = str(sstbox[0]) + " averaged (" + str(lat0[0]) + "-" + str(lat1[1]) + " ; " + str(lon0[0]) + "-" + \
             str(lon1[1]) + ") minus " + str(sstbox[1]) + " averaged (" + str(lat2[0]) + "-" + str(lat3[1]) + " ; " + \
             str(lon2[0]) + "-" + str(lon3[1]) + ") SST"
    reference = "Using CDAT averager and std"
    metric = "grad_lon_sst"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    west, west_areacell, west_keyerror = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox[0], file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    east, east_areacell, east_keyerror = Read_data_mask_area(
        sstfile, sstname, "temperature", metric, sstbox[1], file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    n_year = int(round(west.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(west)

    mv, mv_error, dive_down_diag = None, None, {"value": None, "axis": None}
    keyerror = add_up_errors([east_keyerror, west_keyerror])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        west, method, west_keyerror = PreProcessTS(
            west, method, areacell=west_areacell, average="horizontal", region=sstbox[0], **kwargs)
        east, _, east_keyerror = PreProcessTS(
            east, "", areacell=east_areacell, average="horizontal", region=sstbox[1], **kwargs)
        del east_areacell, west_areacell
        keyerror = add_up_errors([east_keyerror, west_keyerror])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(east) " + str([ax.id for ax in east.getAxisList()]),
                              "axes2": "(west) " + str([ax.id for ax in west.getAxisList()]),
                              "shape1": "(east) " + str(east.shape), "shape2": "(west) " + str(west.shape),
                              "time1": "(east) " + str(TimeBounds(east)), "time2": "(west) " + str(TimeBounds(west))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # Compute monthly gradient
            grad = west - east

            # Computes the standard deviation
            mv, keyerror = AverageTemporal(grad)
            if keyerror is None:
                mv = float(mv)

                # Standard Error of the Standard Deviation (function of nyears)
                mv_error = float(Std(grad)) / NUMPYsqrt(len(grad))

                # Dive down diagnostic
                dive_down_diag = {"value": None, "axis": None}

                if netcdf is True:
                    # additional diagnostic
                    # Read file and select the right region
                    sst_map, areacell, keyerror = Read_data_mask_area(
                        sstfile, sstname, "temperature", metric, "equatorial_pacific_LatExt2", file_area=sstareafile,
                        name_area=sstareaname, file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True,
                        maskocean=False, debug=debug, **kwargs)
                    if keyerror is None:
                        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        sst_map, _, keyerror = PreProcessTS(
                            sst_map, "", areacell=areacell, average="time", compute_anom=False,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del areacell
                        if keyerror is None:
                            # Regridding
                            if "regridding" not in list(kwargs.keys()) or \
                                    isinstance(kwargs["regridding"], dict) is False:
                                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf",
                                                        "regridMethod": "linear", "newgrid_name": "generic_1x1deg"}
                            sst_map = Regrid(sst_map, None, region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                            # Meridional average
                            sst_lon = sst_map(latitude=ReferenceRegions(sstbox[0])["latitude"])
                            sst_lon, keyerror = AverageMeridional(sst_lon)
                            if keyerror is None:
                                # Supplementary metrics
                                std_lon = float(Std(sst_lon, weights=None, axis=0, centered=1, biased=1))
                                std_map = float(Std(sst_map, weights=None, axis="xy", centered=1, biased=1))
                                # Dive down diagnostic
                                dive_down_diag = {"value": ArrayToList(sst_lon), "axis": list(sst_lon.getAxis(0)[:])}
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {
                                    "units": units, "number_of_years_used": n_year,
                                    "time_period": str(actualtimebounds),
                                    "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_lon,
                                    "description": "mean SST across longitudes (meridional averaged [" + str(lat0[0]) +
                                                   "-" + str(lat1[1]) + "])"}
                                dict2 = {
                                    "units": units, "number_of_years_used": n_year,
                                    "time_period": str(actualtimebounds),
                                    "diagnostic_value": mv, "diagnostic_value_error": mv_error, "arraySTD": std_map,
                                    "description": "mean SST map"}
                                dict3 = {"metric_name": name, "metric_method": method, "metric_reference": reference,
                                         "frequency": kwargs["frequency"]}
                                SaveNetcdf(file_name, global_attributes=dict3,
                                           var1=sst_lon, var1_attributes=dict1, var1_name=ovar[0] + dataset,
                                           var2=sst_map, var2_attributes=dict2, var2_name=ovar[1] + dataset)
                                del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": name, "value": mv, "value_error": mv_error, "units": units, "method": method, "nyears": n_year,
        "time_frequency": kwargs["frequency"], "time_period": actualtimebounds, "ref": reference, "keyerror": keyerror,
        "dive_down_diag": dive_down_diag}
    return metric_output


def NinaPrMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod, prfilemod,
              prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs, sstnameobs,
              sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs, prnameobs,
              prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox, event_definition,
              centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False, netcdf=False, netcdf_name='',
              metname='', **kwargs):
    """
    The NinaPrMap() function computes a precipitation anomalies composite of during the peak of La Nina events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA < 'threshold' during 'season' are considered as La Nina events
    Then the PRA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param prbox: string
        name of box (e.g. 'global') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaPrMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina PRA Composite'
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + \
             ', Nina PRA Composited'
    Units = '' if kwargs['normalization'] else 'mm/day'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinaPrMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, 'precipitations', metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, 'precipitations', metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, pr_mod, keyerror_mod3 = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, pr_obs, keyerror_obs3 = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, '', areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        del mod_areacell, obs_areacell, pr_mod_areacell, pr_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1 is not None) or \
                (keyerror_mod2 is not None or keyerror_obs2 is not None):
            prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod pr) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes4': '(obs pr) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod pr) ' + str(pr_mod.shape), 'shape4': '(obs pr) ' + str(pr_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod pr) ' + str(TimeBounds(pr_mod)), 'time4': '(obs pr) ' + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite PRA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            pr_mod = SeasonalMean(pr_mod, season_ev, compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(pr_mod)), 'time2': '(obs) ' + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=prbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            pr_mod = Composite(pr_mod, event_years_mod, kwargs['frequency'])
            pr_obs = Composite(pr_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            pr_mod, keyerror_mod = BasinMask(
                pr_mod, 'pacific', box=prbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            pr_obs, keyerror_obs = BasinMask(
                pr_obs, 'pacific', box=prbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                prRmse, keyerror = RmsAxis(pr_mod, pr_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                prRmseErr = None
                # Metric 2
                prCorr = float(Correlation(pr_mod, pr_obs, axis='xy', centered=1, biased=1))
                prCorrErr = None
                # Metric 3
                std_mod = Std(pr_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(pr_obs, weights=None, axis='xy', centered=1, biased=1)
                prStd = float(std_mod) / float(std_obs)
                prStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}

                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {
                        'units': Units, 'number_of_years_used': yearN_mod, 'time_period': str(actualtimebounds_mod),
                        'nina_years': str(event_years_mod)}
                    dict2 = {
                        'units': Units, 'number_of_years_used': yearN_obs, 'time_period': str(actualtimebounds_obs),
                        'nina_years': str(event_years_obs)}
                    dict3 = {
                        'metric_name': Name, 'metric_valueRMSE_' + dataset2: prRmse,
                        'metric_valueRMSE_error_' + dataset2: prRmseErr, 'metric_valueCORR_' + dataset2: prCorr,
                        'metric_valueCORR_error_' + dataset2: prCorrErr, 'metric_valueSTD_' + dataset2: prStd,
                        'metric_valueCORR_error_' + dataset2: prStdErr, 'metric_method': Method,
                        'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(
                        file_name, var1=pr_mod, var1_attributes=dict1, var1_name='prComp_map__' + dataset1, var2=pr_obs,
                        var2_attributes=dict2, var2_name='prComp_map__' + dataset2, global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if prCorr is not None:
        prCorr = 1 - prCorr
    # Create output
    NinaPrMapMetric = {
        'name': Name, 'Rmse__value': prRmse, 'Rmse__value_error': prRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': prCorr, 'Corr__value_error': prCorrErr, 'Corr__units': '', 'Std__value': prStd,
        'Std__value_error': prStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinaPrMapMetric


def NinaSstDiv(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
               dataset='', debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSstDiv() function computes a zonal composite of La Nina events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA < 'threshold' during 'season' are considered as La Nina events
        2.) diversity of the zonal location of the minimum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the minimum SSTA for each selected event
            2.3) compute the percentage of EP events (minimum SSTA eastward of the given threshold)

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param treshold_ep_ev: float, optional
        see EnsoToolsLib.percentage_val_eastward
        longitude, in degree east, of the westward boundary of eastern Pacific event
        default value is -140°E (i.e., 140°W)
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaDivMetric: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, ref, keyerror,
        dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'treshold_ep_ev',
                    'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina Diversity (percentage of eastern Pacific La Nina)'
    lat = ReferenceRegions(box)['latitude']
    lon = ReferenceRegions(box)['longitude']
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' + \
             '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']), westward boundary of EP events' + \
             str(kwargs['treshold_ep_ev']) + 'E'
    Units = '%'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinaSstDiv'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst, areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    if keyerror is not None:
        ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, _, keyerror = PreProcessTS(
            sst, '', areacell=areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del areacell
        if keyerror is not None:
            ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
        else:
            if debug is True:
                dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                              'shape1': '(sst) ' + str(sst.shape), 'time1': '(sst) ' + str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years = DetectEvents(sst, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': 'nbr(' + str(len(event_years)) + '): ' + str(event_years)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the minimum SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst, areacell, keyerror = Read_data_mask_area(
                sstfile, sstname, 'temperature', metric, box, file_area=sstareafile, name_area=sstareaname,
                file_mask=sstlandmaskfile, name_mask=sstlandmaskfile, maskland=True, maskocean=False, debug=debug,
                **kwargs)
            if keyerror is not None:
                ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst, Method, keyerror = PreProcessTS(
                    sst, Method, areacell=areacell, average=False, compute_anom=False, region=box, **kwargs)
                del areacell
                if keyerror is not None:
                    ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                      'shape1': '(sst) ' + str(sst.shape), 'time1': '(sst) ' + str(TimeBounds(sst))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst = SeasonalMean(sst, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                      'shape1': '(sst) ' + str(sst.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder', 'regridTool',
                                      'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst = Regrid(sst, None, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                          'shape1': '(sst) ' + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst, keyerror = AverageMeridional(sst)
                    if keyerror is not None:
                        ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                          'shape1': '(sst) ' + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sample = Event_selection(sst, kwargs['frequency'], list_event_years=event_years)

                        # 2.2 find the zonal position of the minimum SSTA for each selected event
                        lon_sstmax = FindXYMinMaxInTs(
                            sample, return_val='mini', smooth=True, axis=0, window=5, method='triangle')
                        if debug is True:
                            dict_debug = {'line1': 'longitude of the minimum SSTA: ' + str(lon_sstmax)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)

                        # 2.3 compute the percentage of EP events (minimum SSTA eastward of the given threshold)
                        ep_event, keyerror = percentage_val_eastward(
                            lon_sstmax, metric, box, threshold=kwargs['treshold_ep_ev'])
                        ep_event = float(ep_event)

                        if keyerror is not None:
                            StdErr, dive_down_diag = None, {'value': None, 'axis': None}
                        else:
                            # Standard Error of the Standard Deviation (function of nyears)
                            StdErr = None

                            # Dive down diagnostic
                            dive_down_diag = {'value': ArrayToList(lon_sstmax), 'axis': list(lon_sstmax.getAxis(0)[:])}

                            if netcdf is True:
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {'units': 'longitude (E)', 'number_of_years_used': yearN,
                                         'time_period': str(actualtimebounds), 'nina_years': str(event_years),
                                         'diagnostic_value_' + dataset: ep_event,
                                         'diagnostic_value_error_' + dataset: StdErr}
                                dict2 = {'metric_name': Name, 'metric_method': Method, 'metric_reference': Ref,
                                         'frequency': kwargs['frequency']}
                                SaveNetcdf(file_name, var1=lon_sstmax, var1_attributes=dict1,
                                           var1_name='Nina_lon_pos_minSSTA__' + dataset, global_attributes=dict2)
                                del dict1, dict2
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(ep_event), 'line2': 'metric value_error: ' + str(StdErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinaDivMetric = {
        'name': Name, 'value': ep_event, 'value_error': StdErr, 'units': Units, 'method': Method, 'nyears': yearN,
        'events': event_years, 'time_frequency': kwargs['frequency'], 'time_period': actualtimebounds, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinaDivMetric


def NinaSstDivRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
                   netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSstDivRmse() function computes a zonal minimum of La Nina events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA < 'threshold' during 'season' are considered as La Nina events
        2.) diversity of the zonal location of the minimum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the minimum SSTA for each selected event and compute a pdf

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaDivMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyword, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'PDF of zonal min(SSTA) during La Nina'
    lat = ReferenceRegions(box)['latitude']
    lon = ReferenceRegions(box)['longitude']
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' \
             + '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']'
    Units = 'density'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinaSstDivRmse'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axis': None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                              'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the minimum SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, 'temperature', metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
                file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
            sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, 'temperature', metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
                file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
            if keyerror_mod is not None or keyerror_obs is not None:
                pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst_mod, Method, keyerror_mod = PreProcessTS(
                    sst_mod, Method, areacell=mod_areacell, average=False, compute_anom=False, region=box, **kwargs)
                sst_obs, _, keyerror_obs = PreProcessTS(
                    sst_obs, '', areacell=obs_areacell, average=False, compute_anom=False, region=box, **kwargs)
                del mod_areacell, obs_areacell
                if keyerror_mod is not None or keyerror_obs is not None:
                    pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                    dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                                      'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                                      'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
                    sst_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name',
                                      'regridder', 'regridTool', 'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst_mod, sst_obs, Method = TwoVarRegrid(
                            sst_mod, sst_obs, Method, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst_mod, keyerror_mod = AverageMeridional(sst_mod)
                    sst_obs, keyerror_obs = AverageMeridional(sst_obs)
                    if keyerror_mod is not None or keyerror_obs is not None:
                        pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sample_mod = Event_selection(sst_mod, kwargs['frequency'], list_event_years=event_years_mod)
                        sample_obs = Event_selection(sst_obs, kwargs['frequency'], list_event_years=event_years_obs)

                        # 2.2 find the zonal position of the minimum SSTA for each selected event and compute a pdf
                        # longitude of the minimum SSTA for each selected event
                        lon_min_mod = FindXYMinMaxInTs(
                            sample_mod, return_val='mini', smooth=True, axis=0, window=5, method='triangle')
                        lon_min_obs = FindXYMinMaxInTs(
                            sample_obs, return_val='mini', smooth=True, axis=0, window=5, method='triangle')
                        if debug is True:
                            dict_debug = {'line1': '(mod) longitude  of the minimum SSTA: ' + str(lon_min_mod),
                                          'line2': '(obs) longitude  of the minimum SSTA: ' + str(lon_min_obs)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)

                        # compute PDFs
                        if debug is True:
                            dict_debug = {'line1': 'lon ' + str(lon) + '  ;  nbr_bins old = ' +
                                                   str((lon[1] - lon[0]) / 10) + '  ;  nbr_bins new = ' +
                                                   str(int(round((lon[1] - lon[0]) / 10)))}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'before ComputePDF', 15, **dict_debug)
                        pdf_mod = ComputePDF(lon_min_mod, nbr_bins=int(round((lon[1] - lon[0]) / 10)), interval=lon,
                                             axis_name='longitude')
                        pdf_obs = ComputePDF(lon_min_obs, nbr_bins=int(round((lon[1] - lon[0]) / 10)), interval=lon,
                                             axis_name='longitude')

                        # Computes the root mean square difference
                        pdfRmse, keyerror = RmsZonal(pdf_mod, pdf_obs, centered=centered_rmse, biased=biased_rmse)

                        # Error on the metric
                        pdfRmseErr = None

                        # Dive down diagnostic
                        dive_down_diag = {'model': ArrayToList(pdf_mod), 'observations': ArrayToList(pdf_obs),
                                          'axis': list(pdf_mod.getAxis(0)[:])}
                        if netcdf is True:
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod), 'nina_years': str(event_years_mod)}
                            dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs), 'nina_years': str(event_years_obs)}
                            dict3 = {'metric_name': Name, 'metric_value_' + dataset2: pdfRmse,
                                     'metric_value_error_' + dataset2: pdfRmseErr, 'metric_method': Method,
                                     'metric_reference': Ref, 'frequency': kwargs['frequency']}
                            SaveNetcdf(file_name, var1=pdf_mod, var1_attributes=dict1, var1_name='pdf__' + dataset1,
                                       var2=pdf_obs, var2_attributes=dict2, var2_name='pdf__' + dataset2,
                                       global_attributes=dict3)
                            del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(pdfRmse), 'line2': 'metric value_error: ' + str(pdfRmseErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinaDivMetric = {
        'name': Name, 'value': pdfRmse, 'value_error': pdfRmseErr, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'events_model': event_years_mod,
        'events_observations': event_years_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinaDivMetric


def NinaSstDur(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
               nbr_years_window, dataset='', debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSstDurRmse() function computes a duration of La Nina events.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA < 'threshold' during 'season' are considered as La Nina events
        2.) La Nina duration
            2.1) get a time series of 2 years before and 2 years after the La Nina peak (4 years time series)
            2.2) count the number of consecutive month below a threshold

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaDurMetric: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, time_period, ref,
        keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina Duration'
    Units = 'months'
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + \
             ', number of consecutive months when sstA < -0.5' + Units
    Ref = 'Using CDAT'
    metric = 'NinaSstDur'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst, sst_areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    if keyerror is not None:
        duration_mean, duration_err, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, Method, keyerror = PreProcessTS(
            sst, Method, areacell=sst_areacell, average='horizontal', compute_anom=True, region=region_ev, **kwargs)
        del sst_areacell
        if keyerror is not None:
            duration_mean, duration_err, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
        else:
            if debug is True:
                dict_debug = {'axes1': str([ax.id for ax in sst.getAxisList()]), 'shape1': str(sst.shape),
                              'time1': str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years = DetectEvents(sst, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': str(event_years)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. La Nina duration
            # ------------------------------------------------
            # 2.1 get a time series of 2 years before and 2 years after the La Nina peak (4 years time series)
            # composites
            sample = Event_selection(
                sst, kwargs['frequency'], nbr_years_window=nbr_years_window, list_event_years=event_years)
            if debug is True:
                dict_debug = {'axes1': str([ax.id for ax in sample.getAxisList()]), 'shape1': str(sample.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Event_selection', 15, **dict_debug)

            # 2.2 count the number of consecutive month below a threshold
            if normalize is True:
                duration = DurationAllEvent(sample, -0.5 * float(Std(sst)), nino=False, debug=debug)
            else:
                duration = DurationAllEvent(sample, -0.5, nino=False, debug=debug)

            duration_err = float(Std(duration) / NUMPYsqrt(len(duration)))
            duration_mean = float(duration.mean())

            # Dive down diagnostic
            dive_down_diag = {'value': ArrayToList(duration), 'axis': list(duration.getAxis(0)[:])}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {'units': Units, 'number_of_years_used': yearN, 'time_period': str(actualtimebounds),
                         'nina_years': str(event_years), 'description': "La duration of Nina events",
                         'diagnostic_value': duration_mean, 'diagnostic_value_error': duration_err}
                dict2 = {'metric_name': Name, 'metric_method': Method, 'metric_reference': Ref,
                         'frequency': kwargs['frequency']}
                SaveNetcdf(file_name, var1=duration, var1_attributes=dict1, var1_name='Nina_duration__' + dataset,
                           global_attributes=dict2)
                del dict1, dict2, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(duration_mean),
                      'line2': 'metric value_error: ' + str(duration_err)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinaDurMetric = {
        'name': Name, 'value': duration_mean, 'value_error': duration_err, 'units': Units, 'method': Method,
        'nyears': yearN, 'events': event_years, 'time_frequency': kwargs['frequency'], 'time_period': actualtimebounds,
        'ref': Ref, 'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinaDurMetric


def NinaSstLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
                   netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSstLonRmse() function computes a zonal composite of La Nina events during the peak of the event
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA < 'threshold' during 'season' are considered as La Nina events
    Then the zonal SSTA at the peak of the event is composited for each selected event

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaLonMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina Zonal Composite'
    lat = ReferenceRegions(box)['latitude']
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' \
             + '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']'
    Units = '' if kwargs['normalization'] else 'C'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinaSstLonRmse'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axis': None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                              'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. zonal composite of SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, 'temperature', metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
                file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
            sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, 'temperature', metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
                file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
            if keyerror_mod is not None or keyerror_obs is not None:
                compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst_mod, Method, keyerror_mod = PreProcessTS(
                    sst_mod, Method, areacell=mod_areacell, average=False, compute_anom=False, region=box, **kwargs)
                sst_obs, _, keyerror_obs = PreProcessTS(
                    sst_obs, '', areacell=obs_areacell, average=False, compute_anom=False, region=box, **kwargs)
                del mod_areacell, obs_areacell
                if keyerror_mod is not None or keyerror_obs is not None:
                    compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                    dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                                      'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                                      'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
                    sst_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name',
                                      'regridder', 'regridTool', 'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst_mod, sst_obs, Method = TwoVarRegrid(
                            sst_mod, sst_obs, Method, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst_mod, keyerror_mod = AverageMeridional(sst_mod)
                    sst_obs, keyerror_obs = AverageMeridional(sst_obs)
                    if keyerror_mod is not None or keyerror_obs is not None:
                        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sst_mod = Composite(sst_mod, event_years_mod, kwargs['frequency'])
                        sst_obs = Composite(sst_obs, event_years_obs, kwargs['frequency'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

                        # Computes the root mean square difference
                        compRmse, keyerror = RmsZonal(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)

                        # Error on the metric
                        compRmseErr = None

                        # Dive down diagnostic
                        dive_down_diag = {'model': ArrayToList(sst_mod), 'observations': ArrayToList(sst_obs),
                                          'axis': list(sst_mod.getAxis(0)[:])}
                        if netcdf is True:
                            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                                sstfilemod, sstnamemod, 'temperature', metric, 'equatorial_pacific_LatExt2',
                                file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                                name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
                                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
                            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                                sstfileobs, sstnameobs, 'temperature', metric, 'equatorial_pacific_LatExt2',
                                file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                                name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
                                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
                            if keyerror_mod is not None or keyerror_obs is not None:
                                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            else:
                                map_mod, _, keyerror_mod = PreProcessTS(
                                    map_mod, '', areacell=mod_areacell, average=False, compute_anom=False,
                                    region="equatorial_pacific_LatExt2", **kwargs)
                                map_obs, _, keyerror_obs = PreProcessTS(
                                    map_obs, '', areacell=obs_areacell, average=False, compute_anom=False,
                                    region="equatorial_pacific_LatExt2", **kwargs)
                                del mod_areacell, obs_areacell
                                if keyerror_mod is not None or keyerror_obs is not None:
                                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                                else:
                                    if debug is True:
                                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in map_mod.getAxisList()]),
                                                      'axes2': '(obs) ' + str([ax.id for ax in map_obs.getAxisList()]),
                                                      'shape1': '(mod) ' + str(map_mod.shape),
                                                      'shape2': '(obs) ' + str(map_obs.shape),
                                                      'time1': '(mod) ' + str(TimeBounds(map_mod)),
                                                      'time2': '(obs) ' + str(TimeBounds(map_obs))}
                                        EnsoErrorsWarnings.debug_mode(
                                            '\033[92m', 'after PreProcessTS: netcdf', 15, **dict_debug)
                                    # Seasonal mean
                                    map_mod = SeasonalMean(map_mod, season_ev, compute_anom=True)
                                    map_obs = SeasonalMean(map_obs, season_ev, compute_anom=True)
                                    # Regridding
                                    if isinstance(kwargs['regridding'], dict):
                                        map_mod, map_obs, _ = TwoVarRegrid(
                                            map_mod, map_obs, '', region='equatorial_pacific_LatExt2',
                                            **kwargs['regridding'])
                                        if debug is True:
                                            dict_debug = {
                                                'axes1': '(mod) ' + str([ax.id for ax in map_mod.getAxisList()]),
                                                'axes2': '(obs) ' + str([ax.id for ax in map_obs.getAxisList()]),
                                                'shape1': '(mod) ' + str(map_mod.shape),
                                                'shape2': '(obs) ' + str(map_obs.shape)}
                                            EnsoErrorsWarnings.debug_mode(
                                                '\033[92m', 'after TwoVarRegrid: netcdf', 15, **dict_debug)
                                    # samples
                                    map_mod = Composite(map_mod, event_years_mod, kwargs['frequency'])
                                    map_obs = Composite(map_obs, event_years_obs, kwargs['frequency'])
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                             'time_period': str(actualtimebounds_mod),
                                             'nina_years': str(event_years_mod)}
                                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                             'time_period': str(actualtimebounds_obs),
                                             'nina_years': str(event_years_obs)}
                                    dict3 = {'units': Units, 'number_of_years_used': yearN_mod,
                                             'time_period': str(actualtimebounds_mod),
                                             'nina_years': str(event_years_mod)}
                                    dict4 = {'units': Units, 'number_of_years_used': yearN_obs,
                                             'time_period': str(actualtimebounds_obs),
                                             'nina_years': str(event_years_obs)}
                                    dict5 = {'metric_name': Name, 'metric_value_' + dataset2: compRmse,
                                             'metric_value_error_' + dataset2: compRmseErr, 'metric_method': Method,
                                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                                    SaveNetcdf(
                                        file_name, var1=sst_mod, var1_attributes=dict1,
                                        var1_name='sst_lon__' + dataset1, var2=sst_obs, var2_attributes=dict2,
                                        var2_name='sst_lon__' + dataset2, var3=map_mod, var3_attributes=dict3,
                                        var3_name='sst_map__' + dataset1, var4=map_obs, var4_attributes=dict4,
                                        var4_name='sst_map__' + dataset2, global_attributes=dict5)
                                    del dict1, dict2, dict3, dict4, dict5, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(compRmse), 'line2': 'metric value_error: ' + str(compRmseErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinaLonMetric = {
        'name': Name, 'value': compRmse, 'value_error': compRmseErr, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'events_model': event_years_mod,
        'events_observations': event_years_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinaLonMetric


def NinaSlpMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               slpfilemod, slpnamemod, slpareafilemod, slpareanamemod, slplandmaskfilemod, slplandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
               slpfileobs, slpnameobs, slpareafileobs, slpareanameobs, slplandmaskfileobs, slplandmasknameobs, sstbox,
               slpbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
               netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSlpMap() function computes a sea level pressure anomalies composite of during the peak of La Nina events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA < 'threshold' during 'season' are considered as La Nina events
    Then the SLPA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param slpfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SLP
    :param slpnamemod: string
        name of SLP variable (slp) in 'slpfilemod'
    :param slpareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP areacell
    :param slpareanamemod: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafilemod'
    :param slplandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP landmask
    :param slplandmasknamemod: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param slpfileobs: string
        path_to/filename of the file (NetCDF) of the observed SLP
    :param slpnameobs: string
        name of SLP variable (slp) in 'slpfileobs'
    :param slpareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP areacell
    :param slpareanameobs: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafileobs'
    :param slplandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP landmask
    :param slplandmasknameobs: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param slpbox: string
        name of box (e.g. 'global') for SLP
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaSlpMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina SLPA Composite'
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + \
             ', Nina SLPA Composited'
    Units = '' if kwargs['normalization'] else 'hPa'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinaSlpMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    slp_mod, slp_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        slpfilemod, slpnamemod, 'pressure', metric, slpbox, file_area=slpareafilemod, name_area=slpareanamemod,
        file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    slp_obs, slp_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        slpfileobs, slpnameobs, 'pressure', metric, slpbox, file_area=slpareafileobs, name_area=slpareanameobs,
        file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, slp_mod, keyerror_mod3 = CheckTime(sst_mod, slp_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, slp_obs, keyerror_obs3 = CheckTime(sst_obs, slp_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        slp_mod, Method, keyerror_mod2 = PreProcessTS(
            slp_mod, Method, areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
        slp_obs, _, keyerror_obs2 = PreProcessTS(
            slp_obs, '', areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
        del mod_areacell, obs_areacell, slp_mod_areacell, slp_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1) or \
                (keyerror_mod2 is not None or keyerror_obs2):
            slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                    'axes4': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod slp) ' + str(slp_mod.shape), 'shape4': '(obs slp) ' + str(slp_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod slp) ' + str(TimeBounds(slp_mod)), 'time4': '(obs slp) ' + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite SLPA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            slp_mod = SeasonalMean(slp_mod, season_ev, compute_anom=True) * 1e-2
            slp_obs = SeasonalMean(slp_obs, season_ev, compute_anom=True) * 1e-2
            if debug is True:
                dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                              'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                              'shape1': '(mod slp) ' + str(slp_mod.shape), 'shape2': '(obs slp) ' + str(slp_obs.shape),
                              'time1': '(mod slp) ' + str(TimeBounds(slp_mod)),
                              'time2': '(obs slp) ' + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                slp_mod, slp_obs, Method = TwoVarRegrid(slp_mod, slp_obs, Method, region=slpbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                                  'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                                  'shape1': '(mod slp) ' + str(slp_mod.shape),
                                  'shape2': '(obs slp) ' + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            slp_mod = Composite(slp_mod, event_years_mod, kwargs['frequency'])
            slp_obs = Composite(slp_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                              'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                              'shape1': '(mod slp) ' + str(slp_mod.shape), 'shape2': '(obs slp) ' + str(slp_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            slp_mod, keyerror_mod = BasinMask(
                slp_mod, 'pacific', box=slpbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            slp_obs, keyerror_obs = BasinMask(
                slp_obs, 'pacific', box=slpbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(slp_mod.shape), 'shape2': '(obs) ' + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                slpRmse, keyerror = RmsAxis(slp_mod, slp_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                slpRmseErr = None
                # Metric 2
                slpCorr = float(Correlation(slp_mod, slp_obs, axis='xy', centered=1, biased=1))
                slpCorrErr = None
                # Metric 3
                std_mod = Std(slp_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(slp_obs, weights=None, axis='xy', centered=1, biased=1)
                slpStd = float(std_mod) / float(std_obs)
                slpStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                             'time_period': str(actualtimebounds_mod),
                             'nina_years': str(event_years_mod)}
                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                             'time_period': str(actualtimebounds_obs),
                             'nina_years': str(event_years_obs)}
                    dict3 = {'metric_name': Name, 'metric_valueRMSE_' + dataset2: slpRmse,
                             'metric_valueRMSE_error_' + dataset2: slpRmseErr, 'metric_valueCORR_' + dataset2: slpCorr,
                             'metric_valueCORR_error_' + dataset2: slpCorrErr, 'metric_valueSTD_' + dataset2: slpStd,
                             'metric_valueCORR_error_' + dataset2: slpStdErr, 'metric_method': Method,
                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(file_name, var1=slp_mod, var1_attributes=dict1, var1_name='slp_map__' + dataset1,
                               var2=slp_obs, var2_attributes=dict2, var2_name='slp_map__' + dataset2,
                               global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if slpCorr is not None:
        slpCorr = 1 - slpCorr
    # Create output
    NinaSlpMapMetric = {
        'name': Name, 'Rmse__value': slpRmse, 'Rmse__value_error': slpRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': slpCorr, 'Corr__value_error': slpCorrErr, 'Corr__units': '', 'Std__value': slpStd,
        'Std__value_error': slpStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinaSlpMapMetric


def NinaSstMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, tsbox,
               event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False, netcdf=False,
               netcdf_name='', metname='', **kwargs):
    """
    The NinaSstMap() function computes a surface temperature anomalies composite of during the peak of La Nina events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA < 'threshold' during 'season' are considered as La Nina events
    Then the TSA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaSstMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina TSA Composite'
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + \
             ', Nina TSA Composited'
    Units = '' if kwargs['normalization'] else 'mm/day'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinaSstMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    tsmap_mod, tsmap_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, tsbox, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    tsmap_obs, tsmap_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, tsbox, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tsmap_mod, keyerror_mod3 = CheckTime(sst_mod, tsmap_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tsmap_obs, keyerror_obs3 = CheckTime(sst_obs, tsmap_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        tsmap_mod, Method, keyerror_mod2 = PreProcessTS(
            tsmap_mod, Method, areacell=tsmap_mod_areacell, compute_anom=False, region=tsbox, **kwargs)
        tsmap_obs, _, keyerror_obs2 = PreProcessTS(
            tsmap_obs, '', areacell=tsmap_obs_areacell, compute_anom=False, region=tsbox, **kwargs)
        del mod_areacell, obs_areacell, tsmap_mod_areacell, tsmap_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1 is not None) or \
                (keyerror_obs2 is not None or keyerror_mod2 is not None):
            tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                    'axes4': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod ts) ' + str(tsmap_mod.shape), 'shape4': '(obs ts) ' + str(tsmap_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod ts) ' + str(TimeBounds(tsmap_mod)),
                    'time4': '(obs ts) ' + str(TimeBounds(tsmap_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite TSA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            tsmap_mod = SeasonalMean(tsmap_mod, season_ev, compute_anom=True)
            tsmap_obs = SeasonalMean(tsmap_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                              'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                              'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                              'shape2': '(obs ts) ' + str(tsmap_obs.shape),
                              'time1': '(mod ts) ' + str(TimeBounds(tsmap_mod)),
                              'time2': '(obs ts) ' + str(TimeBounds(tsmap_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tsmap_mod, tsmap_obs, Method = TwoVarRegrid(
                    tsmap_mod, tsmap_obs, Method, region=tsbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                                  'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                                  'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                                  'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            tsmap_mod = Composite(tsmap_mod, event_years_mod, kwargs['frequency'])
            tsmap_obs = Composite(tsmap_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                              'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                              'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                              'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            tsmap_mod, keyerror_mod = BasinMask(
                tsmap_mod, 'pacific', box=tsbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            tsmap_obs, keyerror_obs = BasinMask(
                tsmap_obs, 'pacific', box=tsbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                                  'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                                  'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                                  'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                tsRmse, keyerror = RmsAxis(tsmap_mod, tsmap_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                tsRmseErr = None
                # Metric 2
                tsCorr = float(Correlation(tsmap_mod, tsmap_obs, axis='xy', centered=1, biased=1))
                tsCorrErr = None
                # Metric 3
                std_mod = Std(tsmap_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(tsmap_obs, weights=None, axis='xy', centered=1, biased=1)
                tsStd = float(std_mod) / float(std_obs)
                tsStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                             'time_period': str(actualtimebounds_mod), 'nina_years': str(event_years_mod)}
                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                             'time_period': str(actualtimebounds_obs), 'nina_years': str(event_years_obs)}
                    dict3 = {'metric_name': Name, 'metric_valueRMSE_' + dataset2: tsRmse,
                             'metric_valueRMSE_error_' + dataset2: tsRmseErr, 'metric_valueCORR_' + dataset2: tsCorr,
                             'metric_valueCORR_error_' + dataset2: tsCorrErr, 'metric_valueSTD_' + dataset2: tsStd,
                             'metric_valueCORR_error_' + dataset2: tsStdErr, 'metric_method': Method,
                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(file_name, var1=tsmap_mod, var1_attributes=dict1, var1_name='ts_map__' + dataset1,
                               var2=tsmap_obs, var2_attributes=dict2, var2_name='ts_map__' + dataset2,
                               global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if tsCorr is not None:
        tsCorr = 1 - tsCorr
    # Create output
    NinaSstMapMetric = {
        'name': Name, 'Rmse__value': tsRmse, 'Rmse__value_error': tsRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': tsCorr, 'Corr__value_error': tsCorrErr, 'Corr__units': '', 'Std__value': tsStd,
        'Std__value_error': tsStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinaSstMapMetric


def NinaSstTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  box, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='',
                  debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinaSstTsRmse() function computes a time composite of La Nina events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA < 'threshold' during 'season' are considered as La Nina events
        Then a 'nbr_years_window' long time series centered on selected events is composited for each selected event

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': -0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinaTsMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'La Nina Composite Time Series'
    Method = 'Nina events = ' + region_ev + ' sstA < ' + str(threshold) + ' during ' + season_ev + ', time series of ' \
             + str(nbr_years_window) + ' years (centered on events)'
    Units = '' if kwargs['normalization'] else 'C'
    Ref = 'Using CDAT rms (uncentered and biased) calculation'
    metric = 'NinaSstTsRmse'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axis': None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                              'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA < 'threshold' during 'season' are considered as La Nina events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=False)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=False)
            if debug is True:
                dict_debug = {'nina1': '(mod) ' + str(event_years_mod), 'nina2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. temporal composite of SSTA
            # ------------------------------------------------
            # interannual anomalies
            sst_mod = ComputeInterannualAnomalies(sst_mod)
            sst_obs = ComputeInterannualAnomalies(sst_obs)

            # composites
            composite_mod = Composite(sst_mod, event_years_mod, kwargs['frequency'], nbr_years_window=nbr_years_window)
            composite_obs = Composite(sst_obs, event_years_obs, kwargs['frequency'], nbr_years_window=nbr_years_window)
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in composite_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in composite_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(composite_mod.shape),
                              'shape2': '(obs) ' + str(composite_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # Computes the root mean square difference
            compRmse, keyerror = RmsAxis(
                composite_mod, composite_obs, axis=0, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            compRmseErr = None

            # Dive down diagnostic
            dive_down_diag = {'model': ArrayToList(composite_mod), 'observations': ArrayToList(composite_obs),
                              'axis': list(composite_mod.getAxis(0)[:])}
            if netcdf is True:
                # Read file and select the right region
                sst_hov_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                    sstfilemod, sstnamemod, 'temperature', metric, 'equatorial_pacific', file_area=sstareafilemod,
                    name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True,
                    maskocean=False, time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
                sst_hov_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                    sstfileobs, sstnameobs, 'temperature', metric, 'equatorial_pacific', file_area=sstareafileobs,
                    name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True,
                    maskocean=False, time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                if keyerror is None:
                    # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                    sst_hov_mod, _, keyerror_mod = PreProcessTS(
                        sst_hov_mod, Method, areacell=mod_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    sst_hov_obs, _, keyerror_obs = PreProcessTS(
                        sst_hov_obs, '', areacell=obs_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del mod_areacell, obs_areacell
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_hov_mod.shape),
                                          'shape2': '(obs) ' + str(sst_hov_obs.shape),
                                          'time1': '(mod) ' + str(TimeBounds(sst_hov_mod)),
                                          'time2': '(obs) ' + str(TimeBounds(sst_hov_obs))}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)
                        # Regridding
                        if 'regridding' not in list(kwargs.keys()):
                            kwargs['regridding'] = {'regridder': 'cdms', 'regridTool': 'esmf', 'regridMethod': 'linear',
                                                    'newgrid_name': 'generic_1x1deg'}
                        else:
                            if not isinstance(kwargs['regridding'], dict):
                                kwargs['regridding'] = {'regridder': 'cdms', 'regridTool': 'esmf',
                                                        'regridMethod': 'linear', 'newgrid_name': 'generic_1x1deg'}
                        sst_hov_mod, sst_hov_obs, Method = TwoVarRegrid(
                            sst_hov_mod, sst_hov_obs, Method, region='equatorial_pacific', **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_hov_mod.shape),
                                          'shape2': '(obs) ' + str(sst_hov_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)
                        # Meridional average
                        sst_hov_mod, keyerror_mod = AverageMeridional(sst_hov_mod)
                        sst_hov_obs, keyerror_obs = AverageMeridional(sst_hov_obs)
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                              'axes2': '(obs) ' + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                              'shape1': '(mod) ' + str(sst_hov_mod.shape),
                                              'shape2': '(obs) ' + str(sst_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)
                            # samples
                            sst_hov_mod = Composite(
                                sst_hov_mod, event_years_mod, kwargs['frequency'], nbr_years_window=nbr_years_window)
                            sst_hov_obs = Composite(
                                sst_hov_obs, event_years_obs, kwargs['frequency'], nbr_years_window=nbr_years_window)
                            if debug is True:
                                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                              'axes2': '(obs) ' + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                              'shape1': '(mod) ' + str(sst_hov_mod.shape),
                                              'shape2': '(obs) ' + str(sst_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod), 'nina_years': str(event_years_mod),
                                     'description': "time series of " + box + " sstA centered on La Nina peak"}
                            dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs), 'nina_years': str(event_years_obs),
                                     'description': "time series of " + box + " sstA centered on La Nina peak"}
                            dict3 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod), 'nina_years': str(event_years_mod),
                                     'description': "zonal monthly of equatorial_pacific sstA centered on La Nina peak"}
                            dict4 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs), 'nina_years': str(event_years_obs),
                                     'description': "zonal monthly of equatorial_pacific sstA centered on La Nina peak"}
                            dict5 = {'metric_name': Name, 'metric_value_' + dataset2: compRmse,
                                     'metric_value_error_' + dataset2: compRmseErr, 'metric_method': Method,
                                     'metric_reference': Ref, 'frequency': kwargs['frequency']}
                            SaveNetcdf(
                                file_name, var1=composite_mod, var1_attributes=dict1, var1_name='sst_ts__' + dataset1,
                                var2=composite_obs, var2_attributes=dict2, var2_name='sst_ts__' + dataset2,
                                var3=sst_hov_mod, var3_attributes=dict3, var3_name='sst_hov__' + dataset1,
                                var4=sst_hov_obs, var4_attributes=dict4, var4_name='sst_hov__' + dataset2,
                                global_attributes=dict5)
                            del dict1, dict2, dict3, dict4, dict5, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(compRmse), 'line2': 'metric value_error: ' + str(compRmseErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinaTsMetric = {
        'name': Name, 'value': compRmse, 'value_error': compRmseErr, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'events_model': event_years_mod,
        'events_observations': event_years_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinaTsMetric


def NinoPrMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod, prfilemod,
              prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod, sstfileobs, sstnameobs,
              sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, prfileobs, prnameobs,
              prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs, sstbox, prbox, event_definition,
              centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False, netcdf=False, netcdf_name='',
              metname='', **kwargs):
    """
    The NinoPrMap() function computes a precipitation anomalies composite of during the peak of El Nino events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA > 'threshold' during 'season' are considered as El Nino events
    Then the PRA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR areacell
    :param prareanamemod: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafilemod'
    :param prlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled PR landmask
    :param prlandmasknamemod: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR areacell
    :param prareanameobs: string, optional
        name of areacell for the PR variable (areacella, areacello,...) in 'prareafileobs'
    :param prlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed PR landmask
    :param prlandmasknameobs: string, optional
        name of landmask for the PR variable (sftlf,...) in 'prlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param prbox: string
        name of box (e.g. 'global') for PR
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoPrMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino PRA Composite'
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + \
             ', Nino PRA Composited'
    Units = '' if kwargs['normalization'] else 'mm/day'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinoPrMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    pr_mod, pr_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        prfilemod, prnamemod, 'precipitations', metric, prbox, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    pr_obs, pr_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        prfileobs, prnameobs, 'precipitations', metric, prbox, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, pr_mod, keyerror_mod3 = CheckTime(sst_mod, pr_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, pr_obs, keyerror_obs3 = CheckTime(sst_obs, pr_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        pr_mod, Method, keyerror_mod2 = PreProcessTS(
            pr_mod, Method, areacell=pr_mod_areacell, compute_anom=False, region=prbox, **kwargs)
        pr_obs, _, keyerror_obs2 = PreProcessTS(
            pr_obs, '', areacell=pr_obs_areacell, compute_anom=False, region=prbox, **kwargs)
        del mod_areacell, obs_areacell, pr_mod_areacell, pr_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1 is not None) or \
                (keyerror_mod2 is not None or keyerror_obs2 is not None):
            prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod pr) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes4': '(obs pr) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod pr) ' + str(pr_mod.shape), 'shape4': '(obs pr) ' + str(pr_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod pr) ' + str(TimeBounds(pr_mod)), 'time4': '(obs pr) ' + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': '(mod) ' + str(event_years_mod), 'nino2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite PRA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            pr_mod = SeasonalMean(pr_mod, season_ev, compute_anom=True)
            pr_obs = SeasonalMean(pr_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(pr_mod)), 'time2': '(obs) ' + str(TimeBounds(pr_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                pr_mod, pr_obs, Method = TwoVarRegrid(pr_mod, pr_obs, Method, region=prbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            pr_mod = Composite(pr_mod, event_years_mod, kwargs['frequency'])
            pr_obs = Composite(pr_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            pr_mod, keyerror_mod = BasinMask(
                pr_mod, 'pacific', box=prbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            pr_obs, keyerror_obs = BasinMask(
                pr_obs, 'pacific', box=prbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                prCorr, prCorrErr, prRmse, prRmseErr, prStd, prStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in pr_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in pr_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(pr_mod.shape), 'shape2': '(obs) ' + str(pr_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                prRmse, keyerror = RmsAxis(pr_mod, pr_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                prRmseErr = None
                # Metric 2
                prCorr = float(Correlation(pr_mod, pr_obs, axis='xy', centered=1, biased=1))
                prCorrErr = None
                # Metric 3
                std_mod = Std(pr_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(pr_obs, weights=None, axis='xy', centered=1, biased=1)
                prStd = float(std_mod) / float(std_obs)
                prStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}

                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {
                        'units': Units, 'number_of_years_used': yearN_mod, 'time_period': str(actualtimebounds_mod),
                        'nino_years': str(event_years_mod)}
                    dict2 = {
                        'units': Units, 'number_of_years_used': yearN_obs, 'time_period': str(actualtimebounds_obs),
                        'nino_years': str(event_years_obs)}
                    dict3 = {
                        'metric_name': Name, 'metric_valueRMSE_' + dataset2: prRmse,
                        'metric_valueRMSE_error_' + dataset2: prRmseErr, 'metric_valueCORR_' + dataset2: prCorr,
                        'metric_valueCORR_error_' + dataset2: prCorrErr, 'metric_valueSTD_' + dataset2: prStd,
                        'metric_valueCORR_error_' + dataset2: prStdErr, 'metric_method': Method,
                        'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(
                        file_name, var1=pr_mod, var1_attributes=dict1, var1_name='prComp_map__' + dataset1, var2=pr_obs,
                        var2_attributes=dict2, var2_name='prComp_map__' + dataset2, global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if prCorr is not None:
        prCorr = 1 - prCorr
    # Create output
    NinoPrMapMetric = {
        'name': Name, 'Rmse__value': prRmse, 'Rmse__value_error': prRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': prCorr, 'Corr__value_error': prCorrErr, 'Corr__units': '', 'Std__value': prStd,
        'Std__value_error': prStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinoPrMapMetric


def NinoSstDiv(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
               dataset='', debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSstDiv() function computes a zonal composite of El Nino events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > 'threshold' during 'season' are considered as El Nino events
        2.) diversity of the zonal location of the maximum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the maximum SSTA for each selected event
            2.3) compute the percentage of EP events (maximum SSTA eastward of the given threshold)

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param treshold_ep_ev: float, optional
        see EnsoToolsLib.percentage_val_eastward
        longitude, in degree east, of the westward boundary of eastern Pacific event
        default value is -140°E (i.e., 140°W)
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoDivMetric: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, ref, keyerror,
        dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'treshold_ep_ev',
                    'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino Diversity (percentage of eastern Pacific El Nino)'
    lat = ReferenceRegions(box)['latitude']
    lon = ReferenceRegions(box)['longitude']
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' + \
             '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']), westward boundary of EP events' + \
             str(kwargs['treshold_ep_ev']) + 'E'
    Units = '%'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinoSstDiv'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst, areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    if keyerror is not None:
        ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, _, keyerror = PreProcessTS(
            sst, '', areacell=areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del areacell
        if keyerror is not None:
            ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
        else:
            if debug is True:
                dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                              'shape1': '(sst) ' + str(sst.shape), 'time1': '(sst) ' + str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years = DetectEvents(sst, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': 'nbr(' + str(len(event_years)) + '): ' + str(event_years)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the maximum SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst, areacell, keyerror = Read_data_mask_area(
                sstfile, sstname, 'temperature', metric, box, file_area=sstareafile, name_area=sstareaname,
                file_mask=sstlandmaskfile, name_mask=sstlandmaskfile, maskland=True, maskocean=False, debug=debug,
                **kwargs)
            if keyerror is not None:
                ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst, Method, keyerror = PreProcessTS(
                    sst, Method, areacell=areacell, average=False, compute_anom=False, region=box, **kwargs)
                del areacell
                if keyerror is not None:
                    ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                      'shape1': '(sst) ' + str(sst.shape), 'time1': '(sst) ' + str(TimeBounds(sst))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst = SeasonalMean(sst, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                      'shape1': '(sst) ' + str(sst.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder', 'regridTool',
                                      'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst = Regrid(sst, None, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                          'shape1': '(sst) ' + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst, keyerror = AverageMeridional(sst)
                    if keyerror is not None:
                        ep_event, StdErr, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sst.getAxisList()]),
                                          'shape1': '(sst) ' + str(sst.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sample = Event_selection(sst, kwargs['frequency'], list_event_years=event_years)

                        # 2.2 find the zonal position of the maximum SSTA for each selected event
                        lon_sstmax = FindXYMinMaxInTs(
                            sample, return_val='maxi', smooth=True, axis=0, window=5, method='triangle')
                        if debug is True:
                            dict_debug = {'line1': 'longitude of the maximum SSTA: ' + str(lon_sstmax)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)

                        # 2.3 compute the percentage of EP events (maximum SSTA eastward of the given threshold)
                        ep_event, keyerror = percentage_val_eastward(
                            lon_sstmax, metric, box, threshold=kwargs['treshold_ep_ev'])
                        ep_event = float(ep_event)

                        if keyerror is not None:
                            StdErr, dive_down_diag = None, {'value': None, 'axis': None}
                        else:
                            # Standard Error of the Standard Deviation (function of nyears)
                            StdErr = None

                            # Dive down diagnostic
                            dive_down_diag = {'value': ArrayToList(lon_sstmax), 'axis': list(lon_sstmax.getAxis(0)[:])}

                            if netcdf is True:
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {'units': 'longitude (E)', 'number_of_years_used': yearN,
                                         'time_period': str(actualtimebounds), 'nino_years': str(event_years),
                                         'diagnostic_value_' + dataset: ep_event,
                                         'diagnostic_value_error_' + dataset: StdErr}
                                dict2 = {'metric_name': Name, 'metric_method': Method, 'metric_reference': Ref,
                                         'frequency': kwargs['frequency']}
                                SaveNetcdf(file_name, var1=lon_sstmax, var1_attributes=dict1,
                                           var1_name='Nino_lon_pos_minSSTA__' + dataset, global_attributes=dict2)
                                del dict1, dict2
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(ep_event), 'line2': 'metric value_error: ' + str(StdErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinoDivMetric = {
        'name': Name, 'value': ep_event, 'value_error': StdErr, 'units': Units, 'method': Method, 'nyears': yearN,
        'events': event_years, 'time_frequency': kwargs['frequency'], 'time_period': actualtimebounds, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinoDivMetric


def NinoSstDivRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
                   netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSstDivRmse() function computes a zonal maximum of El Nino events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > 'threshold' during 'season' are considered as El Nino events
        2.) diversity of the zonal location of the maximum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the maximum SSTA for each selected event and compute a pdf

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoDivMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyword, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'PDF of zonal min(SSTA) during El Nino'
    lat = ReferenceRegions(box)['latitude']
    lon = ReferenceRegions(box)['longitude']
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' \
             + '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']'
    Units = 'density'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinoSstDivRmse'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axis': None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                              'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': '(mod) ' + str(event_years_mod), 'nino2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the maximum SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, 'temperature', metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
                file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
            sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, 'temperature', metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
                file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
            if keyerror_mod is not None or keyerror_obs is not None:
                pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst_mod, Method, keyerror_mod = PreProcessTS(
                    sst_mod, Method, areacell=mod_areacell, average=False, compute_anom=False, region=box, **kwargs)
                sst_obs, _, keyerror_obs = PreProcessTS(
                    sst_obs, '', areacell=obs_areacell, average=False, compute_anom=False, region=box, **kwargs)
                del mod_areacell, obs_areacell
                if keyerror_mod is not None or keyerror_obs is not None:
                    pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                    dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                                      'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                                      'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
                    sst_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name',
                                      'regridder', 'regridTool', 'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst_mod, sst_obs, Method = TwoVarRegrid(
                            sst_mod, sst_obs, Method, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst_mod, keyerror_mod = AverageMeridional(sst_mod)
                    sst_obs, keyerror_obs = AverageMeridional(sst_obs)
                    if keyerror_mod is not None or keyerror_obs is not None:
                        pdfRmse, pdfRmseErr, event_years_mod, event_years_obs = None, None, None, None
                        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sample_mod = Event_selection(sst_mod, kwargs['frequency'], list_event_years=event_years_mod)
                        sample_obs = Event_selection(sst_obs, kwargs['frequency'], list_event_years=event_years_obs)

                        # 2.2 find the zonal position of the maximum SSTA for each selected event and compute a pdf
                        # longitude of the maximum SSTA for each selected event
                        lon_min_mod = FindXYMinMaxInTs(
                            sample_mod, return_val='maxi', smooth=True, axis=0, window=5, method='triangle')
                        lon_min_obs = FindXYMinMaxInTs(
                            sample_obs, return_val='maxi', smooth=True, axis=0, window=5, method='triangle')
                        if debug is True:
                            dict_debug = {'line1': '(mod) longitude  of the maximum SSTA: ' + str(lon_min_mod),
                                          'line2': '(obs) longitude  of the maximum SSTA: ' + str(lon_min_obs)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)

                        # compute PDFs
                        if debug is True:
                            dict_debug = {'line1': 'lon ' + str(lon) + '  ;  nbr_bins old = ' +
                                                   str((lon[1] - lon[0]) / 10) + '  ;  nbr_bins new = ' +
                                                   str(int(round((lon[1] - lon[0]) / 10)))}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'before ComputePDF', 15, **dict_debug)
                        pdf_mod = ComputePDF(lon_min_mod, nbr_bins=int(round((lon[1] - lon[0]) / 10)), interval=lon,
                                             axis_name='longitude')
                        pdf_obs = ComputePDF(lon_min_obs, nbr_bins=int(round((lon[1] - lon[0]) / 10)), interval=lon,
                                             axis_name='longitude')

                        # Computes the root mean square difference
                        pdfRmse, keyerror = RmsZonal(pdf_mod, pdf_obs, centered=centered_rmse, biased=biased_rmse)

                        # Error on the metric
                        pdfRmseErr = None

                        # Dive down diagnostic
                        dive_down_diag = {'model': ArrayToList(pdf_mod), 'observations': ArrayToList(pdf_obs),
                                          'axis': list(pdf_mod.getAxis(0)[:])}
                        if netcdf is True:
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                     'time_period': str(actualtimebounds_mod), 'nino_years': str(event_years_mod)}
                            dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                     'time_period': str(actualtimebounds_obs), 'nino_years': str(event_years_obs)}
                            dict3 = {'metric_name': Name, 'metric_value_' + dataset2: pdfRmse,
                                     'metric_value_error_' + dataset2: pdfRmseErr, 'metric_method': Method,
                                     'metric_reference': Ref, 'frequency': kwargs['frequency']}
                            SaveNetcdf(file_name, var1=pdf_mod, var1_attributes=dict1, var1_name='pdf__' + dataset1,
                                       var2=pdf_obs, var2_attributes=dict2, var2_name='pdf__' + dataset2,
                                       global_attributes=dict3)
                            del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(pdfRmse), 'line2': 'metric value_error: ' + str(pdfRmseErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinoDivMetric = {
        'name': Name, 'value': pdfRmse, 'value_error': pdfRmseErr, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'events_model': event_years_mod,
        'events_observations': event_years_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinoDivMetric


def NinoSstDur(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box, event_definition,
               nbr_years_window, dataset='', debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSstDurRmse() function computes a duration of El Nino events.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > 'threshold' during 'season' are considered as El Nino events
        2.) El Nino duration
            2.1) get a time series of 2 years before and 2 years after the El Nino peak (4 years time series)
            2.2) count the number of consecutive month above a threshold

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoDurMetric: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, time_period, ref,
        keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino Duration'
    Units = 'months'
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + \
             ', number of consecutive months when sstA > 0.5' + Units
    Ref = 'Using CDAT'
    metric = 'NinoSstDur'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst, sst_areacell, keyerror = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    if keyerror is not None:
        duration_mean, duration_err, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst, Method, keyerror = PreProcessTS(
            sst, Method, areacell=sst_areacell, average='horizontal', compute_anom=True, region=region_ev, **kwargs)
        del sst_areacell
        if keyerror is not None:
            duration_mean, duration_err, dive_down_diag, event_years = None, None, {'value': None, 'axis': None}, None
        else:
            if debug is True:
                dict_debug = {'axes1': str([ax.id for ax in sst.getAxisList()]), 'shape1': str(sst.shape),
                              'time1': str(TimeBounds(sst))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years = DetectEvents(sst, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': str(event_years)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. El Nino duration
            # ------------------------------------------------
            # 2.1 get a time series of 2 years before and 2 years after the El Nino peak (4 years time series)
            # composites
            sample = Event_selection(
                sst, kwargs['frequency'], nbr_years_window=nbr_years_window, list_event_years=event_years)
            if debug is True:
                dict_debug = {'axes1': str([ax.id for ax in sample.getAxisList()]), 'shape1': str(sample.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Event_selection', 15, **dict_debug)

            # 2.2 count the number of consecutive month above a threshold
            if normalize is True:
                duration = DurationAllEvent(sample, 0.5 * float(Std(sst)), nino=True, debug=debug)
            else:
                duration = DurationAllEvent(sample, 0.5, nino=True, debug=debug)

            duration_err = float(Std(duration) / NUMPYsqrt(len(duration)))
            duration_mean = float(duration.mean())

            # Dive down diagnostic
            dive_down_diag = {'value': ArrayToList(duration), 'axis': list(duration.getAxis(0)[:])}

            if netcdf is True:
                if ".nc" in netcdf_name:
                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                else:
                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                dict1 = {'units': Units, 'number_of_years_used': yearN, 'time_period': str(actualtimebounds),
                         'nino_years': str(event_years), 'description': "duration of El Nino events",
                         'diagnostic_value': duration_mean, 'diagnostic_value_error': duration_err}
                dict2 = {'metric_name': Name, 'metric_method': Method, 'metric_reference': Ref,
                         'frequency': kwargs['frequency']}
                SaveNetcdf(file_name, var1=duration, var1_attributes=dict1, var1_name='Nino_duration__' + dataset,
                           global_attributes=dict2)
                del dict1, dict2, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(duration_mean),
                      'line2': 'metric value_error: ' + str(duration_err)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinoDurMetric = {
        'name': Name, 'value': duration_mean, 'value_error': duration_err, 'units': Units, 'method': Method,
        'nyears': yearN, 'events': event_years, 'time_frequency': kwargs['frequency'], 'time_period': actualtimebounds,
        'ref': Ref, 'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinoDurMetric


def NinoSstLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                   sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, box,
                   event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
                   netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSstLonRmse() function computes a zonal composite of El Nino events during the peak of the event
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA > 'threshold' during 'season' are considered as El Nino events
    Then the zonal SSTA at the peak of the event is composited for each selected event

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoLonMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino Zonal Composite'
    lat = ReferenceRegions(box)['latitude']
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + ', zonal SSTA ' \
             + '(meridional averaged [' + str(lat[0]) + ' ; ' + str(lat[1]) + ']'
    Units = '' if kwargs['normalization'] else 'C'
    Ref = 'Using CDAT regridding and rms (uncentered and biased) calculation'
    metric = 'NinoSstLonRmse'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axis': None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                              'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                              'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                              'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                              'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': '(mod) ' + str(event_years_mod), 'nino2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. zonal composite of SSTA
            # ------------------------------------------------
            # Read file and select the right region
            sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                sstfilemod, sstnamemod, 'temperature', metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
                file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
            sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                sstfileobs, sstnameobs, 'temperature', metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
                file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True, maskocean=False,
                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
            if keyerror_mod is not None or keyerror_obs is not None:
                compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                # 2.1 zonal SSTA at the peak of the event is computed for each selected event
                # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                sst_mod, Method, keyerror_mod = PreProcessTS(
                    sst_mod, Method, areacell=mod_areacell, average=False, compute_anom=False, region=box, **kwargs)
                sst_obs, _, keyerror_obs = PreProcessTS(
                    sst_obs, '', areacell=obs_areacell, average=False, compute_anom=False, region=box, **kwargs)
                del mod_areacell, obs_areacell
                if keyerror_mod is not None or keyerror_obs is not None:
                    compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                    dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                else:
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape),
                                      'time1': '(mod) ' + str(TimeBounds(sst_mod)),
                                      'time2': '(obs) ' + str(TimeBounds(sst_obs))}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

                    # Seasonal mean
                    sst_mod = SeasonalMean(sst_mod, season_ev, compute_anom=True)
                    sst_obs = SeasonalMean(sst_obs, season_ev, compute_anom=True)
                    if debug is True:
                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                      'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                      'shape1': '(mod) ' + str(sst_mod.shape), 'shape2': '(obs) ' + str(sst_obs.shape)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

                    # Regridding
                    if isinstance(kwargs['regridding'], dict):
                        known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name',
                                      'regridder', 'regridTool', 'regridMethod'}
                        extra_args = set(kwargs['regridding']) - known_args
                        if extra_args:
                            EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                        sst_mod, sst_obs, Method = TwoVarRegrid(
                            sst_mod, sst_obs, Method, region=box, **kwargs['regridding'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

                    # Meridional average
                    sst_mod, keyerror_mod = AverageMeridional(sst_mod)
                    sst_obs, keyerror_obs = AverageMeridional(sst_obs)
                    if keyerror_mod is not None or keyerror_obs is not None:
                        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
                        dive_down_diag = {'model': None, 'observations': None, 'axis': None}
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    else:
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                        # samples
                        sst_mod = Composite(sst_mod, event_years_mod, kwargs['frequency'])
                        sst_obs = Composite(sst_obs, event_years_obs, kwargs['frequency'])
                        if debug is True:
                            dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                                          'axes2': '(obs) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                                          'shape1': '(mod) ' + str(sst_mod.shape),
                                          'shape2': '(obs) ' + str(sst_obs.shape)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

                        # Computes the root mean square difference
                        compRmse, keyerror = RmsZonal(sst_mod, sst_obs, centered=centered_rmse, biased=biased_rmse)

                        # Error on the metric
                        compRmseErr = None

                        # Dive down diagnostic
                        dive_down_diag = {'model': ArrayToList(sst_mod), 'observations': ArrayToList(sst_obs),
                                          'axis': list(sst_mod.getAxis(0)[:])}
                        if netcdf is True:
                            map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                                sstfilemod, sstnamemod, 'temperature', metric, 'equatorial_pacific_LatExt2',
                                file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                                name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
                                time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
                            map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                                sstfileobs, sstnameobs, 'temperature', metric, 'equatorial_pacific_LatExt2',
                                file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                                name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
                                time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
                            if keyerror_mod is not None or keyerror_obs is not None:
                                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            else:
                                map_mod, _, keyerror_mod = PreProcessTS(
                                    map_mod, '', areacell=mod_areacell, average=False, compute_anom=False,
                                    region="equatorial_pacific_LatExt2", **kwargs)
                                map_obs, _, keyerror_obs = PreProcessTS(
                                    map_obs, '', areacell=obs_areacell, average=False, compute_anom=False,
                                    region="equatorial_pacific_LatExt2", **kwargs)
                                del mod_areacell, obs_areacell
                                if keyerror_mod is not None or keyerror_obs is not None:
                                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                                else:
                                    if debug is True:
                                        dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in map_mod.getAxisList()]),
                                                      'axes2': '(obs) ' + str([ax.id for ax in map_obs.getAxisList()]),
                                                      'shape1': '(mod) ' + str(map_mod.shape),
                                                      'shape2': '(obs) ' + str(map_obs.shape),
                                                      'time1': '(mod) ' + str(TimeBounds(map_mod)),
                                                      'time2': '(obs) ' + str(TimeBounds(map_obs))}
                                        EnsoErrorsWarnings.debug_mode(
                                            '\033[92m', 'after PreProcessTS: netcdf', 15, **dict_debug)
                                    # Seasonal mean
                                    map_mod = SeasonalMean(map_mod, season_ev, compute_anom=True)
                                    map_obs = SeasonalMean(map_obs, season_ev, compute_anom=True)
                                    # Regridding
                                    if isinstance(kwargs['regridding'], dict):
                                        map_mod, map_obs, _ = TwoVarRegrid(
                                            map_mod, map_obs, '', region='equatorial_pacific_LatExt2',
                                            **kwargs['regridding'])
                                        if debug is True:
                                            dict_debug = {
                                                'axes1': '(mod) ' + str([ax.id for ax in map_mod.getAxisList()]),
                                                'axes2': '(obs) ' + str([ax.id for ax in map_obs.getAxisList()]),
                                                'shape1': '(mod) ' + str(map_mod.shape),
                                                'shape2': '(obs) ' + str(map_obs.shape)}
                                            EnsoErrorsWarnings.debug_mode(
                                                '\033[92m', 'after TwoVarRegrid: netcdf', 15, **dict_debug)
                                    # samples
                                    map_mod = Composite(map_mod, event_years_mod, kwargs['frequency'])
                                    map_obs = Composite(map_obs, event_years_obs, kwargs['frequency'])
                                    if ".nc" in netcdf_name:
                                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                    else:
                                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                                             'time_period': str(actualtimebounds_mod),
                                             'nino_years': str(event_years_mod)}
                                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                                             'time_period': str(actualtimebounds_obs),
                                             'nino_years': str(event_years_obs)}
                                    dict3 = {'units': Units, 'number_of_years_used': yearN_mod,
                                             'time_period': str(actualtimebounds_mod),
                                             'nino_years': str(event_years_mod)}
                                    dict4 = {'units': Units, 'number_of_years_used': yearN_obs,
                                             'time_period': str(actualtimebounds_obs),
                                             'nino_years': str(event_years_obs)}
                                    dict5 = {'metric_name': Name, 'metric_value_' + dataset2: compRmse,
                                             'metric_value_error_' + dataset2: compRmseErr, 'metric_method': Method,
                                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                                    SaveNetcdf(
                                        file_name, var1=sst_mod, var1_attributes=dict1,
                                        var1_name='sst_lon__' + dataset1, var2=sst_obs, var2_attributes=dict2,
                                        var2_name='sst_lon__' + dataset2, var3=map_mod, var3_attributes=dict3,
                                        var3_name='sst_map__' + dataset1, var4=map_obs, var4_attributes=dict4,
                                        var4_name='sst_map__' + dataset2, global_attributes=dict5)
                                    del dict1, dict2, dict3, dict4, dict5, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'metric value: ' + str(compRmse), 'line2': 'metric value_error: ' + str(compRmseErr)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinoLonMetric = {
        'name': Name, 'value': compRmse, 'value_error': compRmseErr, 'units': Units, 'method': Method,
        'nyears_model': yearN_mod, 'nyears_observations': yearN_obs, 'events_model': event_years_mod,
        'events_observations': event_years_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag}
    return NinoLonMetric


def NinoSlpMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               slpfilemod, slpnamemod, slpareafilemod, slpareanamemod, slplandmaskfilemod, slplandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
               slpfileobs, slpnameobs, slpareafileobs, slpareanameobs, slplandmaskfileobs, slplandmasknameobs, sstbox,
               slpbox, event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False,
               netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSlpMap() function computes a sea level pressure anomalies composite of during the peak of El Nino events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA > 'threshold' during 'season' are considered as El Nino events
    Then the SLPA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param slpfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SLP
    :param slpnamemod: string
        name of SLP variable (slp) in 'slpfilemod'
    :param slpareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP areacell
    :param slpareanamemod: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafilemod'
    :param slplandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SLP landmask
    :param slplandmasknamemod: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param slpfileobs: string
        path_to/filename of the file (NetCDF) of the observed SLP
    :param slpnameobs: string
        name of SLP variable (slp) in 'slpfileobs'
    :param slpareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP areacell
    :param slpareanameobs: string, optional
        name of areacell for the SLP variable (areacella, areacello,...) in 'slpareafileobs'
    :param slplandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SLP landmask
    :param slplandmasknameobs: string, optional
        name of landmask for the SLP variable (sftlf,...) in 'slplandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param slpbox: string
        name of box (e.g. 'global') for SLP
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoSlpMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino SLPA Composite'
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + \
             ', Nino SLPA Composited'
    Units = '' if kwargs['normalization'] else 'hPa'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinoSlpMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    slp_mod, slp_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        slpfilemod, slpnamemod, 'pressure', metric, slpbox, file_area=slpareafilemod, name_area=slpareanamemod,
        file_mask=slplandmaskfilemod, name_mask=slplandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    slp_obs, slp_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        slpfileobs, slpnameobs, 'pressure', metric, slpbox, file_area=slpareafileobs, name_area=slpareanameobs,
        file_mask=slplandmaskfileobs, name_mask=slplandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, slp_mod, keyerror_mod3 = CheckTime(sst_mod, slp_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, slp_obs, keyerror_obs3 = CheckTime(sst_obs, slp_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        slp_mod, Method, keyerror_mod2 = PreProcessTS(
            slp_mod, Method, areacell=slp_mod_areacell, compute_anom=False, region=slpbox, **kwargs)
        slp_obs, _, keyerror_obs2 = PreProcessTS(
            slp_obs, '', areacell=slp_obs_areacell, compute_anom=False, region=slpbox, **kwargs)
        del mod_areacell, obs_areacell, slp_mod_areacell, slp_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1) or \
                (keyerror_mod2 is not None or keyerror_obs2):
            slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                    'axes4': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod slp) ' + str(slp_mod.shape), 'shape4': '(obs slp) ' + str(slp_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod slp) ' + str(TimeBounds(slp_mod)), 'time4': '(obs slp) ' + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': '(mod) ' + str(event_years_mod), 'nino2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite SLPA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            slp_mod = SeasonalMean(slp_mod, season_ev, compute_anom=True) * 1e-2
            slp_obs = SeasonalMean(slp_obs, season_ev, compute_anom=True) * 1e-2
            if debug is True:
                dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                              'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                              'shape1': '(mod slp) ' + str(slp_mod.shape), 'shape2': '(obs slp) ' + str(slp_obs.shape),
                              'time1': '(mod slp) ' + str(TimeBounds(slp_mod)),
                              'time2': '(obs slp) ' + str(TimeBounds(slp_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                slp_mod, slp_obs, Method = TwoVarRegrid(slp_mod, slp_obs, Method, region=slpbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                                  'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                                  'shape1': '(mod slp) ' + str(slp_mod.shape),
                                  'shape2': '(obs slp) ' + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            slp_mod = Composite(slp_mod, event_years_mod, kwargs['frequency'])
            slp_obs = Composite(slp_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod slp) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                              'axes2': '(obs slp) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                              'shape1': '(mod slp) ' + str(slp_mod.shape), 'shape2': '(obs slp) ' + str(slp_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            slp_mod, keyerror_mod = BasinMask(
                slp_mod, 'pacific', box=slpbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            slp_obs, keyerror_obs = BasinMask(
                slp_obs, 'pacific', box=slpbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                slpCorr, slpCorrErr, slpRmse, slpRmseErr, slpStd, slpStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod) ' + str([ax.id for ax in slp_mod.getAxisList()]),
                                  'axes2': '(obs) ' + str([ax.id for ax in slp_obs.getAxisList()]),
                                  'shape1': '(mod) ' + str(slp_mod.shape), 'shape2': '(obs) ' + str(slp_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                slpRmse, keyerror = RmsAxis(slp_mod, slp_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                slpRmseErr = None
                # Metric 2
                slpCorr = float(Correlation(slp_mod, slp_obs, axis='xy', centered=1, biased=1))
                slpCorrErr = None
                # Metric 3
                std_mod = Std(slp_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(slp_obs, weights=None, axis='xy', centered=1, biased=1)
                slpStd = float(std_mod) / float(std_obs)
                slpStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                             'time_period': str(actualtimebounds_mod),
                             'nino_years': str(event_years_mod)}
                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                             'time_period': str(actualtimebounds_obs),
                             'nino_years': str(event_years_obs)}
                    dict3 = {'metric_name': Name, 'metric_valueRMSE_' + dataset2: slpRmse,
                             'metric_valueRMSE_error_' + dataset2: slpRmseErr, 'metric_valueCORR_' + dataset2: slpCorr,
                             'metric_valueCORR_error_' + dataset2: slpCorrErr, 'metric_valueSTD_' + dataset2: slpStd,
                             'metric_valueCORR_error_' + dataset2: slpStdErr, 'metric_method': Method,
                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(file_name, var1=slp_mod, var1_attributes=dict1, var1_name='slp_map__' + dataset1,
                               var2=slp_obs, var2_attributes=dict2, var2_name='slp_map__' + dataset2,
                               global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if slpCorr is not None:
        slpCorr = 1 - slpCorr
    # Create output
    NinoSlpMapMetric = {
        'name': Name, 'Rmse__value': slpRmse, 'Rmse__value_error': slpRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': slpCorr, 'Corr__value_error': slpCorrErr, 'Corr__units': '', 'Std__value': slpStd,
        'Std__value_error': slpStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinoSlpMapMetric


def NinoSstDiversity(sstfile, sstname, sstareafile, sstareaname, sstlandmaskfile, sstlandmaskname, box,
                     event_definition, dataset='', debug=False, netcdf=False, netcdf_name='', metname='', **kwargs):
    """
    The NinoSstDiversity() function computes a zonal composite of El Nino events during the peak of the event.
        1.) detect events
            1.1) SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
            1.2) SSTA > 'threshold' during 'season' are considered as El Nino events
        2.) diversity of the zonal location of the maximum SSTA
            2.1) zonal SSTA at the peak of the event is computed for each selected event
            2.2) find the zonal position of the maximum SSTA for each selected event
            2.3) compute the spread of the distribution

    Inputs:
    ------
    :param sstfile: string
        path_to/filename of the file (NetCDF) of the SST
    :param sstname: string
        name of SST variable (tos, ts) in 'sstfile'
    :param sstareafile: string
        path_to/filename of the file (NetCDF) of the areacell for SST
    :param sstareaname: string
        name of areacell variable (areacella, areacello) in 'sstareafile'
    :param sstlandmaskfile: string
        path_to/filename of the file (NetCDF) of the landmask for SST
    :param sstlandmaskname: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfile'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param dataset: string, optional
        name of current dataset (e.g., 'model', 'obs', ...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param treshold_ep_ev: float, optional
        see EnsoToolsLib.percentage_val_eastward
        longitude, in degree east, of the westward boundary of eastern Pacific event
        default value is -140°E (i.e., 140°W)
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoDivMetric: dict
        name, value, value_error, units, method, nyears, events, time_frequency, time_period, ref, keyerror,
        dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    my_thresh = 'std' if normalize is True else 'C'
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'treshold_ep_ev',
                    'time_bounds']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "El Nino Diversity (interquartile range)"
    lat = ReferenceRegions(box)['latitude']
    lon = ReferenceRegions(box)['longitude']

    Method = "Nino events = " + region_ev + " SSTA > " + str(threshold) + my_thresh + " during " + season_ev + \
             ", zonal SSTA (meridional averaged [" + str(lat[0]) + " ; " + str(lat[1]) + "]), the zonal SSTA " + \
             "maximum is located for each event, the diversity is the interquartile range (IQR = Q3 - Q1)"
    Units = 'long'
    Ref = 'Using CDAT regridding'
    metric = 'NinoSstDiversity'
    if metname == '':
        metname = deepcopy(metric)

    # ------------------------------------------------
    # 1. detect events
    # ------------------------------------------------
    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst, sst_areacell, keyerror1 = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, region_ev, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)
    sstmap, sstmap_areacell, keyerror2 = Read_data_mask_area(
        sstfile, sstname, 'temperature', metric, box, file_area=sstareafile, name_area=sstareaname,
        file_mask=sstlandmaskfile, name_mask=sstlandmaskname, maskland=True, maskocean=False, debug=debug, **kwargs)

    # Number of years
    yearN = int(round(sst.shape[0] / 12))

    # Time period
    actualtimebounds = TimeBounds(sst)

    keyerror = add_up_errors([keyerror1, keyerror2])
    if keyerror is not None:
        dispersion1, dispersion1_err, dispersion2, dispersion2_err, event_years = None, None, None, None, None
        dive_down_diag = None, None, {'value': None, 'axis': None}
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        enso, _, keyerror1 = PreProcessTS(
            sst, '', areacell=sst_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sstmap, Method, keyerror2 = PreProcessTS(
            sstmap, Method, areacell=sstmap_areacell, average=False, compute_anom=False, region=box, **kwargs)
        del sst_areacell, sstmap_areacell
        if keyerror1 is not None or keyerror2 is not None:
            dispersion1, dispersion1_err, dispersion2, dispersion2_err, event_years = None, None, None, None, None
            dive_down_diag = None, None, {'value': None, 'axis': None}
        else:
            if debug is True:
                dict_debug = {'axes1': '(sst enso) ' + str([ax.id for ax in enso.getAxisList()]),
                              'axes2': '(sst map) ' + str([ax.id for ax in sstmap.getAxisList()]),
                              'shape1': '(sst enso) ' + str(enso.shape), 'shape2': '(sst map) ' + str(sstmap.shape),
                              'time1': '(sst enso) ' + str(TimeBounds(enso)),
                              'time2': '(sst map) ' + str(TimeBounds(sstmap))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years = DetectEvents(enso, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': 'nbr(' + str(len(event_years)) + '): ' + str(event_years)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. diversity of the zonal location of the maximum SSTA
            # ------------------------------------------------
            # 2.1 zonal SSTA at the peak of the event is computed for each selected event
            # Seasonal mean
            sstmap = SeasonalMean(sstmap, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sstmap.getAxisList()]),
                              'shape1': '(sst) ' + str(sstmap.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder', 'regridTool',
                              'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sstmap = Regrid(sstmap, None, region=box, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sstmap.getAxisList()]),
                                  'shape1': '(sst) ' + str(sstmap.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # Meridional average
            sstlon, keyerror = AverageMeridional(sstmap)
            if keyerror is not None:
                dispersion1, dispersion1_err, dispersion2, dispersion2_err = None, None, None, None
                dive_down_diag = None, None, {'value': None, 'axis': None}
            else:
                if debug is True:
                    dict_debug = {'axes1': '(sst) ' + str([ax.id for ax in sstlon.getAxisList()]),
                                  'shape1': '(sst) ' + str(sstlon.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after AverageMeridional', 15, **dict_debug)

                if len(event_years) > 0:
                    # samples
                    sample = Event_selection(sstlon, kwargs['frequency'], list_event_years=event_years)

                    # 2.2 find the zonal position of the maximum SSTA for each selected event
                    lon_sstmax = FindXYMinMaxInTs(
                        sample, return_val='maxi', smooth=True, axis=0, window=5, method='triangle')
                    if debug is True:
                        dict_debug = {'line1': 'longitude of the maximum SSTA: ' + str(lon_sstmax)}
                        EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)

                    # 2.3 compute the spread of the distribution
                    dispersion1 = statistical_dispersion(lon_sstmax, method='IQR')
                    dispersion2 = statistical_dispersion(lon_sstmax, method='MAD')
                    dispersion1_err = None
                    dispersion2_err = None
                else:
                    lon_sstmax = MyEmpty(sstlon[:5, 0], time=True, time_id='years')
                    dispersion1 = None
                    dispersion2 = None
                    dispersion1_err = None
                    dispersion2_err = None

                # Dive down diagnostic
                dive_down_diag = {'value': ArrayToList(lon_sstmax), 'axis': list(lon_sstmax.getAxis(0)[:])}
                if netcdf is True:
                    # nina events
                    nina_years = DetectEvents(enso, season_ev, -threshold, normalization=normalize, nino=False)
                    if len(event_years) > 0:
                        # samples
                        sample = Event_selection(sstlon, kwargs['frequency'], list_event_years=nina_years)
                        # 2.2 find the zonal position of the maximum SSTA for each selected event
                        lon_sstmin = FindXYMinMaxInTs(
                            sample, return_val='mini', smooth=True, axis=0, window=5, method='triangle')
                        if debug is True:
                            dict_debug = {'line1': 'longitude of the minimum SSTA: ' + str(lon_sstmax)}
                            EnsoErrorsWarnings.debug_mode('\033[92m', 'after FindXYMinMaxInTs', 15, **dict_debug)
                        # 2.3 compute the spread of the distribution
                        nina_disp1 = statistical_dispersion(lon_sstmin, method='IQR')
                        nina_disp2 = statistical_dispersion(lon_sstmin, method='MAD')
                        nina_disp1_err = None
                        nina_disp2_err = None
                    else:
                        lon_sstmin = MyEmpty(sstlon[:5, 0], time=True, time_id='years')
                        nina_disp1 = None
                        nina_disp2 = None
                        nina_disp1_err = None
                        nina_disp2_err = None
                    # save
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {'units': Units, 'number_of_years_used': yearN, 'time_period': str(actualtimebounds),
                             'nino_years': str(event_years),
                             'description':
                                 "Nino events = " + region_ev + " SSTA > " + str(threshold) + my_thresh + " during " +
                                 season_ev + ", zonal SSTA " + "(meridional averaged [" + str(lat[0]) + " ; " +
                                 str(lat[1]) + "]), the zonal SSTA maximum is located for each event, the diversity " +
                                 "is the interquartile range (IQR = Q3 - Q1), second value is the median absolute " +
                                 "deviation (MAD = median([Xi - median(tab)]))",
                             'diagnostic_value_' + dataset: dispersion1,
                             'diagnostic_value_error_' + dataset: dispersion1_err,
                             'diagnostic_value2_' + dataset: dispersion2,
                             'diagnostic_value_error2_' + dataset: dispersion2_err}
                    dict2 = {'units': Units, 'number_of_years_used': yearN, 'time_period': str(actualtimebounds),
                             'nino_years': str(nina_years),
                             'description':
                                 "Nina events = " + region_ev + " SSTA < -" + str(threshold) + my_thresh + " during " +
                                 season_ev + ", zonal SSTA " + "(meridional averaged [" + str(lat[0]) + " ; " +
                                 str(lat[1]) + "]), the zonal SSTA minimum is located for each event, the diversity " +
                                 "is the interquartile range (IQR = Q3 - Q1), second value is the median absolute " +
                                 "deviation (MAD = median([Xi - median(tab)]))",
                             'diagnostic_value_' + dataset: nina_disp1,
                             'diagnostic_value_error_' + dataset: nina_disp1_err,
                             'diagnostic_value2_' + dataset: nina_disp2,
                             'diagnostic_value_error2_' + dataset: nina_disp2_err}
                    dict3 = {'metric_name': Name, 'metric_method': Method, 'metric_reference': Ref,
                             'frequency': kwargs['frequency']}
                    SaveNetcdf(file_name, var1=lon_sstmax, var1_attributes=dict1,
                               var1_name='Nino_lon_pos_maxSSTA__' + dataset, var2=lon_sstmin, var2_attributes=dict2,
                               var2_name='Nina_lon_pos_minSSTA__' + dataset, global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    # metric value
    if debug is True:
        dict_debug = {'line1': 'diagnostic (IQR) value: ' + str(dispersion1),
                      'line2': 'diagnostic (IQR) value_error: ' + str(dispersion1_err),
                      'line3': 'diagnostic (MAD) value: ' + str(dispersion2),
                      'line4': 'diagnostic (MAD) value_error: ' + str(dispersion2_err)}
        EnsoErrorsWarnings.debug_mode('\033[92m', 'end of ' + metric, 10, **dict_debug)
    # Create output
    NinoDivMetric = {
        'name': Name, 'value': dispersion1, 'value_error': dispersion1_err, 'value2': dispersion2,
        'value_error2': dispersion2_err, 'units': Units, 'method': Method, 'nyears': yearN, 'events': event_years,
        'time_frequency': kwargs['frequency'], 'time_period': actualtimebounds, 'ref': Ref, 'keyerror': keyerror,
        'dive_down_diag': dive_down_diag}
    return NinoDivMetric


def NinoSstMap(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
               sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs, tsbox,
               event_definition, centered_rmse=0, biased_rmse=1, dataset1='', dataset2='', debug=False, netcdf=False,
               netcdf_name='', metname='', **kwargs):
    """
    The NinoSstMap() function computes a surface temperature anomalies composite of during the peak of El Nino events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA > 'threshold' during 'season' are considered as El Nino events
    Then the TSA at the peak of the event is composited for each selected event
    First metric: rmse(observations vs model).
    Second metric: correlation(observations vs model).
    Third metric: std(model)/std(observations)
    These metrics can be used to compute a Taylor diagram.

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST areacell
    :param sstareanamemod: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafilemod'
    :param sstlandmaskfilemod: string, optional
        path_to/filename of the file (NetCDF) of the modeled SST landmask
    :param sstlandmasknamemod: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST areacell
    :param sstareanameobs: string, optional
        name of areacell for the SST variable (areacella, areacello,...) in 'sstareafileobs'
    :param sstlandmaskfileobs: string, optional
        path_to/filename of the file (NetCDF) of the observed SST landmask
    :param sstlandmasknameobs: string, optional
        name of landmask for the SST variable (sftlf,...) in 'sstlandmaskfileobs'
    :param sstbox: string
        name of box (e.g. 'global') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoSstMapMetric: dict
        name, Rmse__value (rms [obs;model]), Rmse__value_error, Rmse__units, method, Corr__value (corr [obs;model]),
        Corr__value_error, Corr__units, Std__value (std_model / std_obs), Std__value_error, Std__units, nyears_model,
        nyears_observations, time_frequency, time_period_mod, time_period_obs, ref, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition['region_ev']
    season_ev = event_definition['season_ev']
    threshold = event_definition['threshold']
    normalize = event_definition['normalization']
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'smoothing', 'time_bounds_mod',
                    'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = 'El Nino TSA Composite'
    Method = 'Nino events = ' + region_ev + ' sstA > ' + str(threshold) + ' during ' + season_ev + \
             ', Nino TSA Composited'
    Units = '' if kwargs['normalization'] else 'mm/day'
    Ref = 'Using CDAT regridding, correlation (centered and biased), std (centered and biased) and ' + \
          'rms (uncentered and biased) calculation'
    metric = 'NinoSstMap'
    if metname == '':
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode('\033[92m', metric, 10)
    sst_mod, mod_areacell, keyerror_mod1 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs1 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)
    tsmap_mod, tsmap_mod_areacell, keyerror_mod2 = Read_data_mask_area(
        sstfilemod, sstnamemod, 'temperature', metric, tsbox, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_mod'], debug=debug, **kwargs)
    tsmap_obs, tsmap_obs_areacell, keyerror_obs2 = Read_data_mask_area(
        sstfileobs, sstnameobs, 'temperature', metric, tsbox, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=False, maskocean=False,
        time_bounds=kwargs['time_bounds_obs'], debug=debug, **kwargs)

    # Checks if the same time period is used for both variables and if the minimum number of time steps is respected
    sst_mod, tsmap_mod, keyerror_mod3 = CheckTime(sst_mod, tsmap_mod, metric_name=metric, debug=debug, **kwargs)
    sst_obs, tsmap_obs, keyerror_obs3 = CheckTime(sst_obs, tsmap_obs, metric_name=metric, debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if (keyerror_mod1 is not None or keyerror_obs1 is not None or keyerror_mod2 is not None) or \
            (keyerror_obs2 is not None or keyerror_mod3 is not None or keyerror_obs3 is not None):
        tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
        dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
        tmp = [keyerror_mod1, keyerror_mod2, keyerror_mod3, keyerror_obs1, keyerror_obs2, keyerror_obs3]
        keyerror = add_up_errors(tmp)
    else:
        # ------------------------------------------------
        # 1. detect events
        # ------------------------------------------------
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod1 = PreProcessTS(
            sst_mod, '', areacell=mod_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs1 = PreProcessTS(
            sst_obs, '', areacell=obs_areacell, average='horizontal', compute_anom=False, region=region_ev, **kwargs)
        tsmap_mod, Method, keyerror_mod2 = PreProcessTS(
            tsmap_mod, Method, areacell=tsmap_mod_areacell, compute_anom=False, region=tsbox, **kwargs)
        tsmap_obs, _, keyerror_obs2 = PreProcessTS(
            tsmap_obs, '', areacell=tsmap_obs_areacell, compute_anom=False, region=tsbox, **kwargs)
        del mod_areacell, obs_areacell, tsmap_mod_areacell, tsmap_obs_areacell
        if (keyerror_mod1 is not None or keyerror_obs1 is not None) or \
                (keyerror_obs2 is not None or keyerror_mod2 is not None):
            tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
            dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
            keyerror = add_up_errors([keyerror_mod1, keyerror_mod2, keyerror_obs1, keyerror_obs2])
        else:
            if debug is True:
                dict_debug = {
                    'axes1': '(mod sst) ' + str([ax.id for ax in sst_mod.getAxisList()]),
                    'axes2': '(obs sst) ' + str([ax.id for ax in sst_obs.getAxisList()]),
                    'axes3': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                    'axes4': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                    'shape1': '(mod sst) ' + str(sst_mod.shape), 'shape2': '(obs sst) ' + str(sst_obs.shape),
                    'shape3': '(mod ts) ' + str(tsmap_mod.shape), 'shape4': '(obs ts) ' + str(tsmap_obs.shape),
                    'time1': '(mod sst) ' + str(TimeBounds(sst_mod)), 'time2': '(obs sst) ' + str(TimeBounds(sst_obs)),
                    'time3': '(mod ts) ' + str(TimeBounds(tsmap_mod)),
                    'time4': '(obs ts) ' + str(TimeBounds(tsmap_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after PreProcessTS', 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {'nino1': '(mod) ' + str(event_years_mod), 'nino2': '(obs) ' + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after DetectEvents', 15, **dict_debug)

            # ------------------------------------------------
            # 2. composite TSA
            # ------------------------------------------------
            # 2.2 Seasonal mean and anomalies
            tsmap_mod = SeasonalMean(tsmap_mod, season_ev, compute_anom=True)
            tsmap_obs = SeasonalMean(tsmap_obs, season_ev, compute_anom=True)
            if debug is True:
                dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                              'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                              'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                              'shape2': '(obs ts) ' + str(tsmap_obs.shape),
                              'time1': '(mod ts) ' + str(TimeBounds(tsmap_mod)),
                              'time2': '(obs ts) ' + str(TimeBounds(tsmap_obs))}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after SeasonalMean', 15, **dict_debug)

            # Regridding
            if isinstance(kwargs['regridding'], dict):
                known_args = {'model_orand_obs', 'newgrid', 'missing', 'order', 'mask', 'newgrid_name', 'regridder',
                              'regridTool', 'regridMethod'}
                extra_args = set(kwargs['regridding']) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tsmap_mod, tsmap_obs, Method = TwoVarRegrid(
                    tsmap_mod, tsmap_obs, Method, region=tsbox, **kwargs['regridding'])
                if debug is True:
                    dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                                  'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                                  'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                                  'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after TwoVarRegrid', 15, **dict_debug)

            # 2.3 Composites
            tsmap_mod = Composite(tsmap_mod, event_years_mod, kwargs['frequency'])
            tsmap_obs = Composite(tsmap_obs, event_years_obs, kwargs['frequency'])
            if debug is True:
                dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                              'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                              'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                              'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                EnsoErrorsWarnings.debug_mode('\033[92m', 'after Composite', 15, **dict_debug)

            # mask Pacific
            tsmap_mod, keyerror_mod = BasinMask(
                tsmap_mod, 'pacific', box=tsbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            tsmap_obs, keyerror_obs = BasinMask(
                tsmap_obs, 'pacific', box=tsbox, lat1=-15, lat2=15, latkey='between', debug=debug)
            if keyerror_mod is not None or keyerror_obs is not None:
                tsCorr, tsCorrErr, tsRmse, tsRmseErr, tsStd, tsStdErr = None, None, None, None, None, None
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            else:
                if debug is True:
                    dict_debug = {'axes1': '(mod ts) ' + str([ax.id for ax in tsmap_mod.getAxisList()]),
                                  'axes2': '(obs ts) ' + str([ax.id for ax in tsmap_obs.getAxisList()]),
                                  'shape1': '(mod ts) ' + str(tsmap_mod.shape),
                                  'shape2': '(obs ts) ' + str(tsmap_obs.shape)}
                    EnsoErrorsWarnings.debug_mode('\033[92m', 'after BasinMask', 15, **dict_debug)

                # Metric 1
                tsRmse, keyerror = RmsAxis(tsmap_mod, tsmap_obs, axis='xy', centered=centered_rmse, biased=biased_rmse)
                tsRmseErr = None
                # Metric 2
                tsCorr = float(Correlation(tsmap_mod, tsmap_obs, axis='xy', centered=1, biased=1))
                tsCorrErr = None
                # Metric 3
                std_mod = Std(tsmap_mod, weights=None, axis='xy', centered=1, biased=1)
                std_obs = Std(tsmap_obs, weights=None, axis='xy', centered=1, biased=1)
                tsStd = float(std_mod) / float(std_obs)
                tsStdErr = None

                # Dive down diagnostic
                dive_down_diag = {'model': None, 'observations': None, 'axisLat': None, 'axisLon': None}
                if netcdf is True:
                    if ".nc" in netcdf_name:
                        file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                    else:
                        file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                    dict1 = {'units': Units, 'number_of_years_used': yearN_mod,
                             'time_period': str(actualtimebounds_mod), 'nino_years': str(event_years_mod)}
                    dict2 = {'units': Units, 'number_of_years_used': yearN_obs,
                             'time_period': str(actualtimebounds_obs), 'nino_years': str(event_years_obs)}
                    dict3 = {'metric_name': Name, 'metric_valueRMSE_' + dataset2: tsRmse,
                             'metric_valueRMSE_error_' + dataset2: tsRmseErr, 'metric_valueCORR_' + dataset2: tsCorr,
                             'metric_valueCORR_error_' + dataset2: tsCorrErr, 'metric_valueSTD_' + dataset2: tsStd,
                             'metric_valueCORR_error_' + dataset2: tsStdErr, 'metric_method': Method,
                             'metric_reference': Ref, 'frequency': kwargs['frequency']}
                    SaveNetcdf(file_name, var1=tsmap_mod, var1_attributes=dict1, var1_name='ts_map__' + dataset1,
                               var2=tsmap_obs, var2_attributes=dict2, var2_name='ts_map__' + dataset2,
                               global_attributes=dict3)
                    del dict1, dict2, dict3, file_name
    if tsCorr is not None:
        tsCorr = 1 - tsCorr
    # Create output
    NinoSstMapMetric = {
        'name': Name, 'Rmse__value': tsRmse, 'Rmse__value_error': tsRmseErr, 'Rmse__units': Units, 'method': Method,
        'Corr__value': tsCorr, 'Corr__value_error': tsCorrErr, 'Corr__units': '', 'Std__value': tsStd,
        'Std__value_error': tsStdErr, 'Std__units': Units + ' / ' + Units, 'nyears_model': yearN_mod,
        'nyears_observations': yearN_obs, 'time_frequency': kwargs['frequency'],
        'time_period_model': actualtimebounds_mod, 'time_period_observations': actualtimebounds_obs, 'ref': Ref,
        'keyerror': keyerror, 'dive_down_diag': dive_down_diag, 'units': ''}
    return NinoSstMapMetric


def NinoSstTsRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                  sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                  box, event_definition, nbr_years_window, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="",
                  debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The NinoSstTsRmse() function computes a time composite of El Nino events
    SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        Then SSTA > 'threshold' during 'season' are considered as El Nino events
        Then a 'nbr_years_window' long time series centered on selected events is composited for each selected event

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('nino3') for SST
    :param event_definition: dict
        dictionary providing the necessary information to detect ENSO events (region_ev, season_ev, threshold)
        e.g., event_definition = {'region_ev': 'nino3', 'season_ev': 'DEC', 'threshold': 0.75}
    :param nbr_years_window: integer
        number of years used to compute the composite (e.g. 6)
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'HadISST',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds: tuple, optional
        tuple of the first and last dates to extract from the files (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return NinoTsMetric: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, events_model, events_observations,
        time_frequency, time_period_model, time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    """
    # setting variables
    region_ev = event_definition["region_ev"]
    season_ev = event_definition["season_ev"]
    threshold = event_definition["threshold"]
    normalize = event_definition["normalization"]
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "smoothing", "time_bounds_mod",
                    "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "El Nino Composite Time Series"
    Method = "Nino events = " + region_ev + " sstA > " + str(threshold) + " during " + season_ev + ", time series of " \
             + str(nbr_years_window) + " years (centered on events)"
    Units = "" if kwargs["normalization"] else "C"
    Ref = "Using CDAT rms (uncentered and biased) calculation"
    metric = "NinoSstTsRmse"
    if metname == "":
        metname = deepcopy(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, region_ev, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, region_ev, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    if keyerror_mod is not None or keyerror_obs is not None:
        compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
        dive_down_diag = {"model": None, "observations": None, "axis": None}
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    else:
        # 1.1 SSTA averaged in 'region_ev' are normalized / detrended / smoothed (running average) if applicable
        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        sst_mod, _, keyerror_mod = PreProcessTS(
            sst_mod, "", areacell=mod_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, average="horizontal", compute_anom=False, region=region_ev, **kwargs)
        del mod_areacell, obs_areacell
        if keyerror_mod is not None or keyerror_obs is not None:
            compRmse, compRmseErr, event_years_mod, event_years_obs = None, None, None, None
            dive_down_diag = {"model": None, "observations": None, "axis": None}
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        else:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape),
                              "time1": "(mod) " + str(TimeBounds(sst_mod)),
                              "time2": "(obs) " + str(TimeBounds(sst_obs))}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # 1.2 SSTA > 'threshold' during 'season' are considered as El Nino events
            # Lists event years
            event_years_mod = DetectEvents(sst_mod, season_ev, threshold, normalization=normalize, nino=True)
            event_years_obs = DetectEvents(sst_obs, season_ev, threshold, normalization=normalize, nino=True)
            if debug is True:
                dict_debug = {"nino1": "(mod) " + str(event_years_mod), "nino2": "(obs) " + str(event_years_obs)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after DetectEvents", 15, **dict_debug)

            # ------------------------------------------------
            # 2. temporal composite of SSTA
            # ------------------------------------------------
            # interannual anomalies
            sst_mod = ComputeInterannualAnomalies(sst_mod)
            sst_obs = ComputeInterannualAnomalies(sst_obs)

            # composites
            composite_mod = Composite(sst_mod, event_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
            composite_obs = Composite(sst_obs, event_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in composite_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in composite_obs.getAxisList()]),
                              "shape1": "(mod) " + str(composite_mod.shape),
                              "shape2": "(obs) " + str(composite_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Composite", 15, **dict_debug)

            # Computes the root mean square difference
            compRmse, keyerror = RmsAxis(
                composite_mod, composite_obs, axis=0, centered=centered_rmse, biased=biased_rmse)

            # Error on the metric
            compRmseErr = None

            # Dive down diagnostic
            dive_down_diag = {"model": ArrayToList(composite_mod), "observations": ArrayToList(composite_obs),
                              "axis": list(composite_mod.getAxis(0)[:])}
            if netcdf is True:
                # Read file and select the right region
                sst_hov_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                    sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific", file_area=sstareafilemod,
                    name_area=sstareanamemod, file_mask=sstlandmaskfilemod, name_mask=sstlandmaskfilemod, maskland=True,
                    maskocean=False, time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                sst_hov_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                    sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific", file_area=sstareafileobs,
                    name_area=sstareanameobs, file_mask=sstlandmaskfileobs, name_mask=sstlandmaskfileobs, maskland=True,
                    maskocean=False, time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                if keyerror is None:
                    # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
                    sst_hov_mod, _, keyerror_mod = PreProcessTS(
                        sst_hov_mod, Method, areacell=mod_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    sst_hov_obs, _, keyerror_obs = PreProcessTS(
                        sst_hov_obs, "", areacell=obs_areacell, average=False, compute_anom=True,
                        region="equatorial_pacific", **kwargs)
                    del mod_areacell, obs_areacell
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        if debug is True:
                            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                          "axes2": "(obs) " + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                          "shape1": "(mod) " + str(sst_hov_mod.shape),
                                          "shape2": "(obs) " + str(sst_hov_obs.shape),
                                          "time1": "(mod) " + str(TimeBounds(sst_hov_mod)),
                                          "time2": "(obs) " + str(TimeBounds(sst_hov_obs))}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                        # Regridding
                        if "regridding" not in list(kwargs.keys()):
                            kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf", "regridMethod": "linear",
                                                    "newgrid_name": "generic_1x1deg"}
                        else:
                            if not isinstance(kwargs["regridding"], dict):
                                kwargs["regridding"] = {"regridder": "cdms", "regridTool": "esmf",
                                                        "regridMethod": "linear", "newgrid_name": "generic_1x1deg"}
                        sst_hov_mod, sst_hov_obs, Method = TwoVarRegrid(
                            sst_hov_mod, sst_hov_obs, Method, region="equatorial_pacific", **kwargs["regridding"])
                        if debug is True:
                            dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                          "axes2": "(obs) " + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                          "shape1": "(mod) " + str(sst_hov_mod.shape),
                                          "shape2": "(obs) " + str(sst_hov_obs.shape)}
                            EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)
                        # Meridional average
                        sst_hov_mod, keyerror_mod = AverageMeridional(sst_hov_mod)
                        sst_hov_obs, keyerror_obs = AverageMeridional(sst_hov_obs)
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(sst_hov_mod.shape),
                                              "shape2": "(obs) " + str(sst_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)
                            # samples
                            sst_hov_mod = Composite(
                                sst_hov_mod, event_years_mod, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            sst_hov_obs = Composite(
                                sst_hov_obs, event_years_obs, kwargs["frequency"], nbr_years_window=nbr_years_window)
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_hov_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in sst_hov_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(sst_hov_mod.shape),
                                              "shape2": "(obs) " + str(sst_hov_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after Composite", 15, **dict_debug)
                            if ".nc" in netcdf_name:
                                file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                            else:
                                file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                            dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "nino_years": str(event_years_mod),
                                     "description": "time series of " + box + " sstA centered on El Nino peak"}
                            dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "nino_years": str(event_years_obs),
                                     "description": "time series of " + box + " sstA centered on El Nino peak"}
                            dict3 = {"units": Units, "number_of_years_used": yearN_mod,
                                     "time_period": str(actualtimebounds_mod), "nino_years": str(event_years_mod),
                                     "description": "zonal monthly of equatorial_pacific sstA centered on El Nino peak"}
                            dict4 = {"units": Units, "number_of_years_used": yearN_obs,
                                     "time_period": str(actualtimebounds_obs), "nino_years": str(event_years_obs),
                                     "description": "zonal monthly of equatorial_pacific sstA centered on El Nino peak"}
                            dict5 = {"metric_name": Name, "metric_value_" + dataset2: compRmse,
                                     "metric_value_error_" + dataset2: compRmseErr, "metric_method": Method,
                                     "metric_reference": Ref, "frequency": kwargs["frequency"]}
                            SaveNetcdf(
                                file_name, var1=composite_mod, var1_attributes=dict1, var1_name="sst_ts__" + dataset1,
                                var2=composite_obs, var2_attributes=dict2, var2_name="sst_ts__" + dataset2,
                                var3=sst_hov_mod, var3_attributes=dict3, var3_name="sst_hov__" + dataset1,
                                var4=sst_hov_obs, var4_attributes=dict4, var4_name="sst_hov__" + dataset2,
                                global_attributes=dict5)
                            del dict1, dict2, dict3, dict4, dict5, file_name
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(compRmse), "line2": "metric value_error: " + str(compRmseErr)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    NinoTsMetric = {
        "name": Name, "value": compRmse, "value_error": compRmseErr, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "events_model": event_years_mod,
        "events_observations": event_years_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return NinoTsMetric


def SeasonalPrLatRmse(prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                      prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs,
                      box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                      netcdf_name="", metname="", **kwargs):
    """
    The SeasonalPrLatRmse() function computes the climatological (12 months) PR meridional (latitude) standard
    deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the equatorial_pacific_LatExt)

    Inputs:
    ------
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for PR
    :param prareanamemod: string
        name of areacell variable (areacella, areacello) in 'prareafilemod'
    :param prlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for PR
    :param prlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfilemod'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for PR
    :param prareanameobs: string
        name of areacell variable (areacella, areacello) in 'prareafileobs'
    :param prlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for PR
    :param prlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for PR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalPrLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "pr meridional seasonality RMSE"
    Units = "mm/day"
    Method = "Meridional root mean square error of " + box + " climatological pr STD"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalPrLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    pr_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, box, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, box, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(pr_mod.shape[0] / 12))
    yearN_obs = int(round(pr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(pr_mod)
    actualtimebounds_obs = TimeBounds(pr_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        pr_mod, Method, keyerror_mod = PreProcessTS(
            pr_mod, Method, areacell=mod_areacell, compute_sea_cycle=True, region=box, **kwargs)
        pr_obs, _, keyerror_obs = PreProcessTS(
            pr_obs, "", areacell=obs_areacell, compute_sea_cycle=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # standard deviation computation
            prStd_mod = Std(pr_mod)
            prStd_obs = Std(pr_obs)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in prStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(prStd_mod.shape), "shape2": "(obs) " + str(prStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                prStd_mod, prStd_obs, Method = TwoVarRegrid(
                    prStd_mod, prStd_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStd_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in prStd_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(prStd_mod.shape),
                                  "shape2": "(obs) " + str(prStd_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            prStdLat_mod, keyerror_mod = AverageZonal(prStd_mod)
            prStdLat_obs, keyerror_obs = AverageZonal(prStd_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStdLat_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in prStdLat_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(prStdLat_mod.shape),
                                  "shape2": "(obs) " + str(prStdLat_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(prStdLat_mod, prStdLat_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(prStdLat_mod, prStdLat_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(prStdLat_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(prStdLat_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(prStdLat_mod), "observations": ArrayToList(prStdLat_obs),
                                  "axis": list(prStdLat_mod.getAxis(0)[:])}
                if netcdf is True:
                    # Read file and select the right region
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitations", metric, "equatorial_pacific_LatExt2",
                        file_area=prareafilemod, name_area=prareanamemod, file_mask=prlandmaskfilemod,
                        name_mask=prlandmaskfilemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitations", metric, "equatorial_pacific_LatExt2",
                        file_area=prareafileobs, name_area=prareanameobs, file_mask=prlandmaskfileobs,
                        name_mask=prlandmaskfileobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess pr (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(map_mod.shape),
                                              "shape2": "(obs) " + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # standard deviation computation
                            map_mod = Std(map_mod)
                            map_obs = Std(map_obs)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                                pr_mod, pr_obs, _ = TwoVarRegrid(pr_mod, pr_obs, "", region=box, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(pr_mod.shape),
                                                  "shape2": "(obs) " + str(pr_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
                            # Zonal average
                            pr_mod, keyerror_mod = AverageZonal(pr_mod)
                            pr_obs, keyerror_obs = AverageZonal(pr_obs)
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(pr_mod.shape),
                                                  "shape2": "(obs) " + str(pr_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)
                                # Supplementary metrics
                                dict_metric, dict_nc = dict(), dict()
                                dict_metric, dict_nc = fill_dict_axis(
                                    map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="map of the standard deviation of the mean annual cycle of PR")
                                dict_metric, dict_nc = fill_dict_axis(
                                    pr_mod, pr_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 5, ovar[2], "hov", Units, "01", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="mean annual cycle of PR across latitudes")
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of PR across latitudes"}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of PR across latitudes"}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_value_" + dataset2: mv,
                                    "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2 + "_lat": sm_corr,
                                    "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                    "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=prStdLat_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=prStdLat_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalPrLonRmse(prfilemod, prnamemod, prareafilemod, prareanamemod, prlandmaskfilemod, prlandmasknamemod,
                      prfileobs, prnameobs, prareafileobs, prareanameobs, prlandmaskfileobs, prlandmasknameobs,
                      box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                      netcdf_name="", metname="", **kwargs):
    """
    The SeasonalPrLonRmse() function computes the climatological (12 months) PR zonal (longitude) standard
    deviation root mean square error (RMSE) in a 'box' (usually the Equatorial Pacific)

    Inputs:
    ------
    :param prfilemod: string
        path_to/filename of the file (NetCDF) of the modeled PR
    :param prnamemod: string
        name of PR variable (pr) in 'prfilemod'
    :param prareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for PR
    :param prareanamemod: string
        name of areacell variable (areacella, areacello) in 'prareafilemod'
    :param prlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for PR
    :param prlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfilemod'
    :param prfileobs: string
        path_to/filename of the file (NetCDF) of the observed PR
    :param prnameobs: string
        name of PR variable (pr, precip) in 'prfileobs'
    :param prareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for PR
    :param prareanameobs: string
        name of areacell variable (areacella, areacello) in 'prareafileobs'
    :param prlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for PR
    :param prlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'prlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for PR
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalPrLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed PR file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ['detrending', 'frequency', 'min_time_steps', 'normalization', 'regridding', 'smoothing',
                    'time_bounds_mod', 'time_bounds_obs']
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "pr zonal seasonality RMSE"
    Units = "mm/day"
    Method = "Zonal root mean square error of " + box + " climatological pr STD"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalPrLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    pr_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        prfilemod, prnamemod, "precipitations", metric, box, file_area=prareafilemod, name_area=prareanamemod,
        file_mask=prlandmaskfilemod, name_mask=prlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    pr_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        prfileobs, prnameobs, "precipitations", metric, box, file_area=prareafileobs, name_area=prareanameobs,
        file_mask=prlandmaskfileobs, name_mask=prlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(pr_mod.shape[0] / 12))
    yearN_obs = int(round(pr_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(pr_mod)
    actualtimebounds_obs = TimeBounds(pr_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        pr_mod, Method, keyerror_mod = PreProcessTS(
            pr_mod, Method, areacell=mod_areacell, compute_sea_cycle=True, region=box, **kwargs)
        pr_obs, _, keyerror_obs = PreProcessTS(
            pr_obs, "", areacell=obs_areacell, compute_sea_cycle=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                              "shape1": "(mod) " + str(pr_mod.shape), "shape2": "(obs) " + str(pr_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # standard deviation computation
            prStd_mod = Std(pr_mod)
            prStd_obs = Std(pr_obs)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in prStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(prStd_mod.shape), "shape2": "(obs) " + str(prStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                prStd_mod, prStd_obs, Method = TwoVarRegrid(
                    prStd_mod, prStd_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStd_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in prStd_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(prStd_mod.shape),
                                  "shape2": "(obs) " + str(prStd_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            prStdLon_mod, keyerror_mod = AverageMeridional(prStd_mod)
            prStdLon_obs, keyerror_obs = AverageMeridional(prStd_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in prStdLon_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in prStdLon_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(prStdLon_mod.shape),
                                  "shape2": "(obs) " + str(prStdLon_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(prStdLon_mod, prStdLon_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(prStdLon_mod, prStdLon_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(prStdLon_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(prStdLon_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(prStdLon_mod), "observations": ArrayToList(prStdLon_obs),
                                  "axis": list(prStdLon_mod.getAxis(0)[:])}
                if netcdf is True:
                    # Read file and select the right region
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        prfilemod, prnamemod, "precipitations", metric, "equatorial_pacific_LatExt2",
                        file_area=prareafilemod, name_area=prareanamemod, file_mask=prlandmaskfilemod,
                        name_mask=prlandmaskfilemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        prfileobs, prnameobs, "precipitations", metric, "equatorial_pacific_LatExt2",
                        file_area=prareafileobs, name_area=prareanameobs, file_mask=prlandmaskfileobs,
                        name_mask=prlandmaskfileobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess pr (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(map_mod.shape),
                                              "shape2": "(obs) " + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # standard deviation computation
                            map_mod = Std(map_mod)
                            map_obs = Std(map_obs)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                                pr_mod, pr_obs, _ = TwoVarRegrid(pr_mod, pr_obs, "", region=box, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(pr_mod.shape),
                                                  "shape2": "(obs) " + str(pr_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
                            # Meridional average
                            pr_mod, keyerror_mod = AverageMeridional(pr_mod)
                            pr_obs, keyerror_obs = AverageMeridional(pr_obs)
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in pr_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in pr_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(pr_mod.shape),
                                                  "shape2": "(obs) " + str(pr_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after AverageMeridional", 15, **dict_debug)
                                # Supplementary metrics
                                dict_metric, dict_nc = dict(), dict()
                                dict_metric, dict_nc = fill_dict_axis(
                                    map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="map of the standard deviation of the mean annual cycle of PR")
                                dict_metric, dict_nc = fill_dict_axis(
                                    pr_mod, pr_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 5, ovar[2], "hov", Units, "01", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="mean annual cycle of PR across longitudes")
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of PR across longitudes"}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of PR across longitudes"}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_value_" + dataset2: mv,
                                    "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2 + "_lon": sm_corr,
                                    "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                    "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=prStdLon_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=prStdLon_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalSshLatRmse(sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                       sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs,
                       box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                       netcdf_name="", metname="", **kwargs):
    """
    The SeasonalSshLatRmse() function computes the climatological (12 months) SSH meridional (latitude) standard
    deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the nino3_LatExt)

    Inputs:
    ------
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (ssh, sshg, zos) in 'sshfilemod'
    :param sshareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SSH
    :param sshareanamemod: string
        name of areacell variable (areacella, areacello) in 'sshareafilemod'
    :param sshlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SSH
    :param sshlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfilemod'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (ssh, sshg, zos) in 'sshfileobs'
    :param sshareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SSH
    :param sshareanameobs: string
        name of areacell variable (areacella, areacello) in 'sshareafileobs'
    :param sshlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SSH
    :param sshlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for SSH
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'AVISO',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalSshLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ssh meridional seasonality RMSE"
    Units = "cm"
    Method = "Meridional root mean square error of " + box + " climatological ssh STD"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalSshLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    ssh_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sshfilemod, sshnamemod, "sea surface height", metric, box, file_area=sshareafilemod, name_area=sshareanamemod,
        file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    ssh_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sshfileobs, sshnameobs, "sea surface height", metric, box, file_area=sshareafileobs, name_area=sshareanameobs,
        file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(ssh_mod.shape[0] / 12))
    yearN_obs = int(round(ssh_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(ssh_mod)
    actualtimebounds_obs = TimeBounds(ssh_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        ssh_mod, Method, keyerror_mod = PreProcessTS(
            ssh_mod, Method, areacell=mod_areacell, compute_sea_cycle=True, region=box, **kwargs)
        ssh_obs, _, keyerror_obs = PreProcessTS(
            ssh_obs, "", areacell=obs_areacell, compute_sea_cycle=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # change units
            ssh_mod = ssh_mod * 1e2
            ssh_obs = ssh_obs * 1e2

            # standard deviation computation
            sshStd_mod = Std(ssh_mod)
            sshStd_obs = Std(ssh_obs)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sshStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sshStd_mod.shape), "shape2": "(obs) " + str(sshStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sshStd_mod, sshStd_obs, Method = TwoVarRegrid(
                    sshStd_mod, sshStd_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStd_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sshStd_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sshStd_mod.shape),
                                  "shape2": "(obs) " + str(sshStd_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            sshStdLat_mod, keyerror_mod = AverageZonal(sshStd_mod)
            sshStdLat_obs, keyerror_obs = AverageZonal(sshStd_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStdLat_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sshStdLat_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sshStdLat_mod.shape),
                                  "shape2": "(obs) " + str(sshStdLat_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(sshStdLat_mod, sshStdLat_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(sshStdLat_mod, sshStdLat_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(sshStdLat_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(sshStdLat_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(sshStdLat_mod), "observations": ArrayToList(sshStdLat_obs),
                                  "axis": list(sshStdLat_mod.getAxis(0)[:])}
                if netcdf is True:
                    # Read file and select the right region
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific_LatExt2",
                        file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                        name_mask=sshlandmaskfilemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific_LatExt2",
                        file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                        name_mask=sshlandmaskfileobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess ssh (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(map_mod.shape),
                                              "shape2": "(obs) " + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode(
                                    "\033[92m", "after PreProcessTS: netCDF", 15, **dict_debug)
                            # change units
                            map_mod = map_mod * 1e2
                            map_obs = map_obs * 1e2
                            # standard deviation computation
                            map_mod = Std(map_mod)
                            map_obs = Std(map_obs)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                                ssh_mod, ssh_obs, _ = TwoVarRegrid(
                                    ssh_mod, ssh_obs, "", region=box, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(ssh_mod.shape),
                                                  "shape2": "(obs) " + str(ssh_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
                            # Zonal average
                            ssh_mod, keyerror_mod = AverageZonal(ssh_mod)
                            ssh_obs, keyerror_obs = AverageZonal(ssh_obs)
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(ssh_mod.shape),
                                                  "shape2": "(obs) " + str(ssh_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)
                                # Supplementary metrics
                                dict_metric, dict_nc = dict(), dict()
                                dict_metric, dict_nc = fill_dict_axis(
                                    map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="map of the standard deviation of the mean annual cycle of SSH")
                                dict_metric, dict_nc = fill_dict_axis(
                                    ssh_mod, ssh_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 5, ovar[2], "hov", Units, "01", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="mean annual cycle of SSH across latitudes")
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of SSH across latitudes"}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of SSH across latitudes"}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_value_" + dataset2: mv,
                                    "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2 + "_lat": sm_corr,
                                    "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                    "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=sshStdLat_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=sshStdLat_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalSshLonRmse(sshfilemod, sshnamemod, sshareafilemod, sshareanamemod, sshlandmaskfilemod, sshlandmasknamemod,
                       sshfileobs, sshnameobs, sshareafileobs, sshareanameobs, sshlandmaskfileobs, sshlandmasknameobs,
                       box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                       netcdf_name="", metname="", **kwargs):
    """
    The SeasonalSshLonRmse() function computes the climatological (12 months) SSH zonal (longitude) standard
    deviation root mean square error (RMSE) in a 'box' (usually the Equatorial Pacific)

    Inputs:
    ------
    :param sshfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SSH
    :param sshnamemod: string
        name of SSH variable (ssh, tauu) in 'sshfilemod'
    :param sshareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SSH
    :param sshareanamemod: string
        name of areacell variable (areacella, areacello) in 'sshareafilemod'
    :param sshlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SSH
    :param sshlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfilemod'
    :param sshfileobs: string
        path_to/filename of the file (NetCDF) of the observed SSH
    :param sshnameobs: string
        name of SSH variable (ssh, tauu) in 'sshfileobs'
    :param sshareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SSH
    :param sshareanameobs: string
        name of areacell variable (areacella, areacello) in 'sshareafileobs'
    :param sshlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SSH
    :param sshlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sshlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SSH
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'AVISO',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalSshLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SSH file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "ssh zonal seasonality RMSE"
    Units = "cm"
    Method = "Zonal root mean square error of " + box + " climatological ssh STD"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalSshLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    ssh_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sshfilemod, sshnamemod, "sea surface height", metric, box, file_area=sshareafilemod, name_area=sshareanamemod,
        file_mask=sshlandmaskfilemod, name_mask=sshlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    ssh_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sshfileobs, sshnameobs, "sea surface height", metric, box, file_area=sshareafileobs, name_area=sshareanameobs,
        file_mask=sshlandmaskfileobs, name_mask=sshlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(ssh_mod.shape[0] / 12))
    yearN_obs = int(round(ssh_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(ssh_mod)
    actualtimebounds_obs = TimeBounds(ssh_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        ssh_mod, Method, keyerror_mod = PreProcessTS(
            ssh_mod, Method, areacell=mod_areacell, compute_sea_cycle=True, region=box, **kwargs)
        ssh_obs, _, keyerror_obs = PreProcessTS(
            ssh_obs, "", areacell=obs_areacell, compute_sea_cycle=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                              "shape1": "(mod) " + str(ssh_mod.shape), "shape2": "(obs) " + str(ssh_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # change units
            ssh_mod = ssh_mod * 1e2
            ssh_obs = ssh_obs * 1e2

            # standard deviation computation
            sshStd_mod = Std(ssh_mod)
            sshStd_obs = Std(ssh_obs)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sshStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sshStd_mod.shape), "shape2": "(obs) " + str(sshStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sshStd_mod, sshStd_obs, Method = TwoVarRegrid(
                    sshStd_mod, sshStd_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStd_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sshStd_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sshStd_mod.shape),
                                  "shape2": "(obs) " + str(sshStd_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            sshStdLon_mod, keyerror_mod = AverageMeridional(sshStd_mod)
            sshStdLon_obs, keyerror_obs = AverageMeridional(sshStd_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sshStdLon_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sshStdLon_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sshStdLon_mod.shape),
                                  "shape2": "(obs) " + str(sshStdLon_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(sshStdLon_mod, sshStdLon_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(sshStdLon_mod, sshStdLon_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(sshStdLon_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(sshStdLon_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(sshStdLon_mod), "observations": ArrayToList(sshStdLon_obs),
                                  "axis": list(sshStdLon_mod.getAxis(0)[:])}
                if netcdf is True:
                    # Read file and select the right region
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        sshfilemod, sshnamemod, "sea surface height", metric, "equatorial_pacific_LatExt2",
                        file_area=sshareafilemod, name_area=sshareanamemod, file_mask=sshlandmaskfilemod,
                        name_mask=sshlandmaskfilemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        sshfileobs, sshnameobs, "sea surface height", metric, "equatorial_pacific_LatExt2",
                        file_area=sshareafileobs, name_area=sshareanameobs, file_mask=sshlandmaskfileobs,
                        name_mask=sshlandmaskfileobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess ssh (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(map_mod.shape),
                                              "shape2": "(obs) " + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # change units
                            map_mod = map_mod * 1e2
                            map_obs = map_obs * 1e2
                            # standard deviation computation
                            map_mod = Std(map_mod)
                            map_obs = Std(map_obs)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                                ssh_mod, ssh_obs, _ = TwoVarRegrid(
                                    ssh_mod, ssh_obs, "", region=box, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(ssh_mod.shape),
                                                  "shape2": "(obs) " + str(ssh_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
                            # Meridional average
                            ssh_mod, keyerror_mod = AverageMeridional(ssh_mod)
                            ssh_obs, keyerror_obs = AverageMeridional(ssh_obs)
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in ssh_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in ssh_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(ssh_mod.shape),
                                                  "shape2": "(obs) " + str(ssh_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after AverageMeridional", 15, **dict_debug)
                                # Supplementary metrics
                                dict_metric, dict_nc = dict(), dict()
                                dict_metric, dict_nc = fill_dict_axis(
                                    map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="map of the standard deviation of the mean annual cycle of SSH")
                                dict_metric, dict_nc = fill_dict_axis(
                                    ssh_mod, ssh_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 5, ovar[2], "hov", Units, "01", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="mean annual cycle of SSH across longitudes")
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of SSH across longitudes"}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of SSH across longitudes"}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_value_" + dataset2: mv,
                                    "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2 + "_lon": sm_corr,
                                    "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                    "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=sshStdLon_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=sshStdLon_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalSstLatRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                       sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                       box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                       netcdf_name="", metname="", **kwargs):
    """
    The SeasonalSstLatRmse() function computes the climatological (12 months) SST meridional (latitude) standard
    deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the equatorial_pacific_LatExt)

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalSstLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "sst meridional seasonality RMSE"
    Units = "C"
    Method = "Meridional root mean square error of " + box + " climatological sst STD"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalSstLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        sst_mod, Method, keyerror_mod = PreProcessTS(
            sst_mod, Method, areacell=mod_areacell, compute_sea_cycle=True, region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, compute_sea_cycle=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # standard deviation computation
            sstStd_mod = Std(sst_mod)
            sstStd_obs = Std(sst_obs)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sstStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sstStd_mod.shape), "shape2": "(obs) " + str(sstStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sstStd_mod, sstStd_obs, Method = TwoVarRegrid(
                    sstStd_mod, sstStd_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStd_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sstStd_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sstStd_mod.shape),
                                  "shape2": "(obs) " + str(sstStd_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            sstStdLat_mod, keyerror_mod = AverageZonal(sstStd_mod)
            sstStdLat_obs, keyerror_obs = AverageZonal(sstStd_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStdLat_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sstStdLat_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sstStdLat_mod.shape),
                                  "shape2": "(obs) " + str(sstStdLat_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(
                    sstStdLat_mod, sstStdLat_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(sstStdLat_mod, sstStdLat_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(sstStdLat_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(sstStdLat_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(sstStdLat_mod), "observations": ArrayToList(sstStdLat_obs),
                                  "axis": list(sstStdLat_mod.getAxis(0)[:])}
                if netcdf is True:
                    # Read file and select the right region
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific_LatExt2",
                        file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                        name_mask=sstlandmaskfilemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific_LatExt2",
                        file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                        name_mask=sstlandmaskfileobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(map_mod.shape),
                                              "shape2": "(obs) " + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # standard deviation computation
                            map_mod = Std(map_mod)
                            map_obs = Std(map_obs)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                                sst_mod, sst_obs, _ = TwoVarRegrid(
                                    sst_mod, sst_obs, "", region=box, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(sst_mod.shape),
                                                  "shape2": "(obs) " + str(sst_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
                            # Zonal average
                            sst_mod, keyerror_mod = AverageZonal(sst_mod)
                            sst_obs, keyerror_obs = AverageZonal(sst_obs)
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(sst_mod.shape),
                                                  "shape2": "(obs) " + str(sst_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)
                                # Supplementary metrics
                                dict_metric, dict_nc = dict(), dict()
                                dict_metric, dict_nc = fill_dict_axis(
                                    map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="map of the standard deviation of the mean annual cycle of SST")
                                dict_metric, dict_nc = fill_dict_axis(
                                    sst_mod, sst_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 5, ovar[2], "hov", Units, "01", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="mean annual cycle of SST across latitudes")
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of SST across latitudes"}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of SST across latitudes"}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_value_" + dataset2: mv,
                                    "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2 + "_lat": sm_corr,
                                    "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                    "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=sstStdLat_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=sstStdLat_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalSstLonRmse(sstfilemod, sstnamemod, sstareafilemod, sstareanamemod, sstlandmaskfilemod, sstlandmasknamemod,
                       sstfileobs, sstnameobs, sstareafileobs, sstareanameobs, sstlandmaskfileobs, sstlandmasknameobs,
                       box, centered_rmse=0, biased_rmse=1, dataset1="", dataset2="", debug=False, netcdf=False,
                       netcdf_name="", metname="", **kwargs):
    """
    The SeasonalSstLonRmse() function computes the climatological (12 months) SST zonal (longitude) standard
    deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the Equatorial Pacific)

    Inputs:
    ------
    :param sstfilemod: string
        path_to/filename of the file (NetCDF) of the modeled SST
    :param sstnamemod: string
        name of SST variable (tos, ts) in 'sstfilemod'
    :param sstareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for SST
    :param sstareanamemod: string
        name of areacell variable (areacella, areacello) in 'sstareafilemod'
    :param sstlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for SST
    :param sstlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfilemod'
    :param sstfileobs: string
        path_to/filename of the file (NetCDF) of the observed SST
    :param sstnameobs: string
        name of SST variable (tos, ts) in 'sstfileobs'
    :param sstareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for SST
    :param sstareanameobs: string
        name of areacell variable (areacella, areacello) in 'sstareafileobs'
    :param sstlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for SST
    :param sstlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'sstlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for SST
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalSstLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed SST file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "sst zonal seasonality RMSE"
    Units = "C"
    Method = "Zonal root mean square error of " + box + " climatological sst STD"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalSstLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    sst_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        sstfilemod, sstnamemod, "temperature", metric, box, file_area=sstareafilemod, name_area=sstareanamemod,
        file_mask=sstlandmaskfilemod, name_mask=sstlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    sst_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        sstfileobs, sstnameobs, "temperature", metric, box, file_area=sstareafileobs, name_area=sstareanameobs,
        file_mask=sstlandmaskfileobs, name_mask=sstlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(sst_mod.shape[0] / 12))
    yearN_obs = int(round(sst_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(sst_mod)
    actualtimebounds_obs = TimeBounds(sst_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        sst_mod, Method, keyerror_mod = PreProcessTS(
            sst_mod, Method, areacell=mod_areacell, compute_sea_cycle=True, region=box, **kwargs)
        sst_obs, _, keyerror_obs = PreProcessTS(
            sst_obs, "", areacell=obs_areacell, compute_sea_cycle=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sst_mod.shape), "shape2": "(obs) " + str(sst_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # standard deviation computation
            sstStd_mod = Std(sst_mod)
            sstStd_obs = Std(sst_obs)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in sstStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(sstStd_mod.shape), "shape2": "(obs) " + str(sstStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                sstStd_mod, sstStd_obs, Method = TwoVarRegrid(
                    sstStd_mod, sstStd_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStd_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sstStd_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sstStd_mod.shape),
                                  "shape2": "(obs) " + str(sstStd_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            sstStdLon_mod, keyerror_mod = AverageMeridional(sstStd_mod)
            sstStdLon_obs, keyerror_obs = AverageMeridional(sstStd_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sstStdLon_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in sstStdLon_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(sstStdLon_mod.shape),
                                  "shape2": "(obs) " + str(sstStdLon_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(sstStdLon_mod, sstStdLon_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(sstStdLon_mod, sstStdLon_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(sstStdLon_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(sstStdLon_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(sstStdLon_mod), "observations": ArrayToList(sstStdLon_obs),
                                  "axis": list(sstStdLon_mod.getAxis(0)[:])}
                if netcdf is True:
                    # Read file and select the right region
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        sstfilemod, sstnamemod, "temperature", metric, "equatorial_pacific_LatExt2",
                        file_area=sstareafilemod, name_area=sstareanamemod, file_mask=sstlandmaskfilemod,
                        name_mask=sstlandmaskfilemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        sstfileobs, sstnameobs, "temperature", metric, "equatorial_pacific_LatExt2",
                        file_area=sstareafileobs, name_area=sstareanameobs, file_mask=sstlandmaskfileobs,
                        name_mask=sstlandmaskfileobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess sst (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(map_mod.shape),
                                              "shape2": "(obs) " + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # standard deviation computation
                            map_mod = Std(map_mod)
                            map_obs = Std(map_obs)
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                                sst_mod, sst_obs, _ = TwoVarRegrid(
                                    sst_mod, sst_obs, "", region=box, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(sst_mod.shape),
                                                  "shape2": "(obs) " + str(sst_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
                            # Meridional average
                            sst_mod, keyerror_mod = AverageMeridional(sst_mod)
                            sst_obs, keyerror_obs = AverageMeridional(sst_obs)
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in sst_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in sst_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(sst_mod.shape),
                                                  "shape2": "(obs) " + str(sst_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after AverageMeridional", 15, **dict_debug)
                                # Supplementary metrics
                                dict_metric, dict_nc = dict(), dict()
                                dict_metric, dict_nc = fill_dict_axis(
                                    map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="map of the standard deviation of the mean annual cycle of SST")
                                dict_metric, dict_nc = fill_dict_axis(
                                    sst_mod, sst_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 5, ovar[2], "hov", Units, "01", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="mean annual cycle of SST across longitudes")
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of SST across longitudes"}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of SST across longitudes"}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_value_" + dataset2: mv,
                                    "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2 + "_lon": sm_corr,
                                    "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                    "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=sstStdLon_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=sstStdLon_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalTauxLatRmse(tauxfilemod, tauxnamemod, tauxareafilemod, tauxareanamemod, tauxlandmaskfilemod,
                        tauxlandmasknamemod, tauxfileobs, tauxnameobs, tauxareafileobs, tauxareanameobs,
                        tauxlandmaskfileobs, tauxlandmasknameobs, box, centered_rmse=0, biased_rmse=1, dataset1="",
                        dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The SeasonalTauxLatRmse() function computes the climatological (12 months) TAUX meridional (latitude) standard
    deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the equatorial_pacific_LatExt)

    Inputs:
    ------
    :param tauxfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUX
    :param tauxnamemod: string
        name of TAUX variable (tauu, tauuo) in 'tauxfilemod'
    :param tauxareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for TAUX
    :param tauxareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauxareafilemod'
    :param tauxlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for TAUX
    :param tauxlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfilemod'
    :param tauxfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUX
    :param tauxnameobs: string
        name of TAUX variable (taux, tauu) in 'tauxfileobs'
    :param tauxareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for TAUX
    :param tauxareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauxareafileobs'
    :param tauxlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for TAUX
    :param tauxlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for TAUX
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalTauxLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled TAUX file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed TAUX file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "taux meridional seasonality RMSE"
    Units = "1e-3 N/m2"
    Method = "Meridional root mean square error of " + box + " climatological taux STD"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalTauxLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    taux_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        tauxfilemod, tauxnamemod, "wind stress", metric, box, file_area=tauxareafilemod, name_area=tauxareanamemod,
        file_mask=tauxlandmaskfilemod, name_mask=tauxlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    taux_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        tauxfileobs, tauxnameobs, "wind stress", metric, box, file_area=tauxareafileobs, name_area=tauxareanameobs,
        file_mask=tauxlandmaskfileobs, name_mask=tauxlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(taux_mod.shape[0] / 12))
    yearN_obs = int(round(taux_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(taux_mod)
    actualtimebounds_obs = TimeBounds(taux_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        taux_mod, Method, keyerror_mod = PreProcessTS(
            taux_mod, Method, areacell=mod_areacell, compute_sea_cycle=True, region=box, **kwargs)
        taux_obs, _, keyerror_obs = PreProcessTS(
            taux_obs, "", areacell=obs_areacell, compute_sea_cycle=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                              "shape1": "(mod) " + str(taux_mod.shape), "shape2": "(obs) " + str(taux_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # change units
            taux_mod = taux_mod * 1e3
            taux_obs = taux_obs * 1e3

            # standard deviation computation
            tauxStd_mod = Std(taux_mod)
            tauxStd_obs = Std(taux_obs)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauxStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauxStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauxStd_mod.shape), "shape2": "(obs) " + str(tauxStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tauxStd_mod, tauxStd_obs, Method = TwoVarRegrid(
                    tauxStd_mod, tauxStd_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauxStd_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauxStd_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauxStd_mod.shape),
                                  "shape2": "(obs) " + str(tauxStd_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            tauxStdLat_mod, keyerror_mod = AverageZonal(tauxStd_mod)
            tauxStdLat_obs, keyerror_obs = AverageZonal(tauxStd_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauxStdLat_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauxStdLat_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauxStdLat_mod.shape),
                                  "shape2": "(obs) " + str(tauxStdLat_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(
                    tauxStdLat_mod, tauxStdLat_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(tauxStdLat_mod, tauxStdLat_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(tauxStdLat_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(tauxStdLat_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(tauxStdLat_mod), "observations": ArrayToList(tauxStdLat_obs),
                                  "axis": list(tauxStdLat_mod.getAxis(0)[:])}
                if netcdf is True:
                    # Read file and select the right region
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        tauxfilemod, tauxnamemod, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauxareafilemod, name_area=tauxareanamemod, file_mask=tauxlandmaskfilemod,
                        name_mask=tauxlandmaskfilemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        tauxfileobs, tauxnameobs, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauxareafileobs, name_area=tauxareanameobs, file_mask=tauxlandmaskfileobs,
                        name_mask=tauxlandmaskfileobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess taux (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(map_mod.shape),
                                              "shape2": "(obs) " + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # standard deviation computation
                            map_mod = Std(map_mod) * 1e3
                            map_obs = Std(map_obs) * 1e3
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                                taux_mod, taux_obs, _ = TwoVarRegrid(
                                    taux_mod, taux_obs, "", region=box, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(taux_mod.shape),
                                                  "shape2": "(obs) " + str(taux_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
                            # Zonal average
                            taux_mod, keyerror_mod = AverageZonal(taux_mod)
                            taux_obs, keyerror_obs = AverageZonal(taux_obs)
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(taux_mod.shape),
                                                  "shape2": "(obs) " + str(taux_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)
                                # Supplementary metrics
                                dict_metric, dict_nc = dict(), dict()
                                dict_metric, dict_nc = fill_dict_axis(
                                    map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="map of the standard deviation of the mean annual cycle of TAUX")
                                dict_metric, dict_nc = fill_dict_axis(
                                    taux_mod, taux_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 5, ovar[2], "hov", Units, "01", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="mean annual cycle of TAUX across latitudes")
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of TAUX across latitudes"}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of TAUX across latitudes"}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_value_" + dataset2: mv,
                                    "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2 + "_lat": sm_corr,
                                    "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                    "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=tauxStdLat_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=tauxStdLat_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalTauxLonRmse(tauxfilemod, tauxnamemod, tauxareafilemod, tauxareanamemod, tauxlandmaskfilemod,
                        tauxlandmasknamemod, tauxfileobs, tauxnameobs, tauxareafileobs, tauxareanameobs,
                        tauxlandmaskfileobs, tauxlandmasknameobs, box, centered_rmse=0, biased_rmse=1, dataset1="",
                        dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The SeasonalTauxLonRmse() function computes the climatological (12 months) TAUX zonal (longitude) standard
    deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the Equatorial Pacific)

    Inputs:
    ------
    :param tauxfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUX
    :param tauxnamemod: string
        name of TAUX variable (tauu, tauuo) in 'tauxfilemod'
    :param tauxareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for TAUX
    :param tauxareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauxareafilemod'
    :param tauxlandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for TAUX
    :param tauxlandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfilemod'
    :param tauxfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUX
    :param tauxnameobs: string
        name of TAUX variable (taux, tauu) in 'tauxfileobs'
    :param tauxareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for TAUX
    :param tauxareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauxareafileobs'
    :param tauxlandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for TAUX
    :param tauxlandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauxlandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for TAUX
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalTauxLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled TAUX file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed TAUX file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "taux zonal seasonality RMSE"
    Units = "1e-3 N/m2"
    Method = "Zonal root mean square error of " + box + " climatological taux STD"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalTauxLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    taux_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        tauxfilemod, tauxnamemod, "wind stress", metric, box, file_area=tauxareafilemod, name_area=tauxareanamemod,
        file_mask=tauxlandmaskfilemod, name_mask=tauxlandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    taux_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        tauxfileobs, tauxnameobs, "wind stress", metric, box, file_area=tauxareafileobs, name_area=tauxareanameobs,
        file_mask=tauxlandmaskfileobs, name_mask=tauxlandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(taux_mod.shape[0] / 12))
    yearN_obs = int(round(taux_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(taux_mod)
    actualtimebounds_obs = TimeBounds(taux_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        taux_mod, Method, keyerror_mod = PreProcessTS(
            taux_mod, Method, areacell=mod_areacell, compute_sea_cycle=True, region=box, **kwargs)
        taux_obs, _, keyerror_obs = PreProcessTS(
            taux_obs, "", areacell=obs_areacell, compute_sea_cycle=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                              "shape1": "(mod) " + str(taux_mod.shape), "shape2": "(obs) " + str(taux_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # change units
            taux_mod = taux_mod * 1e3
            taux_obs = taux_obs * 1e3

            # standard deviation computation
            tauxStd_mod = Std(taux_mod)
            tauxStd_obs = Std(taux_obs)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauxStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauxStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauxStd_mod.shape), "shape2": "(obs) " + str(tauxStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tauxStd_mod, tauxStd_obs, Method = TwoVarRegrid(
                    tauxStd_mod, tauxStd_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauxStd_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauxStd_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauxStd_mod.shape),
                                  "shape2": "(obs) " + str(tauxStd_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            tauxStdLon_mod, keyerror_mod = AverageMeridional(tauxStd_mod)
            tauxStdLon_obs, keyerror_obs = AverageMeridional(tauxStd_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauxStdLon_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauxStdLon_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauxStdLon_mod.shape),
                                  "shape2": "(obs) " + str(tauxStdLon_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(
                    tauxStdLon_mod, tauxStdLon_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(tauxStdLon_mod, tauxStdLon_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(tauxStdLon_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(tauxStdLon_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(tauxStdLon_mod), "observations": ArrayToList(tauxStdLon_obs),
                                  "axis": list(tauxStdLon_mod.getAxis(0)[:])}
                if netcdf is True:
                    # Read file and select the right region
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        tauxfilemod, tauxnamemod, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauxareafilemod, name_area=tauxareanamemod, file_mask=tauxlandmaskfilemod,
                        name_mask=tauxlandmaskfilemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        tauxfileobs, tauxnameobs, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauxareafileobs, name_area=tauxareanameobs, file_mask=tauxlandmaskfileobs,
                        name_mask=tauxlandmaskfileobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess taux (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(map_mod.shape),
                                              "shape2": "(obs) " + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # standard deviation computation
                            map_mod = Std(map_mod) * 1e3
                            map_obs = Std(map_obs) * 1e3
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                                taux_mod, taux_obs, _ = TwoVarRegrid(
                                    taux_mod, taux_obs, "", region=box, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(taux_mod.shape),
                                                  "shape2": "(obs) " + str(taux_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
                            # Meridional average
                            taux_mod, keyerror_mod = AverageMeridional(taux_mod)
                            taux_obs, keyerror_obs = AverageMeridional(taux_obs)
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in taux_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in taux_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(taux_mod.shape),
                                                  "shape2": "(obs) " + str(taux_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after AverageMeridional", 15, **dict_debug)
                                # Supplementary metrics
                                dict_metric, dict_nc = dict(), dict()
                                dict_metric, dict_nc = fill_dict_axis(
                                    map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="map of the standard deviation of the mean annual cycle of TAUX")
                                dict_metric, dict_nc = fill_dict_axis(
                                    taux_mod, taux_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 5, ovar[2], "hov", Units, "01", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="mean annual cycle of TAUX across longitudes")
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of TAUX across longitudes"}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of TAUX across longitudes"}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_value_" + dataset2: mv,
                                    "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2 + "_lon": sm_corr,
                                    "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                    "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=tauxStdLon_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=tauxStdLon_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalTauyLatRmse(tauyfilemod, tauynamemod, tauyareafilemod, tauyareanamemod, tauylandmaskfilemod,
                        tauylandmasknamemod, tauyfileobs, tauynameobs, tauyareafileobs, tauyareanameobs,
                        tauylandmaskfileobs, tauylandmasknameobs, box, centered_rmse=0, biased_rmse=1, dataset1="",
                        dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The SeasonalTauyLatRmse() function computes the climatological (12 months) TAUY meridional (latitude) standard
    deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the equatorial_pacific_LatExt)

    Inputs:
    ------
    :param tauyfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUY
    :param tauynamemod: string
        name of TAUY variable (tauu, tauuo) in 'tauyfilemod'
    :param tauyareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for TAUY
    :param tauyareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauyareafilemod'
    :param tauylandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for TAUY
    :param tauylandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauylandmaskfilemod'
    :param tauyfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUY
    :param tauynameobs: string
        name of TAUY variable (tauy, tauu) in 'tauyfileobs'
    :param tauyareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for TAUY
    :param tauyareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauyareafileobs'
    :param tauylandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for TAUY
    :param tauylandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauylandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific_LatExt') for TAUY
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalTauyLatRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled TAUY file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed TAUY file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "tauy meridional seasonality RMSE"
    Units = "1e-3 N/m2"
    Method = "Meridional root mean square error of " + box + " climatological tauy STD"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalTauyLatRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    tauy_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        tauyfilemod, tauynamemod, "wind stress", metric, box, file_area=tauyareafilemod, name_area=tauyareanamemod,
        file_mask=tauylandmaskfilemod, name_mask=tauylandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tauy_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        tauyfileobs, tauynameobs, "wind stress", metric, box, file_area=tauyareafileobs, name_area=tauyareanameobs,
        file_mask=tauylandmaskfileobs, name_mask=tauylandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(tauy_mod.shape[0] / 12))
    yearN_obs = int(round(tauy_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(tauy_mod)
    actualtimebounds_obs = TimeBounds(tauy_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        tauy_mod, Method, keyerror_mod = PreProcessTS(
            tauy_mod, Method, areacell=mod_areacell, compute_sea_cycle=True, region=box, **kwargs)
        tauy_obs, _, keyerror_obs = PreProcessTS(
            tauy_obs, "", areacell=obs_areacell, compute_sea_cycle=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauy_mod.shape), "shape2": "(obs) " + str(tauy_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # change units
            tauy_mod = tauy_mod * 1e3
            tauy_obs = tauy_obs * 1e3

            # standard deviation computation
            tauyStd_mod = Std(tauy_mod)
            tauyStd_obs = Std(tauy_obs)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauyStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauyStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauyStd_mod.shape), "shape2": "(obs) " + str(tauyStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tauyStd_mod, tauyStd_obs, Method = TwoVarRegrid(
                    tauyStd_mod, tauyStd_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauyStd_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauyStd_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauyStd_mod.shape),
                                  "shape2": "(obs) " + str(tauyStd_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Zonal average
            tauyStdLat_mod, keyerror_mod = AverageZonal(tauyStd_mod)
            tauyStdLat_obs, keyerror_obs = AverageZonal(tauyStd_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauyStdLat_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauyStdLat_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauyStdLat_mod.shape),
                                  "shape2": "(obs) " + str(tauyStdLat_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsMeridional(
                    tauyStdLat_mod, tauyStdLat_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(tauyStdLat_mod, tauyStdLat_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(tauyStdLat_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(tauyStdLat_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(tauyStdLat_mod), "observations": ArrayToList(tauyStdLat_obs),
                                  "axis": list(tauyStdLat_mod.getAxis(0)[:])}
                if netcdf is True:
                    # Read file and select the right region
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        tauyfilemod, tauynamemod, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauyareafilemod, name_area=tauyareanamemod, file_mask=tauylandmaskfilemod,
                        name_mask=tauylandmaskfilemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        tauyfileobs, tauynameobs, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauyareafileobs, name_area=tauyareanameobs, file_mask=tauylandmaskfileobs,
                        name_mask=tauylandmaskfileobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess tauy (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(map_mod.shape),
                                              "shape2": "(obs) " + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # standard deviation computation
                            map_mod = Std(map_mod) * 1e3
                            map_obs = Std(map_obs) * 1e3
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                                tauy_mod, tauy_obs, _ = TwoVarRegrid(
                                    tauy_mod, tauy_obs, "", region=box, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(tauy_mod.shape),
                                                  "shape2": "(obs) " + str(tauy_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
                            # Zonal average
                            tauy_mod, keyerror_mod = AverageZonal(tauy_mod)
                            tauy_obs, keyerror_obs = AverageZonal(tauy_obs)
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(tauy_mod.shape),
                                                  "shape2": "(obs) " + str(tauy_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageZonal", 15, **dict_debug)
                                # Supplementary metrics
                                dict_metric, dict_nc = dict(), dict()
                                dict_metric, dict_nc = fill_dict_axis(
                                    map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="map of the standard deviation of the mean annual cycle of TAUY")
                                dict_metric, dict_nc = fill_dict_axis(
                                    tauy_mod, tauy_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 5, ovar[2], "hov", Units, "01", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="mean annual cycle of TAUY across latitudes")
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of TAUY across latitudes"}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of TAUY across latitudes"}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_value_" + dataset2: mv,
                                    "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2 + "_lat": sm_corr,
                                    "CORR_error_" + dataset2 + "_lat": sm_corr_error,
                                    "STD_" + dataset2 + "_lat": sm_std, "STD_error_" + dataset2 + "_lat": sm_std_error}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=tauyStdLat_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=tauyStdLat_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output


def SeasonalTauyLonRmse(tauyfilemod, tauynamemod, tauyareafilemod, tauyareanamemod, tauylandmaskfilemod,
                        tauylandmasknamemod, tauyfileobs, tauynameobs, tauyareafileobs, tauyareanameobs,
                        tauylandmaskfileobs, tauylandmasknameobs, box, centered_rmse=0, biased_rmse=1, dataset1="",
                        dataset2="", debug=False, netcdf=False, netcdf_name="", metname="", **kwargs):
    """
    The SeasonalTauyLonRmse() function computes the climatological (12 months) TAUY zonal (longitude) standard
    deviation (of the climatology) root mean square error (RMSE) in a 'box' (usually the Equatorial Pacific)

    Inputs:
    ------
    :param tauyfilemod: string
        path_to/filename of the file (NetCDF) of the modeled TAUY
    :param tauynamemod: string
        name of TAUY variable (tauu, tauuo) in 'tauyfilemod'
    :param tauyareafilemod: string
        path_to/filename of the file (NetCDF) of the model areacell for TAUY
    :param tauyareanamemod: string
        name of areacell variable (areacella, areacello) in 'tauyareafilemod'
    :param tauylandmaskfilemod: string
        path_to/filename of the file (NetCDF) of the model landmask for TAUY
    :param tauylandmasknamemod: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauylandmaskfilemod'
    :param tauyfileobs: string
        path_to/filename of the file (NetCDF) of the observed TAUY
    :param tauynameobs: string
        name of TAUY variable (tauy, tauu) in 'tauyfileobs'
    :param tauyareafileobs: string
        path_to/filename of the file (NetCDF) of the observations areacell for TAUY
    :param tauyareanameobs: string
        name of areacell variable (areacella, areacello) in 'tauyareafileobs'
    :param tauylandmaskfileobs: string
        path_to/filename of the file (NetCDF) of the observations landmask for TAUY
    :param tauylandmasknameobs: string
        name of landmask variable (sftlf, lsmask, landmask) in 'tauylandmaskfileobs'
    :param box: string
        name of box ('equatorial_pacific') for TAUY
    :param centered_rmse: int, optional
        default value = 0 returns uncentered statistic (same as None). To remove the mean first (i.e centered statistic)
        set to 1. NOTE: Most other statistic functions return a centered statistic by default
    :param biased_rmse: int, optional
        default value = 1 returns biased statistic (number of elements along given axis)
        If want to compute an unbiased variance pass anything but 1 (number of elements along given axis minus 1)
    :param dataset1: string, optional
        name of model dataset (e.g., 'model', 'ACCESS1-0', ...)
    :param dataset2: string, optional
        name of observational dataset (e.g., 'obs', 'Tropflux',...)
    :param debug: bolean, optional
        default value = False debug mode not activated
        If you want to activate the debug mode set it to True (prints regularly to see the progress of the calculation)
    :param netcdf: boolean, optional
        default value = False dive_down are not saved in NetCDFs
        If you want to save the dive down diagnostics set it to True
    :param netcdf_name: string, optional
        default value = '' NetCDFs are saved where the program is ran without a root name
        the name of a metric will be append at the end of the root name
        e.g., netcdf_name='/path/to/directory/USER_DATE_METRICCOLLECTION_MODEL'
    :param metname: string, optional
        default value = '' metric name is not changed
        e.g., metname = 'SeasonalTauyLonRmse_2'
    usual kwargs:
    :param detrending: dict, optional
        see EnsoUvcdatToolsLib.Detrend for options
        the aim if to specify if the trend must be removed
        detrending method can be specified
        default value is False
    :param frequency: string, optional
        time frequency of the datasets
        e.g., frequency='monthly'
        default value is None
    :param min_time_steps: int, optional
        minimum number of time steps for the metric to make sens
        e.g., for 30 years of monthly data mintimesteps=360
        default value is None
    :param normalization: boolean, optional
        True to normalize by the standard deviation (needs the frequency to be defined), if you don't want it pass
        anything but true
        default value is False
    :param regridding: dict, optional
        see EnsoUvcdatToolsLib.TwoVarRegrid and EnsoUvcdatToolsLib.Regrid for options
        the aim if to specify if the model is regridded toward the observations or vice versa, of if both model and
        observations are regridded toward another grid
        interpolation tool and method can be specified
        default value is False
    :param smoothing: dict, optional
        see EnsoUvcdatToolsLib.Smoothing for options
        the aim if to specify if variables are smoothed (running mean)
        smoothing axis, window and method can be specified
        default value is False
    :param time_bounds_mod: tuple, optional
        tuple of the first and last dates to extract from the modeled TAUY file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None
    :param time_bounds_obs: tuple, optional
        tuple of the first and last dates to extract from the observed TAUY file (strings)
        e.g., time_bounds=('1979-01-01T00:00:00', '2017-01-01T00:00:00')
        default value is None

    Output:
    ------
    :return metric_output: dict
        name, value, value_error, units, method, nyears_model, nyears_observations, time_frequency, time_period_model,
        time_period_observations, ref, keyerror, dive_down_diag

    Method:
    -------
        uses tools from uvcdat library

    Notes:
    -----
        TODO: add error calculation to rmse (function of nyears)

    """
    # test given kwargs
    needed_kwarg = ["detrending", "frequency", "min_time_steps", "normalization", "regridding", "smoothing",
                    "time_bounds_mod", "time_bounds_obs"]
    for arg in needed_kwarg:
        if arg not in list(kwargs.keys()):
            kwargs[arg] = default_arg_values(arg)

    # Define metric attributes
    Name = "tauy zonal seasonality RMSE"
    Units = "1e-3 N/m2"
    Method = "Zonal root mean square error of " + box + " climatological tauy STD"
    Ref = "Using CDAT regridding and rms (uncentered and biased) calculation"
    metric = "SeasonalTauyLonRmse"
    if metname == "":
        metname = deepcopy(metric)
    ovar = metric_variable_names(metric)

    # Read file and select the right region
    if debug is True:
        EnsoErrorsWarnings.debug_mode("\033[92m", metric, 10)
    tauy_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
        tauyfilemod, tauynamemod, "wind stress", metric, box, file_area=tauyareafilemod, name_area=tauyareanamemod,
        file_mask=tauylandmaskfilemod, name_mask=tauylandmasknamemod, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
    tauy_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
        tauyfileobs, tauynameobs, "wind stress", metric, box, file_area=tauyareafileobs, name_area=tauyareanameobs,
        file_mask=tauylandmaskfileobs, name_mask=tauylandmasknameobs, maskland=True, maskocean=False,
        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)

    # Number of years
    yearN_mod = int(round(tauy_mod.shape[0] / 12))
    yearN_obs = int(round(tauy_obs.shape[0] / 12))

    # Time period
    actualtimebounds_mod = TimeBounds(tauy_mod)
    actualtimebounds_obs = TimeBounds(tauy_obs)

    mv, mv_error, dive_down_diag = None, None, {"model": None, "observations": None, "axis": None}
    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
    if keyerror is None:
        # Preprocess variables (computes anomalies, normalizes, detrends TS, smoothes TS, averages horizontally)
        # here only the detrending (if applicable) and time averaging are performed
        tauy_mod, Method, keyerror_mod = PreProcessTS(
            tauy_mod, Method, areacell=mod_areacell, compute_sea_cycle=True, region=box, **kwargs)
        tauy_obs, _, keyerror_obs = PreProcessTS(
            tauy_obs, "", areacell=obs_areacell, compute_sea_cycle=True, region=box, **kwargs)
        del mod_areacell, obs_areacell
        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
        if keyerror is None:
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauy_mod.shape), "shape2": "(obs) " + str(tauy_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)

            # change units
            tauy_mod = tauy_mod * 1e3
            tauy_obs = tauy_obs * 1e3

            # standard deviation computation
            tauyStd_mod = Std(tauy_mod)
            tauyStd_obs = Std(tauy_obs)
            if debug is True:
                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauyStd_mod.getAxisList()]),
                              "axes2": "(obs) " + str([ax.id for ax in tauyStd_obs.getAxisList()]),
                              "shape1": "(mod) " + str(tauyStd_mod.shape), "shape2": "(obs) " + str(tauyStd_obs.shape)}
                EnsoErrorsWarnings.debug_mode("\033[92m", "after Std", 15, **dict_debug)

            # Regridding
            if isinstance(kwargs["regridding"], dict):
                known_args = {"model_orand_obs", "newgrid", "missing", "order", "mask", "newgrid_name", "regridder",
                              "regridTool", "regridMethod"}
                extra_args = set(kwargs["regridding"]) - known_args
                if extra_args:
                    EnsoErrorsWarnings.unknown_key_arg(extra_args, INSPECTstack())
                tauyStd_mod, tauyStd_obs, Method = TwoVarRegrid(
                    tauyStd_mod, tauyStd_obs, Method, region=box, **kwargs["regridding"])
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauyStd_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauyStd_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauyStd_mod.shape),
                                  "shape2": "(obs) " + str(tauyStd_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after TwoVarRegrid", 15, **dict_debug)

            # Meridional average
            tauyStdLon_mod, keyerror_mod = AverageMeridional(tauyStd_mod)
            tauyStdLon_obs, keyerror_obs = AverageMeridional(tauyStd_obs)
            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
            if keyerror is None:
                if debug is True:
                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauyStdLon_mod.getAxisList()]),
                                  "axes2": "(obs) " + str([ax.id for ax in tauyStdLon_obs.getAxisList()]),
                                  "shape1": "(mod) " + str(tauyStdLon_mod.shape),
                                  "shape2": "(obs) " + str(tauyStdLon_obs.shape)}
                    EnsoErrorsWarnings.debug_mode("\033[92m", "after AverageMeridional", 15, **dict_debug)

                # Computes the root mean square difference
                mv, keyerror = RmsZonal(
                    tauyStdLon_mod, tauyStdLon_obs, centered=centered_rmse, biased=biased_rmse)

                # Error on the metric
                mv_error = None

                # Supplementary metrics
                sm_corr = float(Correlation(tauyStdLon_mod, tauyStdLon_obs, axis=0, centered=1, biased=1))
                sm_corr_error = None
                sm_std_mod = Std(tauyStdLon_mod, weights=None, axis=0, centered=1, biased=1)
                sm_std_obs = Std(tauyStdLon_obs, weights=None, axis=0, centered=1, biased=1)
                sm_std = float(sm_std_mod) / float(sm_std_obs)
                sm_std_error = None

                # Dive down diagnostic
                dive_down_diag = {"model": ArrayToList(tauyStdLon_mod), "observations": ArrayToList(tauyStdLon_obs),
                                  "axis": list(tauyStdLon_mod.getAxis(0)[:])}
                if netcdf is True:
                    # Read file and select the right region
                    map_mod, mod_areacell, keyerror_mod = Read_data_mask_area(
                        tauyfilemod, tauynamemod, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauyareafilemod, name_area=tauyareanamemod, file_mask=tauylandmaskfilemod,
                        name_mask=tauylandmaskfilemod, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_mod"], debug=debug, **kwargs)
                    map_obs, obs_areacell, keyerror_obs = Read_data_mask_area(
                        tauyfileobs, tauynameobs, "wind stress", metric, "equatorial_pacific_LatExt2",
                        file_area=tauyareafileobs, name_area=tauyareanameobs, file_mask=tauylandmaskfileobs,
                        name_mask=tauylandmaskfileobs, maskland=True, maskocean=False,
                        time_bounds=kwargs["time_bounds_obs"], debug=debug, **kwargs)
                    keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                    if keyerror is None:
                        # Preprocess tauy (computes anomalies, normalizes, detrends TS, smoothes TS, ...)
                        map_mod, _, keyerror_mod = PreProcessTS(
                            map_mod, "", areacell=mod_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        map_obs, _, keyerror_obs = PreProcessTS(
                            map_obs, "", areacell=obs_areacell, compute_sea_cycle=True,
                            region="equatorial_pacific_LatExt2", **kwargs)
                        del mod_areacell, obs_areacell
                        keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                        if keyerror is None:
                            if debug is True:
                                dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                              "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                              "shape1": "(mod) " + str(map_mod.shape),
                                              "shape2": "(obs) " + str(map_obs.shape)}
                                EnsoErrorsWarnings.debug_mode("\033[92m", "after PreProcessTS", 15, **dict_debug)
                            # standard deviation computation
                            map_mod = Std(map_mod) * 1e3
                            map_obs = Std(map_obs) * 1e3
                            # Regridding
                            if isinstance(kwargs["regridding"], dict):
                                map_mod, map_obs, _ = TwoVarRegrid(
                                    map_mod, map_obs, "", region="equatorial_pacific_LatExt2", **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in map_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in map_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(map_mod.shape),
                                                  "shape2": "(obs) " + str(map_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 1", 15, **dict_debug)
                                tauy_mod, tauy_obs, _ = TwoVarRegrid(
                                    tauy_mod, tauy_obs, "", region=box, **kwargs["regridding"])
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(tauy_mod.shape),
                                                  "shape2": "(obs) " + str(tauy_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after TwoVarRegrid: netcdf map 2", 15, **dict_debug)
                            # Meridional average
                            tauy_mod, keyerror_mod = AverageMeridional(tauy_mod)
                            tauy_obs, keyerror_obs = AverageMeridional(tauy_obs)
                            keyerror = add_up_errors([keyerror_mod, keyerror_obs])
                            if keyerror is None:
                                if debug is True:
                                    dict_debug = {"axes1": "(mod) " + str([ax.id for ax in tauy_mod.getAxisList()]),
                                                  "axes2": "(obs) " + str([ax.id for ax in tauy_obs.getAxisList()]),
                                                  "shape1": "(mod) " + str(tauy_mod.shape),
                                                  "shape2": "(obs) " + str(tauy_obs.shape)}
                                    EnsoErrorsWarnings.debug_mode(
                                        "\033[92m", "after AverageMeridional", 15, **dict_debug)
                                # Supplementary metrics
                                dict_metric, dict_nc = dict(), dict()
                                dict_metric, dict_nc = fill_dict_axis(
                                    map_mod, map_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 3, ovar[1], "map", Units, "xy", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="map of the standard deviation of the mean annual cycle of TAUY")
                                dict_metric, dict_nc = fill_dict_axis(
                                    tauy_mod, tauy_obs, dataset1, dataset2, actualtimebounds_mod, actualtimebounds_obs,
                                    yearN_mod, yearN_obs, 5, ovar[2], "hov", Units, "01", centered_rmse=centered_rmse,
                                    biased_rmse=biased_rmse, dict_metric=dict_metric,
                                    description="mean annual cycle of TAUY across longitudes")
                                if ".nc" in netcdf_name:
                                    file_name = deepcopy(netcdf_name).replace(".nc", "_" + metname + ".nc")
                                else:
                                    file_name = deepcopy(netcdf_name) + "_" + metname + ".nc"
                                dict1 = {"units": Units, "number_of_years_used": yearN_mod,
                                         "time_period": str(actualtimebounds_mod), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of TAUY across longitudes"}
                                dict2 = {"units": Units, "number_of_years_used": yearN_obs,
                                         "time_period": str(actualtimebounds_obs), "arraySTD": sm_std_obs,
                                         "description":
                                             "standard deviation of the mean annual cycle of TAUY across longitudes"}
                                dict3 = {
                                    "metric_name": Name, "metric_method": Method, "metric_reference": Ref,
                                    "frequency": kwargs["frequency"], "metric_value_" + dataset2: mv,
                                    "metric_value_error_" + dataset2: mv_error, "CORR_" + dataset2 + "_lon": sm_corr,
                                    "CORR_error_" + dataset2 + "_lon": sm_corr_error,
                                    "STD_" + dataset2 + "_lon": sm_std, "STD_error_" + dataset2 + "_lon": sm_std_error}
                                dict3.update(dict_metric)
                                SaveNetcdf(
                                    file_name, global_attributes=dict3,
                                    var1=tauyStdLon_mod, var1_attributes=dict1, var1_name=ovar[0] + dataset1,
                                    var2=tauyStdLon_obs, var2_attributes=dict2, var2_name=ovar[0] + dataset2, **dict_nc)
                                del file_name, dict1, dict2, dict3, dict_metric, dict_nc
    # metric value
    if debug is True:
        dict_debug = {"line1": "metric value: " + str(mv), "line2": "metric value_error: " + str(mv_error)}
        EnsoErrorsWarnings.debug_mode("\033[92m", "end of " + metric, 10, **dict_debug)
    # Create output
    metric_output = {
        "name": Name, "value": mv, "value_error": mv_error, "units": Units, "method": Method,
        "nyears_model": yearN_mod, "nyears_observations": yearN_obs, "time_frequency": kwargs["frequency"],
        "time_period_model": actualtimebounds_mod, "time_period_observations": actualtimebounds_obs, "ref": Ref,
        "keyerror": keyerror, "dive_down_diag": dive_down_diag}
    return metric_output
# ---------------------------------------------------------------------------------------------------------------------#
